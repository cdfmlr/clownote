---
title: 爬虫基础
date: 2019-02-10 22:49:55
tags: Crawler
categories:
	- Crawler
---

# 爬虫基础

> 本文主要介绍爬虫涉及到的基础知识。



## HTTP 基本原理

### URI & URL

| 简写 | 全名 | 中文 | 说明 |
| -- | -- | -- | -- |
| URI | Uniform Resource Identifier | 统一资源标志符 | 对应着一个资源 |
| URL | Uniform Resource Locator | 统一资源定位符 | 说明从哪里可以找到一个资源 |
| URN | Universal Resource Name | 统一资源名称 | 说明一个资源的唯一名字 |

URI 包含着 URL 和 URN。

通常，我们使用的网址，就是一个URI，同时也是一个URL。

### 超文本
> 超文本是用超链接的方法，将各种不同空间的文字信息组织在一起的网状文本。

我们平时看到的网页的源代码使用 HTML 写出的，HTML 就是一种超文本。

### HTTP & HTTPS

我们用URL访问资源需要指定使用的协议类型，协议就是URL地址一开始的那个字段。

| 协议 | 全称 | 说明 |
| -- | -- | -- |
| HTTP | Hyper Text Transfer Protocol（超文本传输协议）| 用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本文档 |
| HTTPS | Hyper Text Transfer Protocol over Secure Socket Layer（在SSL上的HTTP）| 以安全为目标的 HTTP 通道，是 HTTP 的安全版 |

HTTPS 相较于 HTTP 有以下优点：

* 建立一个信息安全通道来保证数据传输的安全。
* 确认网站的真实性，凡是使用了 HTTPS 的网站，都可以通过点击浏览器地址栏的锁头标志来
查看网站认证之后的真实信息，也可以通过 CA 机构颁发的安全签章来查询。

### HTTP 请求过程

我们在浏览器中输入网址，便显示出一个网页的过程实际上是：

0. 浏览器向网站的服务器发送了一个请求；
1. 服务器接收到这个请求后进行解析和处理；
2. 服务器返回对应的响应；
3. 浏览器对收到的响应解析；
4. 浏览器将解析的结果显示出来，即最后我们看到的网页。

我们可以在 Chrome浏览器 的开发者工具的“Network”页面看到的一个条目就是一次 *发送请求和接收响应* 的过程。

| Network的列 | 说明 |
| -- | -- |
| Name | 请求的名称，一般会将 URL 的最后一部分内容当作名称 。 |
| Status | 响应的状态码，这里显示为 200， 代表响应是正常的 。 通过状态码，我们可以判断发送了请求之后是杏得到了正常的响应 。  |
| Type | 请求的文梢类型 。 这里为 document，代表我们这次请求的是一个 HTML文档， 内容就是一些 HTML代码。 |
| Initiator |  请求源 。 用来标记请求是由哪个对象或进程发起的 。 |
| Size |  从服务器下载的文件和请求的资源大小 。 如果是从缓存中取得的资源，则该列会显示from cache。 |
| Time |  发起请求到获取响应所用的总时间 。  |
| Waterfall | 网络请求的可视化瀑布流 。 |

点击条目，我们将看到更加详细的请求、响应信息。

### 请求（Request）

请求，是由 **客户端** 向 **服务端** 发出的。

请求分为4部分：

* **请求方法**（Request Method）：常见的请求方法分为两种 GET 和 POST：
    * `GET` 请求中的参数包含在 URL 里面，数据可以在 URL 中看到（可能暴露敏感信息）。GET请求提交的数据最多只有 1024 字节。
    * `POST` 请求中的 URL 不包含数据，数据都是通过表单形式传输的，会包含在请求体中。POST方式没有大小限制（可以传文件）。
    * 我们输入一个网址，这是一个 GET 请求。
    * 其实还有一些其他的请求方法，可以参考 [HTTP请求方法](http://www.runoob.com/http/http-methods.html)

* **请求地址**（Request URL）：统一资惊定位符 URL，唯一确定我们想请求的资源。
* **请求头**（Request Headers）：用来说明服务器要使用的附加信息，比较重要的信息有 Cookie、 Referer、 User-Agent：
    * `Accept`: 请求报头域，用于指定客户端可 **接受哪些类型的信息**。
    * `Accept-Language`: 指定客户端可接受的 **语言类型**。
    * `Acceot-Encoding`: 指定客户端可接受的 **内容编码**。
    * `Host`: 指定 **请求资源的主机 IP 和端口号**，其内容为 **请求 URL 的原始服务器或网关的位置**。
    * `Cookies`: 网站为了辨别用户进行会话跟踪而 **存储在用户本地的数据**。它的主要功能是 **维持当前访问会话**，如检查用户登录。
    * `Referer`: 此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相 应的处理，如做来源统计、防盗链处理等 。
    * `User-Agent`: 简称 UA，它可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。 **在做爬虫时加上此信息，可以伪装为浏览器; 如果不加，很可能会被识别州为爬虫 。**
    * `Content-Type`: 也叫互联网媒体类型( Internet Media Type )或者 MIME类型，在 HTTP协议消息头中，它用来表示具体请求中的媒体类型信息。请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。
        * text/html 代表 HTML 格式
        * image/gif代表 GIF 图片
        * application/json代表 JSON类型
        * 详见 [Content-Type 请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。对照表](http://tool.oschina.net/commons/)

* **请求体**（Request Body）：
    * `POST` 的请求体是表单数据;
    * `GET` 的请求体为空;

### 响应（Response）

响应，由 **服务端** 返回给 **客户端**：

* **响应状态码**（Response Status Code）：表示服务器的响应状态
    * HTTP状态码分类，具体HTTP状态码列表，见[HTTP状态码](http://www.runoob.com/http/http-status-codes.html)

| 分类 | 分类描述 |
| -- | -- |
| 1** | 信息，服务器收到请求，需要请求者继续执行操作 |
| 2** | 成功，操作被成功接收并处理 |
| 3** | 重定向，需要进一步的操作以完成请求 |
| 4** | 客户端错误，请求包含语法错误或无法完成请求 |
| 5** | 服务器错误，服务器在处理请求的过程中发生了错误 |


* **响应头**（Response Headers）: 响应头包含了服务器对请求的应答信息
    * `Date`: 标识 **响应产生的时间**
    * `Last-Modified`: 指定 **资源的最后修改时间**
    * `Content-Encoding`: 指定响应 **内容的编码**
    * `Server`: 包含 **服务器的信息**，比如名称、版本号等
    * `Content-Type`: 文档类型，指定 **返回的数据类型** 是什么（和 Request 的一样）
    * `Set-Cookie`: **设置 Cookies**。告诉浏览器需要将此内容放在 Cookies 中， 下次请求携带 Cookies 请求
    * `Expires`: **指定响应的过期时间**，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从**缓存**中加载，降低服务器负载，缩短加载时间。

* **响应体**（Response Body）响应的正文数据都在响应体中
    * 请求网页时，它的响应体就 是网页的 HTML代码
    * 请求一张图片时， 它的响应体就是图片的二进制数据
    * 我们做爬虫请求网页后，要解析的内容就是响应体

## 网页基础

网页分为三大部分 ———— HTML（框架），CSS（样式），JavaScript（逻辑）。

### HTML, 超文本标记语言

详见 [HTML 教程](http://www.w3school.com.cn/html/index.asp)

### CSS, 层叠样式表

详见 [CSS 教程](http://www.w3school.com.cn/css/index.asp)

### JavaScript, 脚本语言

详见 [JavaScript 教程](http://www.w3school.com.cn/js/index.asp)

### 网页结构

在 HTML 中，所有标签定义的内容都是节点，它们构成了一个 HTML DOM 树。（DOM：文档对象模型）
通过 HTML DOM，树中的所有节点均可通过 JavaScript访问，所有 HTML 节点元素均可被修改，也可以被创建或删除 。

详见 [HTML DOM 教程](http://www.w3school.com.cn/htmldom/index.asp)

### 选择器

在 css 中，我们使用 css 选择器来定位节点。

详见 [CSS 选择器表](http://www.w3school.com.cn/cssref/css_selectors.asp)

## 爬虫的基本原理

爬虫是**获取**网页并**提取**和**保存**信息的**自动化程序**。

可以说，我们能在浏览器中看到的一切内容，都可以通过爬虫得到（包括那些由JavaScript渲染出来的网页）。

爬虫主要解决以下几个问题：

#### 获取网页

构造一个请求并发送给服务器，然后接收到响应并将其解析出来。

我们可以用urllib、 requests 等库来帮助我们实现 HTTP请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的 Body 部分即可，即得到网页的源代码。
    
#### 提取信息

分析网页源代码，从中提取我们想要的数据。 

最通用的方法是采用 **正则表达式** 提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。 
使用 Beautiful Soup、 pyquery、 lxml 等库，我们可以高效快速地从中提 取网页信息，如节点的属性、文本值等。

#### 保存数据

将提取到的数据保存到某处以便后续使用。 

#### 自动化程序

让爬虫来代替人，完成上述这些操作。
并在抓取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效地运行。


## 会话 和 Cookies

HTTP 本身是*无状态*的，这是指 HTTP 协议对事物的处理是没有记忆能力的。
换言之，服务器 不知道 客户端是说明状态。

然而，我们日常接触的很多网页都可以实现用户登录等需要服务器*记住*客户端的操作。
实现这一类操作，就会用到 **会话** 和 **Cookies**。

#### 会话

**会话**，是一系列有始有终的动作/消息。例如，从拿起电话拨号到挂断 电话这中间的一系列过程可以称为一个会话。 

在 Web 中，**会话对象** 用来存储特定用户会话所需的属性及配置信息。

当用户在应用程序的 Web 页之间跳转时，存储在会话对象中的变量将不会丢失（如用户的登录状态）；
当用户请求来自应用程序的 Web 页时如果该用户还没有会话，则服务器将自动创建一个会话对象 ；
当会话过期或被放弃后，服务器将终止该会话；

#### Cookies

Cookies指某些网站为了辨别用户身份、进行会话跟踪而存储在用户本地终端上的数据。

服务器可以通过 Cookies 得知 那个客户端对应着那个会话、那个用户。
在 **客户端** 的 *Cookies* 和 **服务端** 的 *会话* 的协作下，我们便可以实现会话的维持。

Cookie有如下几个属性 ：

* `Name`: 该 Cookie 的名称。一旦创建，该名称便不可更改。 
* `Value`: 该 Cookie 的值。
    * 如果值为 `Unicode` 字符，需要为字符编码。
    * 如果值为二进制数据，则需要使用 `BASE64` 编码。

* `Domain`: 可以访问该 Cookie的域名。例如，如果设置为 `.zhihu.com`，则所有以 `zhihu.com` 结尾的域名都可以访问该 Cookie。
* `MaxAge`: 该 Cookie失效的时间，单位为秒，也常和 `Expires` 一起使用，通过它可以计算有效时间。
    * MaxAge 如果为正数，则该Cookie在MaxAge秒之后失效。
    * 如果为负数，则关闭 浏览器时 Cookie 即失效，浏览器也不会以任何形式保存该 Cookie。  
* `Path` : 该 Cookie 的使用路径。
如果设置为 `/path/`，则只有路径为 `/path/` 的页面可以访问该 Cookie。
如果设置为 `/` 则本域名下的所有页面都可以访问该 Cookie。
* `Size`字段: 此 Cookie 的大小。
* `HTTP` 字段: Cookie 的 httponly 属性。
    * 若此属性为 true，则只有在 HTTP 头中会带有此 Cookie的信息，而不能通过 document.cookie来访问此 Cookie。
* `Secure`: 该 Cookie 是否仅被使用安全协议传输。安全协议有 HTTPS 和 SSL 等，在网络上传输数据之前先将数据加密。默认为 false。 

## 代理的基本原理

在我们正常请求一个网站时，我们直接把请求发给了Web服务器，Web服务器又直接把响应回传给我们。

而如果我们使用 **代理服务**，就等于是多了一个中间人，即*代理服务器*，这个过程变成了：
我们直接把请求发给 代理服务器，代理服务器 把响应回传给 代理服务器，代理服务器再把这个消息传给我们。

通过代理，我们就可以伪装IP，防制在爬虫时被查出来封IP或事需要提供验证码，影响爬虫运行。

⚠️【注意】并不是所有的 代理 都可以让我们隐藏 IP！
有的 代理服务 会在向 web服务器 的请求 时，不会原封不动的转发我们的消息，而是在其中添加上我们的真实IP等信息，这时就隐藏不了了。