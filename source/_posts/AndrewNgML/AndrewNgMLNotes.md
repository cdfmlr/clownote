---
title: Andrew Ng æœºå™¨å­¦ä¹ ç¬”è®°æ€»ç»“
tags: Machine Learning
categories:
  - Machine Learning
  - AndrewNg
date: 2020-01-16 12:39:27
---


# æœºå™¨å­¦ä¹ 

Emmmmï¼Œè¿™å­¦æœŸåœ¨ Coursera å­¦å®Œäº† Andrew Ng çš„ Machine Learning è¯¾ç¨‹ã€‚æˆ‘å¯¹è¿™ä¸ªè¯¾ç¨‹ä¸€å‘æ˜¯ä¸ä»¥ä¸ºæ„çš„ï¼Œå´ä¸å°å¿ƒæŠ¥äº†ä¸ªåï¼Œè¿˜æ‰‹è´±ç”³è¯·äº†ä¸ªç»æµæ´åŠ©ï¼Œå­¦å®Œå°±å¯ä»¥å…è´¹æ‹¿è¯ä¹¦ï¼ˆå–å‡ ç™¾å—å“’ï¼‰ï¼Œè¯¾ç¨‹æœŸé—´è¿˜é€æ­£ç‰ˆçš„ Matlab Onlineï¼Œè¿™ä¸€ç³»åˆ—çš„å¶(å )ç„¶(å°)äº‹(ä¾¿)ä»¶(å®œ)ä¿ƒä½¿æˆ‘å¼€å§‹åˆ·è¿™ä¸ªè¯¾äº†ã€‚è¶Šå­¦è¶Šè§‰å¾—ï¼Œå—¯ï¼ŒçœŸé¦™ï¼Œæ˜¯çœŸçš„å¾ˆé¦™ï¼è¿™ä¸ªè¯¾çœŸçš„æ˜¯å¾ˆå¥½çš„æœºå™¨å­¦ä¹ å…¥é—¨ï¼Œéš¾æ€ªé‚£ä¹ˆå¤šäººæ¨èã€‚

Coursera é‡Œè¯¾ç¨‹ç¬”è®°æœ‰æ¯ä¸€ç« çš„æ€»ç»“ï¼Œæ€»ç»“çš„éå¸¸å¥½ï¼Œæ¨èå­¦å®Œä¹‹åçœ‹ä¸€çœ‹ã€‚ä½†æˆ‘è¿˜æ˜¯å–œæ¬¢è‡ªå·±å†™è‡ªå·±çš„ï¼Œæ‰€ä»¥æˆ‘ä¹‹å‰è¾¹çœ‹è§†é¢‘è¾¹å†™äº†å‡ ä¹æ¶µç›–æ•´ä¸ªè¯¾ç¨‹çš„[ç¬”è®°](https://clownote.github.io/categories/Machine-Learning/AndrewNg/)ï¼Œå…¶å®å¥½å¤šæ˜¯åœ¨æŠ„è€å¸ˆçš„åŸè¯å’ŒPPTğŸ˜‚ï¼Œå°±å½“ç»ƒä¹ æ‰“å­—ã€è‹±è¯­è¿˜æœ‰ $\LaTeX$ äº†ã€‚æ”¾å‡å›å®¶åœ¨ç«è½¦ä¸Š~~ç™¾æ— èŠèµ–~~å¿ƒè¡€æ¥æ½®ï¼Œæƒ³åˆ°äº†åº”è¯¥æ•´ç†ä¸€ä¸‹è¯¾ç¨‹é‡Œé¢å­¦åˆ°çš„ä¸œè¥¿ï¼Œå°±æœ‰äº†è¿™ç¯‡æ–‡ç« ã€‚

è¿™é‡Œæˆ‘ä¸»è¦æ˜¯å†™äº†å„ç§ç®—æ³•çš„æè¿°ï¼Œè¿˜ä»ç¼–ç¨‹ä½œä¸šé‡Œæå–äº†ç®—æ³•å¤§æ¦‚çš„ä»£ç å®ç°ï¼Œæ–¹ä¾¿æ—¥åå¿«é€ŸæŸ¥é˜…å§ã€‚ä¸€å¼€å§‹çš„å›å½’æ¯”è¾ƒç®€å•ï¼Œæ‰€ä»¥æˆ‘å†™çš„å¾ˆå°‘ï¼Œå°±å †äº†ç‚¹å…¬å¼ï¼ˆå…¶å®æ˜¯ç¡¬å§ä¸Šé“ºç©ºè°ƒå¤ªå†·è‡´ä½¿æˆ‘ç”Ÿç—…äº†ï¼Œæ€è·¯å µå¡å†™ä¸å‡ºä¸œè¥¿æ¥ğŸ˜·ï¼‰ï¼›åé¢SVMã€æ¨èç³»ç»Ÿä»€ä¹ˆçš„æ¯”è¾ƒå¤æ‚å°±å¤šå†™äº†ä¸€äº›ï¼ˆå…¶å®æ˜¯æˆ‘æŒæ¡çš„ä¸å¥½ï¼Œå½’çº³ä¸å‡ºé‡ç‚¹ğŸ¤¯ï¼‰ã€‚è‡³äºè¯¾ç¨‹é‡Œè€å¸ˆèŠ±å¤§åŠ›æ°”è®²çš„å…³äºæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„è®¾è®¡ã€ä¼˜åŒ–ã€debug è¿˜æœ‰å„ç§~~å¥‡æŠ€æ·«å·§~~ ~~éªšæ“ä½œ~~ å®ç”¨æŠ€å·§ ä»¥åŠ Octave å…¥é—¨å“ªä¸€å—æˆ‘å°±ä¸€æ¦‚ä¸æäº†ï¼Œè¿™äº›ä¸œè¥¿è¿˜æ˜¯è¦çœ‹è€å¸ˆçš„è§†é¢‘æ‰èƒ½ä½“ä¼šåˆ°ç²¾é«“ï¼ˆ...ä¼˜(æŒ‚)ç§€(ç§‘)çš„å¤§å­¦ç”Ÿè‡ªç„¶æ˜¯ä¸èƒ½æ‰¿è®¤åŸå› æ˜¯è‡ªå·±æ‡’æƒ°çš„ğŸ˜ï¼‰ã€‚

ç”±äºæˆ‘çœ‹è¯¾ç¨‹çš„æ—¶å€™å…¨ç¨‹æ²¡æœ‰å¼€ä¸­æ–‡å­—å¹•ï¼Œå¹³æ—¶æŸ¥é˜…çš„ä¸­æ–‡èµ„æ–™ä¹Ÿæ¯”è¾ƒå°‘ï¼Œæ‰€ä»¥å¥½å¤šæœ¯è¯­æˆ‘éƒ½ä¸çŸ¥é“è¦æ€ä¹ˆç¿»è¯‘ï¼Œå†™è¿™ç¯‡æ–‡ç« çš„æ—¶å€™æˆ‘å¤§æ¦‚æŸ¥äº†ä¸€äº›è‡ªå·±éš¾ä»¥è¡¨è¾¾çš„ï¼Œå…¶ä»–çš„å…¨é è‡†æµ‹ï¼Œæˆ‘ä¸ä¿è¯æ­£ç¡®ã€‚

Emmmï¼Œä¸å°å¿ƒå°±å†™äº†å‡ ç™¾å­—çš„åºŸè¯ğŸ˜‚ä¸‹é¢å°±å¼€å§‹å§ã€‚



## ç›‘ç£å­¦ä¹ 

ç›‘ç£å­¦ä¹ æ˜¯ç»™xã€yæ•°æ®å»è®­ç»ƒçš„ã€‚

![image-20191105144318970](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n5s7zmffj30lv0ca40u.jpg)

### å›å½’é—®é¢˜

> åšé¢„æµ‹ï¼Œå€¼åŸŸä¸ºè¿ç»­çš„æ•°ï¼ˆä¾‹å¦‚åŒºé—´$[0,100]$ï¼‰

æ•°å­¦æ¨¡å‹ï¼š

* **é¢„æµ‹å‡½æ•°**ï¼š
$$
  h_\theta(x)
  =
  \sum_{i=0}^m \theta_ix_i
  =
  \left[\begin{array}{c}\theta_0 & \theta_1 & \ldots & \theta_n\end{array}\right]
  \left[\begin{array}{c}x_0 \\ x_1 \\ \vdots \\ x_n \end{array}\right]
  =
  \theta^TX
$$

* **å¾…æ±‚å‚æ•°**ï¼š$\theta=\left[\begin{array}{c}\theta_0 & \theta_1 & \ldots & \theta_n\end{array}\right]$

* **ä»£ä»·å‡½æ•°**ï¼š
$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h(x^{(i)})-y^{(i)})^2
  =\frac{(\sum_{i=1}^mh_\theta(X)-Y)^2}{2m}
$$

ğŸ‘‰ä»£ç å®ç°ï¼š

```matlab
  function J = computeCostMulti(X, y, theta)
  %COMPUTECOSTMULTI Compute cost for linear regression with multiple variables
  %   J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the
  %   parameter for linear regression to fit the data points in X and y
  
  m = length(y); % number of training examples
  
  J = 0;
  
  J = 1 / (2*m) * (X*theta - y)' * (X*theta - y);
  
  % or less vectorizedly: 
  % predictions = X * theta;
  % sqrErrors = (predictions - y) .^ 2;
  % J = 1 / (2*m) * sum(sqrErrors);
  
  end
```

* **ä¼˜åŒ–ç›®æ ‡**ï¼šæ‰¾åˆ°ä¸€ç»„$\theta$ä½¿$J$æœ€å°ã€‚

æ±‚è§£æ–¹æ³•ï¼š

#### æ¢¯åº¦ä¸‹é™

ï¼ˆè¿™é‡Œæš‚ä¸”åªè®¨è®º batch gradient descentï¼‰

$$
\begin{array}{ll}
\textrm{repeat until convergence } \{ \\
\qquad \theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\\
\qquad\quad:= \theta_j - \alpha \frac{1}{m} \sum^m_{i=1}[h_\theta(x^{(i)})-y^{(i)}] \cdot x_j^{(i)}\qquad \textrm{for }j:=0, ..., n \\
\}
\end{array}
$$

å‘é‡åŒ–è¡¨ç¤ºï¼š$\theta=\theta-\frac{\alpha}{m}X^T(h_\theta(X)-Y)$

ğŸ‘‰ä»£ç å®ç°ï¼š

```matlab
function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)
%GRADIENTDESCENTMULTI Performs gradient descent to learn theta
%   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by
%   taking num_iters gradient steps with learning rate alpha

m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters

    predictions = X * theta;
    errors = (predictions - y);
    theta = theta - alpha / m * (X' * errors);
    
    % Save the cost J in every iteration    
    J_history(iter) = computeCostMulti(X, y, theta);

end

end
```

#### æ­£è§„æ–¹ç¨‹

$$
\theta = (X^TX)^{-1}X^Ty
$$
ğŸ‘‰ä»£ç å®ç°ï¼š

```matlab
function [theta] = normalEqn(X, y)
%NORMALEQN Computes the closed-form solution to linear regression 
%   NORMALEQN(X,y) computes the closed-form solution to linear 
%   regression using the normal equations.

theta = zeros(size(X, 2), 1);

theta = pinv(X' * X) * X' * y;

end
```

è¿™é‡Œæˆ‘ä»¬æ±‚ä¼ªé€†ä»¥ç¡®ä¿æ­£å¸¸è¿è¡Œã€‚é€šå¸¸é€ æˆ$X^TX$ä¸å¯é€†çš„åŸå› æ˜¯ï¼š

1. å­˜åœ¨å¯çº¦ç‰¹å¾ï¼Œå³ç»™å®šçš„æŸä¸¤/å¤šä¸ªç‰¹å¾çº¿æ€§ç›¸å…³ï¼Œåªä¿ç•™ä¸€ä¸ªåˆ é™¤å…¶ä»–å³å¯è§£å†³ã€‚ï¼ˆe.g. there are the size of house in feet^2 and the size of house in meter^2, where we know that 1 meter = 3.28 feetï¼‰
2. ç»™å®šç‰¹å¾è¿‡å¤šï¼Œ($m \le n$). å¯ä»¥åˆ é™¤ä¸€äº›ä¸é‡è¦çš„ç‰¹å¾ï¼ˆè€ƒè™‘PCAç®—æ³•ï¼‰

#### æ¢¯åº¦ä¸‹é™vsæ­£è§„æ–¹ç¨‹

| | Gradient Descent           | Normal Equation                                           |
| -- | -------------------------- | --------------------------------------------------------- |
| éœ€è¦é€‰æ‹© alpha | âœ…            | -                                                         |
| ç¬¬ä¸‰æ–¹ | âœ…     | -                                                         |
| æ—¶é—´å¤æ‚åº¦     |$O(kn^2)$      | æ±‚$X^TX$çš„ä¼ªé€†éœ€è¦$O(n^3)$ |
| n ç›¸å½“å¤§æ—¶ | å¯ä»¥å·¥ä½œ         | ååˆ†ç¼“æ…¢ç”šè‡³ä¸å¯è®¡ç®—                       |

å®é™…ä¸Šï¼Œå½“ $n>10,000$ æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸æ›´å€¾å‘äºä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼Œå¦åˆ™æ­£è§„æ–¹ç¨‹ä¸€èˆ¬éƒ½è¡¨ç°å¾—æ›´å¥½ã€‚

#### æ³¨ï¼šç‰¹å¾ç¼©æ”¾

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿è¾“å…¥å€¼å¤§æ¦‚åœ¨ä¸€å®šçš„èŒƒå›´å†…æ¥ä½¿æ¢¯åº¦ä¸‹é™è¿è¡Œæ›´å¿«ï¼Œæ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ‰€æœ‰å€¼å˜åˆ° $[-1,1]$ çš„èŒƒå›´å†…ï¼ŒåŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡å¤„ç†è®©è¾“å…¥å€¼ä¹‹é—´çš„å·®è·ä¸è¦å¤ªå¤§ï¼ˆä¾‹å¦‚ï¼Œè¾“å…¥å€¼ä¸­åŒæ—¶æœ‰ 0.000001 å’Œ 1 è¿™æ ·å·®è·å¤§çš„å€¼ä¼šå½±å“æ¢¯åº¦ä¸‹é™çš„æ•ˆç‡ï¼‰ã€‚

åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æƒ³è¦ä¿è¯å˜é‡å€¼åœ¨ $[-3,-\frac{1}{3}) \cup (+\frac{1}{3}, +3]$ è¿™ä¸ªèŒƒå›´å†…å–å€¼ã€‚

ä¸ºè¾¾æˆè¯¥ç›®æ ‡ï¼Œæˆ‘ä»¬åšå¦‚ä¸‹æ“ä½œï¼š

1. Feature scaling

$$
\begin{array}{rl}
\textrm{Range:} & s_i = max(x_i)-min(x_i)\\
\textrm{Or Range:} & s_i = \textrm{standard deviation of } x_i\\
\textrm{Scaling:} & x_i:=\frac{x_i}{s_i}
\end{array}
$$

2. Mean normalizaton

$$
\begin{array}{rl}
\textrm{Mean(Average):} & \mu_i = \frac{sum(x_i)}{m}\\
\textrm{normalizing:} & x_i:=x_i-\mu_i
\end{array}
$$

æŠŠä¸¤ä¸ªæ“ä½œå’Œåœ¨ä¸€èµ·ï¼Œå³ï¼š
$$
x_i:=\frac{x_i-\mu_i}{s_i}
$$
å…¶ä¸­ï¼Œ$\mu_i$ æ˜¯ç‰¹å¾(i)çš„å€¼çš„å¹³å‡ï¼Œ$s_i$æ˜¯å€¼çš„èŒƒå›´ã€‚

ğŸ‘‰ä»£ç å®ç°ï¼š

```matlab
function [X_norm, mu, sigma] = featureNormalize(X)
%FEATURENORMALIZE Normalizes the features in X 
%   FEATURENORMALIZE(X) returns a normalized version of X where
%   the mean value of each feature is 0 and the standard deviation
%   is 1. This is often a good preprocessing step to do when
%   working with learning algorithms.

X_norm = X;
mu = zeros(1, size(X, 2));
sigma = zeros(1, size(X, 2));

mu = mean(X);
sigma = std(X);
X_norm = (X - mu) ./ sigma;

end
```



### åˆ†ç±»é—®é¢˜

> åšé¢„æµ‹ï¼Œå€¼åŸŸä¸ºç¦»æ•£çš„å‡ ä¸ªç‰¹å®šå€¼ï¼ˆä¾‹å¦‚ 0 æˆ– 1ï¼›0/1/2/3ï¼‰

#### é€»è¾‘å›å½’

å‡è®¾å‡½æ•°ï¼š
$$
\left\{\begin{array}{l}
h_\theta(x) = g(\theta^Tx)\\
z = \theta^T x\\
g(z) = \frac{1}{1+e^{-z}}\\
\end{array}\right.
$$
å…¶ä¸­ï¼Œ$g(z)$ ç§°ä¸º Simoid å‡½æ•°ï¼Œæˆ–é€»è¾‘å‡½æ•°ï¼Œå…¶å›¾åƒå¦‚ä¸‹ï¼š

![image-20190917162504420](https://tva1.sinaimg.cn/large/006tNbRwly1gauxvtzb5qj30ur0g8wgd.jpg)

ğŸ‘‰ä»£ç å®ç°ï¼š

```matlab
function g = sigmoid(z)
%SIGMOID Compute sigmoid functoon
%   J = SIGMOID(z) computes the sigmoid of z.

g = 1.0 ./ (1.0 + exp(-z));
end
```


ä¸Šå¼å¯åŒ–ç®€å¾—ï¼š
$$
h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}
$$

$h_\theta$ çš„è¾“å‡ºæ˜¯é¢„æµ‹å€¼ä¸º1çš„å¯èƒ½æ€§ï¼Œå¹¶æœ‰ä¸‹ä¸¤å¼æˆç«‹ï¼š
$$
h_\theta(x)=P(y=1 \mid x;\theta)=1-P(y=0 \mid x; \theta)
$$

$$
P(y=0 \mid x;\theta) + P(y=1 \mid x;\theta) = 1
$$

**å†³ç­–è¾¹ç•Œ**ï¼šé€»è¾‘å›å½’çš„å†³ç­–è¾¹ç•Œå°±æ˜¯å°†åŒºåŸŸåˆ†æˆ$y=0$å’Œ $y=1$ ä¸¤éƒ¨åˆ†çš„ä¸€ä¸ªè¶…å¹³é¢ã€‚

å†³ç­–è¾¹ç•Œç”±å‡è®¾å‡½æ•°å†³å®šã€‚è¿™æ˜¯ç”±äºè¦å®Œæˆåˆ†ç±»ï¼Œéœ€ç”¨$h_\theta$çš„è¾“å‡ºæ¥å†³å®šç»“æœæ˜¯ 0 è¿˜æ˜¯ 1ã€‚å®š 0.5 ä¸ºåˆ†ç•Œï¼Œå³ï¼š
$$
\begin{array}{rcl}
h_\theta(x) \ge 0.5 &\Rightarrow& y=1\\
h_\theta(x) < 0.5 &\Rightarrow& y=0
\end{array}
$$
ç”± Simoid å‡½æ•°çš„æ€§è´¨ï¼Œä¸Šå¼ç­‰ä»·ä¸ºï¼š
$$
\begin{array}{rcl}
\theta^TX \ge 0 &\Rightarrow& y=1\\
\theta^TX \le 0 &\Rightarrow& y=0\\
\end{array}
$$
é‚£ä¹ˆå¯¹äºç»™å®šçš„ä¸€ç»„ $\theta$ï¼Œä¾‹å¦‚$\theta=\left[\begin{array}{c}5\\-1\\0\end{array} \right]$ï¼Œæœ‰ $y=1$ å½“ä¸”ä»…å½“ $5+(-1)x_1+0x_2 \ge 0$ï¼Œè¿™æ—¶å†³ç­–è¾¹ç•Œä¸º $x_1=5$ã€‚

![image-20190917173722734](https://tva1.sinaimg.cn/large/006tNbRwly1gauyq6oc10j306e04umx0.jpg)

å†³ç­–è¾¹ç•Œä¹Ÿå¯ä»¥æ˜¯ä¸‹é¢è¿™ç§å¤æ‚çš„æƒ…å†µï¼š

![image-20190917174144868](https://tva1.sinaimg.cn/large/006tNbRwly1gauyt0wbq3j30mv08xwfh.jpg)

**é€»è¾‘å›å½’æ¨¡å‹**ï¼š
$$
\begin{array}{rcl}
\textrm{Training set} &:& \{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \ldots, (x^{(m)},y^{(m)})\}\\
\\
\textrm{m examples} &:&
x \in \left[\begin{array}{c}
x_0\\x_1\\ \vdots \\ x_n
\end{array}\right] \textrm{where }(x_0=1)
,\quad y \in \{0,1\}\\
\\
\textrm{Hypothesis} &:& h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}\\\\
\textrm{Cost Function} &:&
J(\theta)=-\frac{1}{m}\sum_{i=1}^m\Bigg[y^{(i)}log\Big(h_\theta(x)\Big)+(1-y^{(i)})log\Big(1-h_\theta(x^{(i)})\Big)\Bigg]
\end{array}
$$
å‘é‡åŒ–è¡¨ç¤ºï¼š
$$
\begin{array}{l}
h=g(X\theta)\\
J(\theta)=\frac{1}{m}\cdot\big(-y^T log(h) -(1-y)^T log(1-h)\big)
\end{array}
$$
**æ¢¯åº¦ä¸‹é™**ï¼š
$$
\begin{array}{l}
Repeat \quad \{\\
\qquad \theta_j:=\theta_j-\frac{\alpha}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}\\
\}
\end{array}
$$
å‘é‡åŒ–è¡¨ç¤ºï¼š
$$
\theta:=\theta-\frac{\alpha}{m}X^T(g(X\theta)-\overrightarrow{y})
$$
ğŸ‘‰ä»£ç å®ç°ï¼ˆä½¿ç”¨Advanced Optimizationï¼‰ï¼š

1. æä¾›$J(\theta), \frac{\partial}{\partial\theta_j}J(\theta)$

```matlab
function [J, grad] = costFunction(theta, X, y)
%COSTFUNCTION Compute cost and gradient for logistic regression
%   J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the
%   parameter for logistic regression and the gradient of the cost
%   w.r.t. to the parameters.

m = length(y); % number of training examples

J = 0;
grad = zeros(size(theta));

h = sigmoid(X*theta);

J = 1/m * (-y'*log(h) - (1-y)'*log(1-h));

grad = 1/m * X'*(h-y);

end
```

2. è°ƒç”¨ Advanced Optimization å‡½æ•°è§£å†³ä¼˜åŒ–é—®é¢˜ï¼š

```matlab
options = optimset('GradObj', 'on', 'MaxIter', 100);
initialTheta = zeros(2, 1);

[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
```

##### å¤šå…ƒåˆ†ç±»

æˆ‘ä»¬é‡‡ç”¨ä¸€ç³»åˆ—çš„å•å…ƒï¼ˆé€»è¾‘ï¼‰åˆ†ç±»æ¥å®Œæˆå¤šå…ƒåˆ†ç±»ï¼š
$$
\begin{array}{l}
y \in \{0,1,\cdots,n\}\\\\
h_\theta^{(0)}(x)=P(y=0|x;\theta)\\
h_\theta^{(0)}(x)=P(y=0|x;\theta)\\
\vdots\\
h_\theta^{(0)}(x)=P(y=0|x;\theta)\\\\
prediction = \mathop{max}\limits_{\theta}\big(h_\theta^{(i)}(x)\big)
\end{array}
$$

![img](https://tva1.sinaimg.cn/large/006y8mN6ly1g78nj8fvy4j30d507agmp.jpg)

#### æ³¨ï¼šè¿‡æ‹Ÿåˆ

![img](https://tva1.sinaimg.cn/large/006y8mN6ly1g7aopbltb1j30f0046dg2.jpg)

è¿‡æ‹Ÿåˆå¯¹è®­ç»ƒé›†ä¸­çš„æ•°æ®é¢„æµ‹çš„å¾ˆå¥½ï¼Œä½†å¯¹æ²¡è§è¿‡çš„æ–°æ ·æœ¬é¢„æµ‹æ•ˆæœä¸ä½³ã€‚

è§£å†³è¿‡æ‹Ÿåˆçš„æ–¹æ³•æœ‰ï¼š

1. å‡å°‘ç‰¹å¾æ•°é‡ï¼ˆPCAï¼‰

2. æ­£åˆ™åŒ–ï¼šåœ¨ä»£ä»·å‡½æ•°ä¸­åŠ å…¥ $\theta$ çš„æƒé‡ï¼š

   $\mathop{min}\limits_{\theta} \dfrac{1}{2m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2$

   > æ³¨æ„ï¼Œ$\theta_0$æ˜¯æˆ‘ä»¬åŠ ä¸Šçš„å¸¸æ•°é¡¹ï¼Œä¸åº”è¯¥è¢«æ­£åˆ™åŒ–ã€‚

   ä»£ç å®ç°ï¼š

```matlab
   function [J, grad] = lrCostFunction(theta, X, y, lambda)
   %LRCOSTFUNCTION Compute cost and gradient for logistic regression with 
   %regularization
   %   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using
   %   theta as the parameter for regularized logistic regression and the
   %   gradient of the cost w.r.t. to the parameters. 
   
   m = length(y); % number of training examples
   
   J = 0;
   grad = zeros(size(theta));
   
   % Unregularized cost function & gradient for logistic regression
   h = sigmoid(X * theta);
   J = 1/m * (-y'*log(h) - (1-y)'*log(1-h));
   grad = 1/m * X'*(h-y);
   
   % Regularize
   temp = theta;
   temp(1) = 0;
   J = J + lambda/(2*m) * sum(temp.^2);
   grad = grad + lambda/m * temp;
   
   grad = grad(:);
   
   end
```

#### ç¥ç»ç½‘ç»œ

$$
\left[\begin{array}{c}x_0 \\ x_1 \\ x_2 \\ x_3\end{array}\right]
\rightarrow
\left[\begin{array}{c}a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)} \\ \end{array}\right]
\rightarrow
\left[\begin{array}{c}a_1^{(3)} \\ a_2^{(3)} \\ a_3^{(3)} \\ \end{array}\right]
\rightarrow 
h_\theta(x)
$$

ç¬¬ä¸€å±‚æ˜¯æ•°æ®é›†ï¼Œç§°ä¸ºè¾“å…¥å±‚ï¼Œå¯ä»¥çœ‹ä½œ $a^{(0)}$ ï¼›ä¸­é—´æ˜¯æ•°ä¸ªéšè—å±‚ï¼Œæœ€ç»ˆå¾—åˆ°çš„å°±æ˜¯é¢„æµ‹å‡½æ•°ï¼Œè¿™ä¸€å±‚å«åšè¾“å‡ºå±‚ã€‚

$$
z^{(j)} = \Theta^{(j-1)}a^{(j-1)}
$$

$$
a^{(j)} = g(z^{(j)})
$$

å‡è®¾æœ‰ c ä¸ªå±‚ï¼Œåˆ™:

$$
h_\Theta(x)=a^{(c+1)}=g(z^{(c+1)})
$$

ä¾‹å¦‚ï¼Œç”¨ä¸€å±‚çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥å»ºç«‹ä¸€äº›è¡¨è¾¾é€»è¾‘å‡½æ•°çš„ç¥ç»ç½‘ç»œï¼š
$$
\begin{array}{l}AND:\\&\Theta^{(1)} &=\begin{bmatrix}-30 & 20 & 20\end{bmatrix} \\ NOR:\\&\Theta^{(1)} &= \begin{bmatrix}10 & -20 & -20\end{bmatrix} \\ OR:\\&\Theta^{(1)} &= \begin{bmatrix}-10 & 20 & 20\end{bmatrix} \\\end{array}
$$

##### å¤šå…ƒåˆ†ç±»

$$
y^{(i)}=\begin{bmatrix}1\\0\\0\\0\end{bmatrix},\begin{bmatrix}0\\1\\0\\0\end{bmatrix},\begin{bmatrix}0\\0\\1\\0\end{bmatrix},\begin{bmatrix}0\\0\\0\\1\end{bmatrix}
$$

$$
\left[\begin{array}{c}x_0 \\ x_1 \\ x_2 \\ x_3\end{array}\right]
\rightarrow
\left[\begin{array}{c}a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)} \\ ... \end{array}\right]
\rightarrow
\left[\begin{array}{c}a_1^{(3)} \\ a_2^{(3)} \\ a_3^{(3)} \\ ... \end{array}\right]
\rightarrow 
\cdots
\rightarrow 
\left[\begin{array}{c}h_\Theta(x)_1 \\ h_\Theta(x)_2 \\ h_\Theta(x)_3 \\ h_\Theta(x)_4 \end{array}\right]
$$

##### ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆ

| Notation | Represent                              |
| -------- | -------------------------------------- |
| $L$      | ç¥ç»ç½‘ç»œä¸­çš„æ€»å±‚æ•°                     |
| $s_l$    | ç¬¬$l$å±‚ä¸­çš„èŠ‚ç‚¹æ•°ï¼ˆä¸ç®—åç§»å•å…ƒ$a_0$ï¼‰ |
| $K$      | è¾“å‡ºèŠ‚ç‚¹æ•°                             |

**ä»£ä»·å‡½æ•°**ï¼š
$$
J(\Theta)=-\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}\Big[
y_k^{(i)}log\Big(\big(h_\Theta(x^{(i)})\big)_k\Big)+
(1-y_k^{(i)})log\Big(1-\big(h_\Theta(x^{(i)})\big)_k\Big)
\Big]+
\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}\Big(\Theta_{j,i}^{(l)}\Big)^2
$$
**å‘åä¼ æ’­ç®—æ³•**ï¼š
$$
\begin{array}{lll}
\textrm{Give training set }{(x^{(1)},y^{(1)}),...,(x^{(m)},y^{(m)})}\\
\textrm{Set }\Delta_{i,j}^{(l)}:=0\textrm{ for each } l,i,j \textrm{ (get a matrix full of zeros)}\\
\mathop{\textrm{For}} \textrm{ training example $t=1$ to $m$}:\\
\qquad a^{(1)}:= x^{(t)}\\
\qquad \textrm{Compute $a^{(l)}$ for $l=2,3,\cdots,L$ by forward propagation}\\
\qquad \textrm{Using $y^{(t)}$ to compute } \delta^{(L)}=a^{(L)}-y^{(t)}\\
\qquad \textrm{Compute } \delta^{(l)}=\big((\Theta^{(l)})^T\delta^{(l+1)}\big).*a^{(l)}.*(1-a^{(l)}) \textrm{ for } \delta^{(L-1)},\delta^{(L-2)},...,\delta^{(2)}\\
\qquad \Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T\\
\textrm{End For}\\
D_{i,j}^{(l)}:=\frac{1}{m}\Delta_{i,j}^{(l)}\textrm{ if } j=0\\
D_{i,j}^{(l)}:=\frac{1}{m}\big(\Delta_{i,j}^{(l)}+\lambda\Theta_{i,j}^{(l)}\big) \textrm{ if } j\neq 0 \\
\textrm{Get }
\frac{\partial}{\partial\Theta_{i,j}^{(l)}}J(\Theta)=D_{i,j}^{(l)}
\end{array}
$$
æ³¨ï¼šä¸Šå¼ä¸­ $.*$ ä»£è¡¨ Matlab/Octave ä¸­çš„ element-wise çš„ä¹˜æ³•ã€‚

**å‘åä¼ æ’­çš„ä½¿ç”¨**ï¼š

å…ˆçœ‹å‡ ä¸ªæ¶‰åŠåˆ°çš„æ–¹æ³•ï¼š

* å‚æ•°å±•å¼€ï¼šä¸ºä½¿ç”¨ä¼˜åŒ–å‡½æ•°ï¼Œæˆ‘ä»¬éœ€è¦æŠŠæ‰€æœ‰çš„$\Theta$çŸ©é˜µå±•å¼€å¹¶æ‹¼æ¥æˆä¸€ä¸ªé•¿å‘é‡ï¼š

```matlab
thetaVector = [ Theta1(:); Theta2(:); Theta3(:) ];
deltaVector = [ D1(:); D2(:); D3(:) ];
```

åœ¨å¾—åˆ°ä¼˜åŒ–ç»“æœåè¿”å›åŸæ¥çš„çŸ©é˜µï¼š

```matlab
Theta1 = reshape(thetaVector(1:110),10,11)
Theta2 = reshape(thetaVector(111:220),10,11)
Theta3 = reshape(thetaVector(221:231),1,11)
```

* æ¢¯åº¦æ£€æŸ¥ï¼šåˆ©ç”¨ $\frac{\partial}{\partial\Theta_j}J(\Theta) \approx \frac{J(\Theta_1,...,\Theta_j+\epsilon,...,\Theta_n)-J(\Theta_1,...,\Theta_j-\epsilon,...,\Theta_n)}{2\epsilon}$ å–ä¸€ä¸ªå°çš„é‚»åŸŸå¦‚ $\epsilon=10^{-4}$ï¼Œå¯ä»¥æ£€æŸ¥æˆ‘ä»¬ç”¨å‘åä¼ æ’­æ±‚å‡ºçš„æ¢¯åº¦æ˜¯å¦æ­£ç¡®ï¼ˆè‹¥æ­£ç¡®ï¼Œæœ‰ gradApprox â‰ˆ deltaVector æˆç«‹ï¼‰ã€‚ä»£ç å®ç°ï¼š

```matlab
epsilon = 1e-4;
for i = 1 : n
	thetaPlus = theta;
	thetaPlus(i) += epsilon;
	thetaMinus = theta;
	thetaMinus(i) += epsilon;
	gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) / (2*epsilon);
end
```

* éšå³åˆå§‹åŒ–ï¼šåœ¨å¼€å§‹æ—¶ï¼Œå°† $\Theta_{ij}^{(l)}$ éšæœºåˆå§‹åŒ–ï¼Œåº”ä¿è¯éšæœºå€¼çš„å–å€¼åœ¨ä¸€ä¸ª $[-\epsilon,\epsilon]$ çš„èŒƒå›´å†…ï¼ˆè¿™ä¸ª $\epsilon$ ä¸æ¢¯åº¦æ£€æŸ¥ä¸­çš„æ— å…³ï¼‰ã€‚ä»£ç å®ç°ï¼š

```matlab
# If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.

Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
```

å°†ä¸Šè¿°æŠ€å·§ä¸å‘åä¼ æ’­ç®—æ³•ç»“åˆï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†äº†è®­ç»ƒç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼š

1. éšæœºåˆå§‹åŒ–
2. å‘å‰ä¼ æ’­å¾—åˆ° $h_\Theta(x^{(i)})$ å¯¹ä»»æ„ $x^{(i)}$
3. è®¡ç®—ä»£ä»·å‡½æ•°
4. ä½¿ç”¨å‘åä¼ æ’­è®¡ç®—åå¯¼
5. åˆ©ç”¨æ¢¯åº¦æ£€æŸ¥éªŒè¯å‘åä¼ æ’­æ˜¯å¦æ­£ç¡®ï¼Œè‹¥æ²¡é—®é¢˜åˆ™å…³é—­æ¢¯åº¦æ£€æŸ¥åŠŸèƒ½
6. ä½¿ç”¨æ¢¯åº¦ä¸‹é™æˆ–ä¼˜åŒ–å‡½æ•°å¾—åˆ°$\Theta$

ğŸ‘‰ä»£ç å®ç°ï¼š

1. éšæœºåˆå§‹åŒ–

```matlab
function W = randInitializeWeights(L_in, L_out)
%RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in
%incoming connections and L_out outgoing connections
%   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights 
%   of a layer with L_in incoming connections and L_out outgoing 
%   connections. 
%
%   Note that W should be set to a matrix of size(L_out, 1 + L_in) as
%   the first column of W handles the "bias" terms
%

W = zeros(L_out, 1 + L_in);

% epsilon_init = 0.12
epsilon_init = sqrt(6 / (L_in + L_out));
W = rand(L_out, 1 + L_in) * (2 * epsilon_init) - epsilon_init;

end
```

2. è®¡ç®—ä»£ä»·

```matlab
function [J grad] = nnCostFunction(nn_params, ...
                                   input_layer_size, ...
                                   hidden_layer_size, ...
                                   num_labels, ...
                                   X, y, lambda)
%NNCOSTFUNCTION Implements the neural network cost function for a two layer
%neural network which performs classification
%   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...
%   X, y, lambda) computes the cost and gradient of the neural network. The
%   parameters for the neural network are "unrolled" into the vector
%   nn_params and need to be converted back into the weight matrices. 
% 
%   The returned parameter grad should be a "unrolled" vector of the
%   partial derivatives of the neural network.
%

% Reshape nn_params back into the parameters Theta_1 and Theta_2, the weight matrices
% for our 2 layer neural network
Theta_1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
                 hidden_layer_size, (input_layer_size + 1));

Theta_2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
                 num_labels, (hidden_layer_size + 1));

% Setup some useful variables
m = size(X, 1);
K = num_labels;

J = 0;
Theta_1_grad = zeros(size(Theta_1));
Theta_2_grad = zeros(size(Theta_2));

% y(5000x1) -> Y(5000x10)
Y = zeros(m, K);
for i = 1 : m
    Y(i, y(i)) = 1;
end

% Feedforward
a_1 = X;
a_1_bias = [ones(m, 1), a_1];

z_2 = a_1_bias * Theta_1';
a_2 = sigmoid(z_2);
a_2_bias = [ones(m, 1), a_2];

z_3 = a_2_bias * Theta_2';
a_3 = sigmoid(z_3);
h = a_3;

% Cost Function
% for i = 1 : K
%     yK = Y(:, i);
%     hK = h(:, i);
%     J += 1/m * (-yK'*log(hK) - (1-yK)'*log(1-hK));

% J can be get by element-wise compute more elegantly.
J = 1/m * sum(sum((-Y.*log(h) - (1-Y).*log(1-h))));

% Regularize
J = J + lambda/(2*m) * (sum(sum(Theta_1(:, 2:end).^2)) + sum(sum(Theta_2(:, 2:end).^2)));

% Backpropagation

delta_3 = a_3 .- Y;
delta_2 = (delta_3 * Theta_2) .* sigmoidGradient([ones(m, 1), z_2]);
% sigmoidGradient: return g = sigmoid(z) .* (1 - sigmoid(z));
delta_2 = delta_2(:, 2:end);

Delta_1 = delta_2' * a_1_bias;
Delta_2 = delta_3' * a_2_bias;

Theta_1_grad = Delta_1 ./ m + lambda/m * [zeros(size(Theta_1, 1), 1), Theta_1(:, 2:end)];
Theta_2_grad = Delta_2 ./ m + lambda/m * [zeros(size(Theta_2, 1), 1), Theta_2(:, 2:end)];

% Unroll gradients
grad = [Theta_1_grad(:) ; Theta_2_grad(:)];

end
```

3. é¢„æµ‹

```matlab
function p = predict(Theta1, Theta2, X)
%PREDICT Predict the label of an input given a trained neural network
%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the
%   trained weights of a neural network (Theta1, Theta2)

% Useful values
m = size(X, 1);
num_labels = size(Theta2, 1);

p = zeros(size(X, 1), 1);

h1 = sigmoid([ones(m, 1) X] * Theta1');
h2 = sigmoid([ones(m, 1) h1] * Theta2');
[dummy, p] = max(h2, [], 2);

end
```

4. é©±åŠ¨

```matlab
input_layer_size  = 400;  % 20x20 Input Images of Digits
hidden_layer_size = 25;   % 25 hidden units
num_labels = 10;          % 10 labels, from 1 to 10   
                          % (note that we have mapped "0" to label 10)

fprintf('\nInitializing Neural Network Parameters ...\n')

load('Xy.mat');

fprintf('\nTraining Neural Network... \n')

%  value to see how more training helps.
options = optimset('MaxIter', 1000);

lambda = 1;

% Create "short hand" for the cost function to be minimized
costFunction = @(p) nnCostFunction(p, ...
                                   input_layer_size, ...
                                   hidden_layer_size, ...
                                   num_labels, X, y, lambda);

% Now, costFunction is a function that takes in only one argument (the
% neural network parameters)
[nn_params, cost] = fmincg(costFunction, initial_nn_params, options);

% Obtain Theta1 and Theta2 back from nn_params
Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
                 hidden_layer_size, (input_layer_size + 1));

Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
                 num_labels, (hidden_layer_size + 1));

pred = predict(Theta1, Theta2, X);

fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == y)) * 100);
```

#### æ”¯æŒå‘é‡æœº

ä¼˜åŒ–ç›®æ ‡ï¼š
$$
\min_\theta 
C\sum_{i=1}^m \large[ y^{(i)} \mathop{\textrm{cost}_1}(\theta^Tx^{(i)})
+ (1 - y^{(i)})\ \mathop{\textrm{cost}_0}(\theta^Tx^{(i)})\large]
+ \frac{1}{2}\sum_{j=1}^n \theta_j^2
$$
å½“ $C$ å€¼æ¯”è¾ƒå¤§æ—¶ï¼Œè¿™ä¸ªä¼˜åŒ–ç›®æ ‡ä¼šé€‰æ‹©å°†ç¬¬ä¸€ä¸ªæ±‚å’Œé¡¹è¶‹äºé›¶ï¼Œè¿™æ ·ä¼˜åŒ–ç›®æ ‡å°±å˜æˆäº†ï¼š
$$
\begin{array}{l}
	\min_\theta \frac{1}{2}\sum_{j=1}^{n}\theta_j^2\\\\
	s.t. \quad \begin{array}{l}
		\theta^Tx^{(i)} \ge 1 & \textrm{if } y^{(i)}=1\\
		\theta^Tx^{(i)} \le -1 & \textrm{if } y^{(i)}=0
	\end{array}
\end{array}
$$
ç”±æ¬§æ°ç©ºé—´çš„çŸ¥è¯†ï¼š
$$
\begin{array}{ccl}
||u|| &=& \textrm{length of vector } u= \sqrt{u_1^2+u_2^2}\\
p &=& \textrm{length of projection of } v \textrm{ onto } u \textrm{ (signed)} \\ \\
u^Tv &=& p \cdot ||u||
\end{array}
$$
ä¸Šå¼å¯è¡¨ç¤ºä¸ºï¼š
$$
\begin{array}{l}
	\min_\theta \frac{1}{2}\sum_{j=1}^{n}\theta_j^2
	=\frac{1}{2}\Big(\sqrt{\sum_{j=1}^n\theta_j^2}\Big)^2
	=\frac{1}{2}||\theta||^2 \\\\
	s.t. \quad \begin{array}{l}
		p^{(i)}\cdot ||\theta|| \ge 1 & \textrm{if } y^{(i)}=1\\
		p^{(i)}\cdot ||\theta|| \le -1 & \textrm{if } y^{(i)}=0
	\end{array}\\\\
	\textrm{where $p^{(i)}$ is the projection of $x^{(i)}$ onto the vector $\theta$.}
\end{array}
$$
SVM ä¼šé€‰æ‹©æœ€å¤§çš„é—´éš™ï¼š

![å±å¹•å¿«ç…§ 2019-10-31 12.58.43](https://tva1.sinaimg.cn/large/006tNbRwgy1gax9ziqe3sj324i0nu0z9.jpg)

##### æ ¸æ–¹æ³•

é¢å¯¹å¦‚ä¸‹åˆ†ç±»é—®é¢˜ï¼š

![image-20191102114127170](https://tva1.sinaimg.cn/large/006tNbRwgy1gaxa3eq7zuj30c5083gm3.jpg)

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤šé¡¹å¼æ¥å›å½’ï¼Œä¾‹å¦‚ï¼Œå½“ $\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+\cdots\ge 0$ æ—¶é¢„æµ‹ $y=1$ï¼›

è¿™æ ·æœ‰å¤ªå¤šå¤šé¡¹å¼æ¯”è¾ƒéº»çƒ¦ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘å¦‚ä¸‹æ–¹æ³•ï¼š
$$
\begin{array}{lcl}
\textrm{Predict } y=1 &\textrm{if}&\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\cdots\ge 0
\end{array}
$$
è¿™é‡Œçš„ $f_1=x_1,f_2=x_2,f_3=x_1x_2,f_4=x_1^2,...$

æˆ‘ä»¬ç”¨ $f_i$ æ›¿æ¢äº†å¤šé¡¹å¼ï¼Œé¿å…äº†é«˜æ¬¡é¡¹çš„éº»çƒ¦ï¼Œé‚£ä¹ˆå¦‚ä½•ç¡®å®š $f_i$ï¼Ÿå¤§æ¦‚çš„æ€æƒ³å¦‚ä¸‹ï¼š

![image-20191102124615797](https://tva1.sinaimg.cn/large/006y8mN6ly1g8jljliww8j30o90dmwjt.jpg)

ä¸ºæ–¹ä¾¿æè¿°ï¼Œå‡è®¾æˆ‘ä»¬åªæœ‰ $x_0,x_1,x_2$ï¼Œå¹¶ä¸”åªæ‰“ç®—æ„é€  $f_1,f_2,f_3$ã€‚é‚£ä¹ˆï¼Œä¸ç®¡ $x_0$ï¼ˆåç§»é¡¹ï¼‰ï¼Œæˆ‘ä»¬ä» $x_1$-$x_2$ çš„å›¾åƒä¸­é€‰æ‹© 3 ä¸ªç‚¹ï¼Œè®°ä¸º $l^{(1)},l^{(2)},l^{(3)}$ï¼Œç§°ä¹‹ä¸º*æ ‡è®°ç‚¹*ã€‚ä»»ç»™ $x$ï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—å…¶ä¸å„æ ‡è®°ç‚¹çš„ä¸´è¿‘ç¨‹åº¦å¾—åˆ°ä¸€ç»„ $f_i$ï¼š
$$
f_i = \mathop{\textrm{similarity}}(x,l^{(i)}) = \exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}) = \exp(-\frac{\sum_{j=1}^n(x_j-l_j^{(i)})^2}{2\sigma^2})
$$
è¿™é‡Œå…·ä½“çš„ similarity å‡½æ•°ç§°ä¸º*æ ¸å‡½æ•°*ï¼Œæ ¸å‡½æ•°å¤šç§å¤šæ ·ã€‚æˆ‘ä»¬è¿™é‡Œå†™çš„æ˜¯å¾ˆå¸¸ç”¨çš„ $\exp(-\frac{\sum_{j=1}^n(x_j-l_j^{(i)})^2}{2\sigma^2})$ ï¼Œç§°ä¸º Gaussian Kernelï¼Œä»–çš„ä»£ç å®ç°å¦‚ä¸‹ï¼š

ç”±è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œç»™å®š $x$ï¼Œå¯¹æ¯ä¸ª $l^{(i)}$ æˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ª $f_i$ï¼Œæ»¡è¶³ï¼š

1. å½“ $x$ æ¥è¿‘ $l^{(i)}$ æ—¶

$$
f_i \approx \lim_{x\to l^{(i)}}\exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}) = \exp(-\frac{0^2}{2\sigma^2}) = 1
$$

2. å½“ $x$ è¿œç¦» $l^{(i)}$ æ—¶

$$
f_i \approx \lim_{||x-l^{(i)}||\to +\infin}\exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2}) = \exp(-\frac{\infin^2}{2\sigma^2}) = 0
$$

$\sigma$ çš„é€‰æ‹©ä¼šå½±å“ $f_i$ å€¼éš $x$ è¿œç¦» $l^{(i)}$ è€Œä¸‹é™çš„é€Ÿåº¦ï¼Œ$\sigma^2$ è¶Šå¤§ï¼Œ$f_i$å‡å°åœ°è¶Šæ…¢ï¼š

![image-20191102132852867](https://tva1.sinaimg.cn/large/006y8mN6ly1g8jmrwo86pj30ph0dwwm1.jpg)

ä½¿ç”¨æ ¸æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥åšå‡ºè¿™ç§é¢„æµ‹ï¼š

![image-20191102133600603](https://tva1.sinaimg.cn/large/006y8mN6ly1g8jmzc2qq4j30ot0ditej.jpg)

å½“ä¸”ä»…å½“ç»™å®š$x$ä¸´è¿‘$l^{(1)}$ æˆ– $l^{(2)}$ æ—¶é¢„æµ‹ 1ï¼Œå¦åˆ™é¢„æµ‹ 0.

##### SVM ä¸­ä½¿ç”¨æ ¸

1. ç»™å®šè®­ç»ƒé›† $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$

2. é€‰æ‹©æ ‡è®°ç‚¹ï¼š$l^{(i)}=x^{(i)} \quad \textrm{for }i=1,2,\cdots,m$

3. å¯¹äºæ ·æœ¬ $x$ï¼Œè®¡ç®—æ ¸ï¼š$f_i=\mathop{\textrm{similarity}}(x,l^{(i)}) \quad \textrm{for }i=1,2,\cdots,m$

4. ä»¤ $f=[f_0,f_1,f_2,\cdots,f_m]^T$ï¼Œå…¶ä¸­ $f_0 \equiv 1$ã€‚

é¢„æµ‹ï¼š
   * ç»™å®š $x$ï¼Œè®¡ç®— $f\in\R^{m+1}$
   * é¢„æµ‹ $y=1$ å¦‚æœ $\theta^Tf=\theta_0f_0+\theta_1f_1+\cdots\ge 0$

è®­ç»ƒï¼š
$$
\min_\theta 
C\sum_{i=1}^m \large[ y^{(i)} \mathop{\textrm{cost}_1}(\theta^Tf^{(i)})
+ (1 - y^{(i)})\ \mathop{\textrm{cost}_0}(\theta^Tf^{(i)})\large]
+ \frac{1}{2}\sum_{j=1}^n \theta_j^2
$$
å®ç°ï¼š

æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¯¸å¦‚ liblinear, libsvm ä¹‹ç±»çš„åº“æ¥å¾—åˆ° SVM çš„ å‚æ•° $\theta$ï¼Œè¦ä½¿ç”¨è¿™äº›åº“ï¼Œæˆ‘ä»¬ä¸€èˆ¬éœ€è¦åšä»¥ä¸‹å·¥ä½œï¼š

* é€‰æ‹©å‚æ•° $C$
* é€‰æ‹©æ ¸å‡½æ•°
  * **No kernel**ï¼ˆå³çº¿æ€§æ ¸ï¼Œäº¦å³åšé€»è¾‘å›å½’ï¼šPredict $y=1$ if $\theta^Tx\ge0$ï¼‰ï¼Œé€‚ç”¨äº **nå¤§ må°** çš„æƒ…å†µï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
  * **Gaussian kernel**ï¼ˆ$f_i=\exp(-\frac{||x-l^{(i)}||^2}{2\sigma^2})\textrm{ where } l^{(i)}=x^{(i)} \textrm{ for } i=1,\cdots,m$ï¼‰é€‚ç”¨äº **må¤§ nå°** çš„æƒ…å†µï¼ˆå¯æ‹Ÿåˆæ›´å¤æ‚çš„éçº¿æ€§è¾¹ç•Œï¼‰
* æä¾›æ ¸å‡½æ•°ï¼ˆGaussian kernel ä¸ºä¾‹ï¼‰ï¼š

$$
\begin{array}{l}
\textrm{function f = kernel(x1, x2)}\\
\qquad \textrm{f} = \exp(-\frac{||\textrm{x1}-\textrm{x2}||^2}{2\sigma^2})\\
\textrm{return}
\end{array}
$$

æ³¨æ„ï¼šä½¿ç”¨ Gaussian Kernel å‰åŠ¡å¿…åšç‰¹å¾ç¼©æ”¾ï¼

ğŸ‘‰æ— æ ¸å‡½æ•°çš„ä»£ç å®ç°ï¼š

```matlab
function sim = linearKernel(x1, x2)
%LINEARKERNEL returns a linear kernel between x1 and x2
%   sim = linearKernel(x1, x2) returns a linear kernel between x1 and x2
%   and returns the value in sim

% Ensure that x1 and x2 are column vectors
x1 = x1(:); x2 = x2(:);

% Compute the kernel
sim = x1' * x2;  % dot product

end
```

ğŸ‘‰é«˜æ–¯æ ¸çš„ä»£ç å®ç°ï¼š

```matlab
function sim = gaussianKernel(x1, x2, sigma)
%RBFKERNEL returns a radial basis function kernel between x1 and x2
%   sim = gaussianKernel(x1, x2) returns a gaussian kernel between x1 and x2
%   and returns the value in sim

% Ensure that x1 and x2 are column vectors
x1 = x1(:); x2 = x2(:);

sim = 0;

sim = exp(-sum((x1 - x2).^2) / (2 * sigma ^ 2));

end
```

ğŸ‘‰SVM ç¤ºä¾‹ï¼š

```matlab
% Load X, y, Xtest and ytest
load('data.mat');

% SVM Parameters
C = 1;         % C = 1 ~ 100 is fine
sigma = 0.1;    % sigma = 0.03 ~ 0.1 gives somewhat good boundary, less is better

% We set the tolerance and max_passes lower here so that the code will run
% faster. However, in practice, you will want to run the training to
% convergence.
model= svmTrain(X, y, C, @(x1, x2) gaussianKernel(x1, x2, sigma)); 
p = svmPredict(model, Xtest);

fprintf('Test Accuracy: %f\n', mean(double(p == ytest)) * 100);
```

##### å¤šå…ƒåˆ†ç±»

![image-20191102171432072](https://tva1.sinaimg.cn/large/006y8mN6ly1g8jul8myufj30mv0cz0xu.jpg)

#### é€»è¾‘å›å½’ vs ç¥ç»ç½‘ç»œ vs SVM

$n$ = ç‰¹å¾æ•°ï¼ˆ$x\in\R^{n+1}$ï¼‰

$m$ = è®­ç»ƒæ ·æœ¬æ•°

* nç›¸å¯¹äºmå¤§ ï¼ˆe.g. $n=10,000, m=10 \sim 1000$ï¼‰
  * é€»è¾‘å›å½’ï¼Œæˆ– æ— æ ¸SVM
* nå°ã€mé€‚ä¸­ï¼ˆe.g. $n=1\sim1000,m=50,000$ï¼‰
  * ç”¨ Gaussian æ ¸ SVM
* nå°ã€må¤§
  * åˆ›é€ /æ·»åŠ ç‰¹å¾ï¼Œç„¶åç”¨ é€»è¾‘å›å½’æˆ–æ— æ ¸SVM

ç¥ç»ç½‘ç»œé€šå¸¸å¯ä»¥è§£å†³ä¸Šè¿°ä»»ä½•ä¸€ç§æƒ…å†µï¼Œä½†å¯èƒ½ç›¸å¯¹è¾ƒæ…¢ã€‚



## æ— ç›‘ç£å­¦ä¹ 

æ— ç›‘ç£å­¦ä¹ æ˜¯åªç»™xæ•°æ®çš„ï¼Œä¸ç»™yã€‚

![å±å¹•å¿«ç…§ 2019-11-05 14.49.05](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n62mi54pj30lw0catay.jpg)

### K-Means èšç±»

> æŠŠä¸€å †ä¸œè¥¿è‡ªåŠ¨åˆ†æˆKå †ã€‚

è¾“å…¥ï¼š

* $K$ï¼šèšç±»çš„ä¸ªæ•°
* $\{x^{(1)},x^{(2)},\cdots,x^{(m)}\}$ï¼šè®­ç»ƒé›†

è¾“å‡ºï¼š

* $K$ ä¸ªç±»

K-Meansç®—æ³•ï¼š
$$
\begin{array}{l}

\textrm{Randomly initialize $K$ cluster centroids $\mu_1, \mu_2,...\mu_k \in \R^n$}\\
\textrm{Repeat }\{\\
\qquad \textrm{for $i=1$ to $m$:}\qquad\textrm{// Cluster assignment step}\\
\qquad\qquad c^{(i)} := k \ \textrm{ s.t. } \min_k||x^{(i)}-\mu_k||^2 \\
\qquad \textrm{for $k=1$ to $K$:}\qquad\textrm{// Move centroid step}\\
\qquad\qquad \mu_k:= \textrm{average (mean) of points assigned to cluster $k$}\\
\}\\

\end{array}
$$
ä»£ä»·å‡½æ•°ï¼š
$$
J(c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_K)=\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}-\mu_{c^{(i)}}||^2
$$
ä¼˜åŒ–ç›®æ ‡ï¼š
$$
\min_{
\begin{array}{c}
	{1}c^{(1)},\cdots,c^{(m)},\\
	\mu_1,\cdots,\mu_K
\end{array}}
J(c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_K)
$$
å¾—åˆ°è¾ƒä¼˜è§£(ä¸ä¸€å®šèƒ½å¾—åˆ°æœ€ä¼˜è§£)çš„ç®—æ³•ï¼š
$$
\begin{array}{l}
\textrm{For $i=1$ to $100$ <or 50~1000> \{}\\
\qquad\textrm{Randomly initialize K-means.}\\
\qquad\textrm{Run K-means. Get $c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_k$}\\
\qquad\textrm{Compute cost function (distortion):}\\
\qquad\qquad J(c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_K)\\
\textrm{\}}\\
\textrm{pick clustering that gave lowest $J$.}
\end{array}
$$
$K$çš„é€‰æ‹©ï¼š

1. æ›´å…·å®é™…é—®é¢˜çš„éœ€æ±‚æ˜“å¾—ï¼›
2. é€‰æ‹©æ‹ç‚¹ï¼š![image-20191106171612460](https://tva1.sinaimg.cn/large/006tNbRwgy1gaxja4d68ej30cb0c3jro.jpg)

ğŸ‘‰**ä»£ç å®ç°**

1. æ‰¾æœ€è¿‘çš„ç±»ä¸­å¿ƒï¼š

```matlab
function idx = findClosestCentroids(X, centroids)
%FINDCLOSESTCENTROIDS computes the centroid memberships for every example
%   idx = FINDCLOSESTCENTROIDS (X, centroids) returns the closest centroids
%   in idx for a dataset X where each row is a single example. idx = m x 1 
%   vector of centroid assignments (i.e. each entry in range [1..K])
%

% Set K
K = size(centroids, 1);

idx = zeros(size(X,1), 1);

for i = 1 : size(X, 1)
    min_j = 0;
    min_l = Inf;
    for j = 1 : size(centroids, 1)
        l = sum((X(i, :) - centroids(j, :)) .^ 2);
        if l <= min_l
            min_j = j;
            min_l = l;
        end
    end
    idx(i) = min_j;
end

end
```

2. è®¡ç®—ä¸­å¿ƒ:

```matlab
function centroids = computeCentroids(X, idx, K)
%COMPUTECENTROIDS returns the new centroids by computing the means of the 
%data points assigned to each centroid.
%   centroids = COMPUTECENTROIDS(X, idx, K) returns the new centroids by 
%   computing the means of the data points assigned to each centroid. It is
%   given a dataset X where each row is a single data point, a vector
%   idx of centroid assignments (i.e. each entry in range [1..K]) for each
%   example, and K, the number of centroids. You should return a matrix
%   centroids, where each row of centroids is the mean of the data points
%   assigned to it.
%

% Useful variables
[m n] = size(X);

centroids = zeros(K, n);

for i = 1 : K
    ck = find(idx == i);
    centroids(i, :) = sum(X(ck,:)) / size(ck, 1);
end

end
```

3. è¿è¡ŒK-Means

```matlab
function [centroids, idx] = runkMeans(X, initial_centroids, ...
                                      max_iters, plot_progress)
%RUNKMEANS runs the K-Means algorithm on data matrix X, where each row of X
%is a single example
%   [centroids, idx] = RUNKMEANS(X, initial_centroids, max_iters, ...
%   plot_progress) runs the K-Means algorithm on data matrix X, where each 
%   row of X is a single example. It uses initial_centroids used as the
%   initial centroids. max_iters specifies the total number of interactions 
%   of K-Means to execute. plot_progress is a true/false flag that 
%   indicates if the function should also plot its progress as the 
%   learning happens. This is set to false by default. runkMeans returns 
%   centroids, a Kxn matrix of the computed centroids and idx, a m x 1 
%   vector of centroid assignments (i.e. each entry in range [1..K])
% è‹¥ä½¿ç”¨ plot_progress éœ€è¦é¢å¤–çš„ç”»å›¾å‡½æ•°å®ç°ï¼Œè¿™é‡Œæ²¡æœ‰ç»™å‡º.
%

% Set default value for plot progress
if ~exist('plot_progress', 'var') || isempty(plot_progress)
    plot_progress = false;
end

% Plot the data if we are plotting progress
if plot_progress
    figure;
    hold on;
end

% Initialize values
[m n] = size(X);
K = size(initial_centroids, 1);
centroids = initial_centroids;
previous_centroids = centroids;
idx = zeros(m, 1);

% Run K-Means
for i=1:max_iters
    
    % Output progress
    fprintf('K-Means iteration %d/%d...\n', i, max_iters);
    if exist('OCTAVE_VERSION')
        fflush(stdout);
    end
    
    % For each example in X, assign it to the closest centroid
    idx = findClosestCentroids(X, centroids);
    
    % Optionally, plot progress here
    if plot_progress
        plotProgresskMeans(X, centroids, previous_centroids, idx, K, i);
        previous_centroids = centroids;
        fprintf('Press enter to continue.\n');
        input("...");
    end
    
    % Given the memberships, compute new centroids
    centroids = computeCentroids(X, idx, K);
end

% Hold off if we are plotting progress
if plot_progress
    hold off;
end

end
```

4. é©±åŠ¨è„šæœ¬ï¼š

```matlab
% Load an example dataset
load('data.mat');

% Settings for running K-Means
K = 3;
max_iters = 10;

% For consistency, here we set centroids to specific values
% but in practice you want to generate them automatically, such as by
% settings them to be random examples (as can be seen in
% kMeansInitCentroids).
initial_centroids = [3 3; 6 2; 8 5];

% Run K-Means algorithm. The 'true' at the end tells our function to plot
% the progress of K-Means
[centroids, idx] = runkMeans(X, initial_centroids, max_iters, true);
fprintf('\nK-Means Done.\n\n');
```

### PCA ç»´æ•°çº¦å‡

> ä¸»æˆåˆ†åˆ†æï¼šæŠŠnç»´çš„æ•°æ®(æŠ•å½±)é™åˆ°kç»´ï¼Œç•¥å»ä¸é‡è¦çš„éƒ¨åˆ†(k<=n)ã€‚

**PCAç®—æ³•**ï¼š

1) æ•°æ®é¢„å¤„ç†

   è®­ç»ƒé›†ï¼š$x^{(1)},x^{(2)},\cdots,x^{(m)}$

   é¢„å¤„ç†(feature scaling & mean normalization):

   - $\mu_j=\frac{1}{m}\sum_{i=1}^m x_j^{(i)},\qquad s_j=\textrm{standard deviation of feature }j$

   - Replace each $x_j^{(i)}$ with $\frac{x_j-\mu_j}{s_j}$

2)é™ç»´

   1. è®¡ç®—åæ–¹å·®çŸ©é˜µ$\Sigma$ï¼ˆè¿™ä¸ªçŸ©é˜µè®°åšå¤§Sigmaï¼Œæ³¨æ„å’Œæ±‚å’Œå·åŒºåˆ†ï¼‰ï¼š
$$
\Sigma = \frac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T
$$
   2. æ±‚$\Sigma$çš„ç‰¹å¾å€¼(å®é™…ä¸Šæ˜¯å¥‡å¼‚å€¼åˆ†è§£)ï¼š`[U, S, V] = svd(Sigma);`
   3. ä»ä¸Šä¸€æ­¥svdå¾—åˆ°:
$$
   U = \left[\begin{array}{cccc}
   | & | &  & |\\
   u^{(1)} & u^{(2)} & \cdots & u^{(n)}\\
   | & | &  & |
   \end{array}\right]
   \in \R^{n\times n}
   \Rightarrow 
   U_{reduce}=\left[\begin{array}{cccc}
   | & | &  & |\\
   u^{(1)} & u^{(2)} & \cdots & u^{(k)}\\
   | & | &  & |
   \end{array}\right]
$$
   4. å®Œæˆé™ç»´ï¼š$x\in\R^n\to z\in\R^k$:
$$
   z = U_{reduce}^Tx
   =\left[\begin{array}{ccc}
   -- & (u^{(1)})^T & --\\
    & \vdots & \\ 
   -- & (u^{(k)})^T & --\\
   \end{array}\right]x
$$

ğŸ‘‰ä»£ç å®ç°ï¼š

```matlab
% do feature scaling & mean normalization

Sigma = 1/m * X' * X;
[U, S, V] = svd(Sigma);

Ureduce = U(:, 1:K);
Z = X * Ureduce;
```

**æ•°æ®å¤åŸ**ï¼šå°†æ•°æ®è¿˜åŸåˆ°åŸæ¥çš„ç»´åº¦ï¼ˆ$z\in\R^k \to x_{approx}\in\R^n$ï¼‰ï¼š
$$
x_{approx}=U_{reduce}z
$$
ä¸€èˆ¬æƒ…å†µä¸‹ $x \neq x_{approx}$ï¼Œæˆ‘ä»¬åªèƒ½æœŸæœ› $x_{approx}$ å°½é‡æ¥è¿‘ $x$. 

![image-20191110215011122](https://tva1.sinaimg.cn/large/006y8mN6gy1g8ta88gafkj30oz0djaeo.jpg)

**$k$(ä¸»æˆåˆ†ä¸ªæ•°)çš„é€‰æ‹©**

ä¸€èˆ¬ï¼Œé€‰æ‹© $k$ ä¸ºä½¿å¾—ä¸‹å¼æˆç«‹çš„æœ€å°å€¼ï¼š
$$
\frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}||^2}\le0.01
$$
ç®—æ³•ï¼š
$$
\begin{array}{l}
\textrm{Try PCA with } k=1,\cdots,n:\\
\quad \textrm{Compute } U_{reduce},z^{(1)},\cdots,z^{(m)},x_{approx}^{(1)},\cdots,x_{approx}^{m}\\
\quad \textrm{Check if } \frac{\frac{1}{m}\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum_{i=1}^{m}||x^{(i)}||^2}\le0.01
\end{array}
$$

### å¼‚å¸¸æ£€æµ‹

> ä»ä¸€å †æ•°æ®ä¸­æ‰¾å‡ºå¼‚å¸¸äºå…¶ä»–çš„ã€‚

é—®é¢˜æè¿°ï¼šç»™å®šæ•°æ®é›† $\{x^{(1)},x^{(2)},\cdots,x^{(m)}\}$ï¼Œé€šè¿‡è®­ç»ƒï¼Œåˆ¤æ–­ $x_{test}$ æ˜¯å¦å¼‚å¸¸ã€‚

è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹$p(x)$ï¼ˆæ¦‚ç‡ï¼‰å»ºç«‹ä¸€ä¸ªæ¨¡å‹ï¼Œé€‰æ‹©ä¸€ä¸ªä¸´ç•Œå€¼ $\epsilon$ï¼Œä½¿ï¼š
$$
\begin{array}{l}
p(x_{test})<\epsilon \Rightarrow \textrm{anomaly}\\
p(x_{test})\ge\epsilon \Rightarrow \textrm{OK}
\end{array}
$$
è¿™æ ·é—®é¢˜å¯ä»¥è½¬åŒ–ä¸º*å¯†åº¦å€¼ä¼°è®¡*ã€‚æˆ‘ä»¬å¸¸ç”¨é«˜æ–¯åˆ†å¸ƒè§£å†³è¿™ä¸ªé—®é¢˜ã€‚

#### é«˜æ–¯åˆ†å¸ƒ

$x$ æœä»é«˜æ–¯åˆ†å¸ƒï¼š$x \sim \mathcal{N}(\mu,\sigma^2)$

åˆ™ï¼Œ$x$ çš„æ¦‚ç‡ä¸ºï¼š
$$
p(x) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$
å…¶ä¸­å‚æ•° $\mu$ å’Œ $\sigma$ ç”±ä¸‹å¼ç¡®å®šï¼ˆè¿™æ˜¯åœ¨æœºå™¨å­¦ä¹ é‡Œå¸¸ç”¨çš„æ ¼å¼ï¼Œä¸ä¸€å®šå’Œæ•°å­¦é‡Œçš„ä¸€æ ·ï¼‰ï¼š
$$
\mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}
$$

$$
\sigma^2=\frac{1}{m}\sum_{i=1}^{m}\left(x^{(i)}-\mu\right)^2
$$

ğŸ‘‰ä»£ç å®ç°ï¼š

```matlab
function [mu sigma2] = estimateGaussian(X)
%ESTIMATEGAUSSIAN This function estimates the parameters of a 
%Gaussian distribution using the data in X
%   [mu sigma2] = estimateGaussian(X), 
%   The input X is the dataset with each n-dimensional data point in one row
%   The output is an n-dimensional vector mu, the mean of the data set
%   and the variances sigma^2, an n x 1 vector
% 

% Useful variables
[m, n] = size(X);

mu = zeros(n, 1);
sigma2 = zeros(n, 1);

mu = mean(X);
sigma2 = var(X) * (m - 1) / m;

end
```



å€Ÿæ­¤æˆ‘ä»¬ä¾¿å¯å¾—åˆ°å¼‚å¸¸æ£€æŸ¥ç®—æ³•ï¼š

##### å¼‚å¸¸æ£€æŸ¥ç®—æ³•

1. é€‰æ‹©è®¤ä¸ºå¯èƒ½è¡¨ç°å‡ºæ ·æœ¬å¼‚å¸¸çš„æ•°æ®ç‰¹å¾ $x_i$
2. è®¡ç®—å‚æ•° $\mu_1,\cdots,\mu_n,\sigma_1^2,\cdots,\sigma_n^2$ 

$$
\mu=\frac{1}{m}\sum_{i=1}^{m}x^{(i)}
$$

$$
\sigma^2=\frac{1}{m}\sum_{i=1}^{m}\left(x^{(i)}-\mu\right)^2
$$

3. å¯¹äºæ–°ç»™çš„æ ·æœ¬ $x$ï¼Œè®¡ç®— $p(x)$ï¼š

$$
p(x)=\prod_{j=1}^{n}p(x_j;\mu_j,\sigma_j^2)=\prod_{j=1}^{n}\frac{1}{\sqrt{2\pi}\sigma_j}\exp\left(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2}\right)
$$

4. å¦‚æœ$p(x)<\epsilon$ï¼Œåˆ™é¢„æµ‹å¼‚å¸¸ã€‚

#### å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ

$$
p(x;\mu,\Sigma)=\frac
{\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)}
{\sqrt{(2\pi)^{n}|\Sigma|}}
$$

å‚æ•°ï¼š

* $\mu\in\R^n$
* $\Sigma\in\R^{n\times n}$ (covariance matrix, `Sigma = 1/m * X' * X;`)

å‚æ•°çš„è®¡ç®—ï¼š
$$
\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)} \qquad
\Sigma=\frac{1}{m}\sum_{i=1}^m\left(x^{(i)}-\mu\right)\left(x^{(i)}-\mu\right)^T
$$

ğŸ‘‰ä»£ç å®ç°ï¼š

```matlab
function p = multivariateGaussian(X, mu, Sigma2)
%MULTIVARIATEGAUSSIAN Computes the probability density function of the
%multivariate gaussian distribution.
%    p = MULTIVARIATEGAUSSIAN(X, mu, Sigma2) Computes the probability 
%    density function of the examples X under the multivariate gaussian 
%    distribution with parameters mu and Sigma2. If Sigma2 is a matrix, it is
%    treated as the covariance matrix. If Sigma2 is a vector, it is treated
%    as the \sigma^2 values of the variances in each dimension (a diagonal
%    covariance matrix)
%

k = length(mu);

if (size(Sigma2, 2) == 1) || (size(Sigma2, 1) == 1)
    Sigma2 = diag(Sigma2);
end

X = bsxfun(@minus, X, mu(:)');
p = (2 * pi) ^ (- k / 2) * det(Sigma2) ^ (-0.5) * ...
    exp(-0.5 * sum(bsxfun(@times, X * pinv(Sigma2), X), 2));

end
```

##### ç”¨å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å¼‚å¸¸æ£€æŸ¥

1. æ‹Ÿåˆå¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„ $p(x)$ æ¨¡å‹ï¼Œé€šè¿‡å‚æ•°ï¼š

$$
\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)} \qquad
\Sigma=\frac{1}{m}\sum_{i=1}^m\left(x^{(i)}-\mu\right)\left(x^{(i)}-\mu\right)^T
$$

2. å¯¹äºæ–°ç»™ $x$ï¼Œè®¡ç®—ï¼š

$$
p(x)=\frac
{\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)}
{\sqrt{(2\pi)^{n}|\Sigma|}}
$$

3. å¦‚æœ$p(x)<\epsilon$ï¼Œåˆ™é¢„æµ‹å¼‚å¸¸ã€‚

#### é—¨æ§›é€‰æ‹©

é€šè¿‡è®¡ç®— $F_1$ å€¼å¯ä»¥å¾—åˆ°æœ€é€‚åˆçš„ $\epsilon$ã€‚

$F_1$ å€¼ç”± precision ($prec$) å’Œ recall ($rec$) ç»™å‡ºï¼š
$$
F_1=\frac{2\cdot prec \cdot rec}{prec+rec}
$$
å…¶ä¸­ï¼š
$$
prec = \frac{tp}{tp+fp}
$$

$$
rec = \frac{tp}{tp+fn}
$$

* $tp$ æ˜¯ true positivesï¼šé¢„æµ‹ä¸ºæ­£ï¼Œå®é™…ä¹Ÿä¸ºæ­£
* $fp$ æ˜¯ false positivesï¼šé¢„æµ‹ä¸ºæ­£ï¼Œå®é™…ä¸ºè´Ÿ
* $fn$ æ˜¯ false negativesï¼šé¢„æµ‹ä¸ºè´Ÿï¼Œå®é™…ä¸ºæ­£

![image-20191026152903469](https://tva1.sinaimg.cn/large/006y8mN6ly1g8bmwqbebxj30po0dgahc.jpg)

ğŸ‘‰ä»£ç å®ç°ï¼š

```matlab
function [bestEpsilon bestF1] = selectThreshold(yval, pval)
%SELECTTHRESHOLD Find the best threshold (epsilon) to use for selecting
%outliers
%   [bestEpsilon bestF1] = SELECTTHRESHOLD(yval, pval) finds the best
%   threshold to use for selecting outliers based on the results from a
%   validation set (pval) and the ground truth (yval).
%

bestEpsilon = 0;
bestF1 = 0;
F1 = 0;

stepsize = (max(pval) - min(pval)) / 1000;
for epsilon = min(pval):stepsize:max(pval)

    cvPredictions = pval < epsilon;
    
    tp = sum((cvPredictions == 1) & (yval == 1));
    fp = sum((cvPredictions == 1) & (yval == 0));
    fn = sum((cvPredictions == 0) & (yval == 1));

    prec = tp / (tp + fp);
    rec = tp / (tp + fn);

    F1 = (2 * prec * rec) / (prec + rec);

    if F1 > bestF1
       bestF1 = F1;
       bestEpsilon = epsilon;
    end
end

end
```



### æ¨èç³»ç»Ÿ

> é€šè¿‡è¯„åˆ†ï¼Œæ¨èç”¨æˆ·æ–°å†…å®¹ã€‚

ç¬¦å·è¯´æ˜ï¼šï¼ˆå‡è®¾æˆ‘ä»¬è¦æ¨èçš„ä¸œè¥¿æ˜¯ç”µå½±ï¼‰

- $n_u$ = ç”¨æˆ·æ•°
- $n_m$ = ç”µå½±æ•°
- $r(i,j)=1$ è‹¥ç”¨æˆ· $j$ å¯¹ç”µå½± $i$ è¯„è¿‡åˆ†ï¼Œå¦åˆ™ä¸º 0
- $y^{(i,j)}$ = ç”¨æˆ· $j$ ç»™ç”µå½± $i$ çš„è¯„åˆ†(åªæœ‰å½“ $r(i,j)=1$ æ—¶æ‰æœ‰å®šä¹‰)

#### åŸºäºå†…å®¹æ¨è

**é¢„æµ‹æ¨¡å‹**ï¼š

![å±å¹•å¿«ç…§ 2019-11-26 16.04.26](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bi5bv4etj30nz06lac1.jpg)

* $r(i,j)=1$ è‹¥ç”¨æˆ· $j$ å¯¹ç”µå½± $i$ è¯„è¿‡åˆ†
* $y^{(i,j)}$ = ç”¨æˆ· $j$ ç»™ç”µå½± $i$ çš„è¯„åˆ†(å¦‚æœæœ‰å®šä¹‰)
* $\theta^{(j)}$ ç”¨æˆ· $j$ çš„å‚æ•°ï¼ˆå‘é‡ï¼‰
* $x^{(i)}$ ç”µå½± $i$ çš„ç‰¹å¾ï¼ˆå‘é‡ï¼‰

å¯¹äºç”¨æˆ· $j$ï¼Œç”µå½± $i$ï¼Œé¢„æµ‹è¯„åˆ†ï¼š$(\theta^{(j)})^T(x^{(i)})$ã€‚

**ä¼˜åŒ–ç›®æ ‡**ï¼š

1. ä¼˜åŒ– $\theta^{(j)}$ ï¼ˆå¯¹äºå•ä¸ªç”¨æˆ· $j$ çš„å‚æ•°ï¼‰

$$
\min_{\theta^{(j)}}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{k=1}^n \left(\theta_k^{(j)}\right)^2
$$

2. ä¼˜åŒ– $\theta^{(1)},\theta^{(2)},\cdots,\theta^{(n_u)}$ï¼ˆå¯¹æ‰€æœ‰ç”¨æˆ·ï¼‰

$$
\min_{\theta^{(1)},\cdots,\theta^{(n_u)}}
\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 +
\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n \left(\theta_k^{(j)}\right)^2
$$

æˆ‘ä»¬å¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™è§£å†³é—®é¢˜ï¼š
$$
\begin{array}{l}
Repeat\quad\{\\
\qquad \theta_0^{(j)}:=\theta_0^{(j)}-\alpha\sum_{i:r(i,j)=1} \big((\theta^{(j)})^T(x^{(i)})-y^{(i,j)}\big)x_0^{(i)}\\
\qquad \theta_k^{(j)}:=\theta_k^{(j)}-\alpha\Big[\Big(\sum_{i:r(i,j)=1}\big((\theta^{(j)})^T(x^{(i)})-y^{(i)}\big)x_k^{(i)}\Big)+\lambda\theta_k^{(j)}\Big]\qquad (\textrm{for } k \neq 0)\\
\}
\end{array}
$$

#### ååŒè¿‡æ»¤

åœ¨åŸºäºå†…å®¹æ¨èä¸­æˆ‘ä»¬æœ‰æ—¶ä¼šå¾ˆéš¾æŠŠæ¡ç”µå½±ï¼ˆæˆ‘ä»¬è¦æ¨èçš„ä¸œè¥¿ï¼‰æœ‰å“ªäº›ç‰¹å¾ï¼ˆ$x^{(i)}$ï¼‰ï¼Œæˆ‘ä»¬æƒ³è®©æœºå™¨å­¦ä¹ è‡ªå·±æ‰¾ç‰¹å¾ï¼Œè¿™å°±ç”¨åˆ°ååŒè¿‡æ»¤ã€‚

**æ–°åŠ çš„ä¼˜åŒ–ç›®æ ‡**ï¼šï¼ˆä¹‹å‰åœ¨åŸºäºå†…å®¹æ¨èé‡Œé¢çš„ä¼˜åŒ–ç›®æ ‡ä»éœ€è€ƒè™‘ï¼‰

* ç»™å®š $\theta^{(1)},\theta^{(2)},\cdots,\theta^{(n_u)}$ï¼Œå­¦ä¹  $x^{(i)}$:

$$
\min_{x^{(i)}}\frac{1}{2}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{k=1}^n \left(x_k^{(i)}\right)^2
$$

* ç»™å®š $\theta^{(1)},\theta^{(2)},\cdots,\theta^{(n_u)}$ ï¼Œå­¦ä¹  $x^{(1)},\cdots,x^{(n_m)}$ ï¼š

$$
\min_{x^{(1)},\cdots,x^{(n_m)}}\frac{1}{2}
\sum_{i=1}^{n_m}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 +
\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n \left(x_k^{(i)}\right)^2
$$

**ååŒè¿‡æ»¤**ï¼š

ç°åœ¨æˆ‘ä»¬çš„é—®é¢˜æ˜¯å³æ²¡æœ‰è®­ç»ƒå¥½çš„ $\theta$ï¼Œåˆæ²¡æœ‰ä¸€ç»„å……åˆ†ä¼˜åŒ–çš„ $x$ï¼Œä½†å­¦ä¹  $\theta$ è¦å…ˆæœ‰ $x$ï¼Œå­¦ä¹  $x$ è¦å…ˆæœ‰ $\theta$ã€‚è¿™å°±å˜æˆäº†ä¸€ä¸ªç±»ä¼¼é¸¡ç”Ÿè›‹ã€è›‹ç”Ÿé¸¡çš„é—®é¢˜ã€‚

æˆ‘ä»¬å¯ä»¥è€ƒè™‘è¿™æ ·è§£å†³è¿™ä¸ªéš¾é¢˜ï¼š

é¦–å…ˆéšæœºçŒœä¸€ç»„ $\theta$ï¼Œç„¶åç”¨è¿™ç»„ $\theta$ å°±å¯ä»¥å¾—åˆ°ä¸€ç»„ $x$ï¼›ç”¨è¿™ç»„å¾—åˆ°çš„ $x$ åˆå¯ä»¥ä¼˜åŒ– $\theta$ï¼Œä¼˜åŒ–åçš„ $\theta$ åˆæ‹¿æ¥ä¼˜åŒ– $x$ ...... ä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥æœŸæœ›å¾—åˆ°ä¸€ç»„ $x$ å’Œ $\theta$ éƒ½å……åˆ†ä¼˜åŒ–çš„è§£ï¼ˆäº‹å®ä¸Šå®ƒä»¬æœ€ç»ˆæ˜¯ä¼šæ”¶æ•›çš„ï¼‰ã€‚

![å±å¹•å¿«ç…§ 2019-11-28 15.20.59](https://tva1.sinaimg.cn/large/006y8mN6ly1g9ds8xkjatj30ob0bd0w0.jpg)
$$
\begin{array}{l}

\textrm{Given }
x^{(1)},\cdots,x^{(n_m)}
\textrm{ , estimate } 
\theta^{(1)},\cdots,\theta^{(n_u)}:\\
\quad
\min_{\theta^{(1)},\cdots,\theta^{(n_u)}}
\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 +
\frac{\lambda}{2}\sum_{j=1}^{n_u}\sum_{k=1}^n \left(\theta_k^{(j)}\right)^2\\

\textrm {Given }
\theta^{(1)},\cdots,\theta^{(n_u)}
\textrm{ , estimate } 
x^{(1)},\cdots,x^{(n_m)}:\\
\quad
\min_{x^{(1)},\cdots,x^{(n_m)}}\frac{1}{2}
\sum_{i=1}^{n_m}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2 +
\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n \left(x_k^{(i)}\right)^2
\end{array}
$$
æˆ‘ä»¬éšæœºåˆå§‹åŒ–ä¸€ç»„å‚æ•°ï¼Œç„¶åé‡å¤æ¥å›è®¡ç®— $\theta$ å’Œ $x$ï¼Œæœ€ç»ˆä¼šå¾—åˆ°è§£ï¼Œä½†è¿™æ ·æ¯”è¾ƒéº»çƒ¦ï¼Œæˆ‘ä»¬å¯ä»¥åšçš„æ›´é«˜æ•ˆï¼š

**åŒæ—¶**ä¼˜åŒ– $x^{(1)},\cdots,x^{(n_m)}$ å’Œ $\theta^{(1)},\cdots,\theta^{(n_u)}$:
$$
J(x^{(1)},\cdots,x^{(n_m)},\theta^{(1)},\cdots,\theta^{(n_u)})=
\frac{1}{2}
\sum_{(i,j):r(i,j)=1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+
\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n \left(x_k^{(i)}\right)^2+
\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^n \left(x_k^{(i)}\right)^2
$$

$$
\min_{\begin{array}{c}x^{(1)},\cdots,x^{(n_m)}\\\theta^{(1)},\cdots,\theta^{(n_u)}\end{array}}
J(x^{(1)},\cdots,x^{(n_m)},\theta^{(1)},\cdots,\theta^{(n_u)})
$$



**ååŒè¿‡æ»¤ç®—æ³•**ï¼š

1. å°† $x^{(1)},\cdots,x^{(n_m)},\theta^{(1)},\cdots,\theta^{(n_u)}$ éšæœºåˆå§‹åŒ–ä¸ºä¸€äº›æ¯”è¾ƒå°çš„éšæœºå€¼
2. ä¼˜åŒ– $J(x^{(1)},\cdots,x^{(n_m)},\theta^{(1)},\cdots,\theta^{(n_u)})$
3. å¯¹äºç»™å®šç”¨æˆ·ï¼Œè¯¥ç”¨æˆ·çš„å‚æ•°æ˜¯ $\theta$ï¼Œåˆ™ç”¨è®­ç»ƒå¾—åˆ°çš„æŸç”µå½±çš„ç‰¹å¾ $x$ ï¼Œæˆ‘ä»¬å¯ä»¥é¢„æµ‹è¯¥ç”¨æˆ·å¯èƒ½ä¸ºæ­¤ç”µå½±è¯„åˆ†ï¼š$\theta^Tx$ã€‚

**ä½ç§©çŸ©é˜µåˆ†è§£**ï¼š

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬æœ€ç»ˆçš„é¢„æµ‹æ˜¯è¿™æ ·çš„ï¼š
$$
Predict = \left[\begin{array}{ccccc} (x^{(1)})^T(\theta^{(1)}) & \ldots & (x^{(1)})^T(\theta^{(n_u)})\\ \vdots & \ddots & \vdots \\ (x^{(n_m)})^T(\theta^{(1)}) & \ldots & (x^{(n_m)})^T(\theta^{(n_u)})\end{array}\right]
$$
è€ƒè™‘åˆ°å‡ ä¹ä¸å¯èƒ½æœ‰ç”¨æˆ·æŠŠæ¥è¿‘æ‰€æœ‰çš„ç”µå½±éƒ½è¯„åˆ†ï¼Œè¿™ä¸ªé¢„æµ‹çŸ©é˜µæ˜¯ç¨€ç–çš„ï¼Œå­˜å‚¨è¿™ä¸ªçŸ©é˜µä¼šé€ æˆå¤§é‡æµªè´¹ï¼Œä¸å¦¨ä»¤ï¼š
$$
X = \left[\begin{array}{ccc}
- & (x^{(1)})^T & - \\
  & \vdots & \\
- & (x^{(n_m)})^T & - \\
\end{array}\right],
\qquad
\Theta = \left[\begin{array}{ccc}
- & (\theta^{(1)})^T & - \\
  & \vdots & \\
- & (\theta^{(n_u)})^T & - \\
\end{array}\right]
$$
åˆ™æœ‰ï¼š
$$
Predict=X\Theta^T
$$
æˆ‘ä»¬ä¾¿å°†å®ƒåˆ†ä¸ºäº†ä¸¤éƒ¨åˆ†ã€‚ç”¨è¿™ä¸ªæ–¹æ³•æ±‚å– $X$ å’Œ $\Theta$ï¼Œè·å¾—æ¨èç³»ç»Ÿéœ€è¦çš„å‚æ•°ï¼Œç§°ä¹‹ä¸º**ä½ç§©çŸ©é˜µåˆ†è§£**ã€‚è¯¥æ–¹æ³•ä¸ä»…èƒ½åœ¨ç¼–ç¨‹æ—¶ç›´æ¥é€šè¿‡å‘é‡åŒ–çš„æ‰‹æ³•è·å¾—å‚æ•°ï¼Œè¿˜é€šè¿‡çŸ©é˜µåˆ†è§£èŠ‚çœäº†å†…å­˜ç©ºé—´ã€‚

**å¯»æ‰¾ç›¸å…³ç”µå½±**ï¼š

æˆ‘ä»¬å¸¸éœ€è¦æ¨èä¸ç”µå½± $i$ ç›¸å…³çš„ç”µå½± $j$ï¼Œå¯ä»¥è¿™æ ·æ‰¾åˆ°ï¼š
$$
\mathop{\textrm{smallest}} ||x^{(i)}-x^{(j)}||
$$
**å‡å€¼å½’ä¸€åŒ–å¤„ç†**ï¼š

å†ç”µå½±æ¨èé—®é¢˜ä¸­ï¼Œç”±äºè¯„åˆ†æ€»æ˜¯1åˆ°5åˆ†ï¼ˆæˆ–å…¶ä»–èŒƒå›´ï¼‰ï¼Œæ•…ä¸ç”¨ç‰¹å¾ç¼©æ”¾ï¼Œä½†å¯ä»¥åš mean normalizationï¼š
$$
\mu_i=\mathop{\textrm{average}} y^{(i,:)}
$$

$$
Y_i = Y_i-\mu_i
$$

å¯¹ç”¨æˆ· $j$, ç”µå½± $i$, é¢„æµ‹:
$$
\left(\Theta^{(j)}\right)^T\left(x^{(i)}\right)+\mu_i
$$
ğŸ‘‰**ä»£ç å®ç°**ï¼š

1. ä»£ä»·å‡½æ•°ï¼š

```matlab
function [J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, ...
                                  num_features, lambda)
%COFICOSTFUNC Collaborative filtering cost function
%   [J, grad] = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...
%   num_features, lambda) returns the cost and gradient for the
%   collaborative filtering problem.
%

% Unfold the U and W matrices from params
X = reshape(params(1:num_movies*num_features), num_movies, num_features);
Theta = reshape(params(num_movies*num_features+1:end), ...
                num_users, num_features);

            
J = 0;
X_grad = zeros(size(X));
Theta_grad = zeros(size(Theta));

h = X * Theta';
er = (h - Y) .* R;

J = 1/2 * sum(sum(er.^2));
X_grad = er * Theta;
Theta_grad = er' * X; 

% Regularized

J += lambda/2 *(sum(sum(Theta.^2)) + sum(sum(X.^2)));
X_grad += lambda * X;
Theta_grad += lambda * Theta;
grad = [X_grad(:); Theta_grad(:)];

end
```

2. å‡å€¼å½’ä¸€

```matlab
function [Ynorm, Ymean] = normalizeRatings(Y, R)
%NORMALIZERATINGS Preprocess data by subtracting mean rating for every 
%movie (every row)
%   [Ynorm, Ymean] = NORMALIZERATINGS(Y, R) normalized Y so that each movie
%   has a rating of 0 on average, and returns the mean rating in Ymean.
%

[m, n] = size(Y);
Ymean = zeros(m, 1);
Ynorm = zeros(size(Y));
for i = 1:m
    idx = find(R(i, :) == 1);
    Ymean(i) = mean(Y(i, idx));
    Ynorm(i, idx) = Y(i, idx) - Ymean(i);
end

end
```

3. é©±åŠ¨è„šæœ¬

```matlab
%  Normalize Ratings
[Ynorm, Ymean] = normalizeRatings(Y, R);

%  Useful Values
num_users = size(Y, 2);
num_movies = size(Y, 1);
num_features = 10;

% Set Initial Parameters (Theta, X)
X = randn(num_movies, num_features);
Theta = randn(num_users, num_features);

initial_parameters = [X(:); Theta(:)];

% Set options for fmincg
options = optimset('GradObj', 'on', 'MaxIter', 100);

% Set Regularization
lambda = 10;
theta = fmincg (@(t)(cofiCostFunc(t, Ynorm, R, num_users, num_movies, ...
                                num_features, lambda)), ...
                initial_parameters, options);

% Unfold the returned theta back into U and W
X = reshape(theta(1:num_movies*num_features), num_movies, num_features);
Theta = reshape(theta(num_movies*num_features+1:end), ...
                num_users, num_features);

fprintf('Recommender system learning completed.\n');

p = X * Theta';
my_predictions = p(:,1) + Ymean;
```

---

EOF