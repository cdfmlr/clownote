---
categories:
- Machine Learning
- Deep Learning with Python
date: 2020-08-12 09:50:52
tag:
- Machine Learning
- Deep Learning
title: Pythonæ·±åº¦å­¦ä¹ ä¹‹ç†è§£å¾ªç¯ç¥ç»ç½‘ç»œ
---

# Deep Learning with Python

è¿™ç¯‡æ–‡ç« æ˜¯æˆ‘å­¦ä¹ ã€ŠDeep Learning with Pythonã€‹(ç¬¬äºŒç‰ˆï¼ŒFranÃ§ois Chollet è‘—) æ—¶å†™çš„ç³»åˆ—ç¬”è®°ä¹‹ä¸€ã€‚æ–‡ç« çš„å†…å®¹æ˜¯ä»  Jupyter notebooks è½¬æˆ Markdown çš„ï¼Œä½ å¯ä»¥å» [GitHub](https://github.com/cdfmlr/Deep-Learning-with-Python-Notebooks) æˆ– [Gitee](https://gitee.com/cdfmlr/Deep-Learning-with-Python-Notebooks) æ‰¾åˆ°åŸå§‹çš„ `.ipynb` ç¬”è®°æœ¬ã€‚

ä½ å¯ä»¥å»[è¿™ä¸ªç½‘ç«™åœ¨çº¿é˜…è¯»è¿™æœ¬ä¹¦çš„æ­£ç‰ˆåŸæ–‡](https://livebook.manning.com/book/deep-learning-with-python)(è‹±æ–‡)ã€‚è¿™æœ¬ä¹¦çš„ä½œè€…ä¹Ÿç»™å‡ºäº†é…å¥—çš„ [Jupyter notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks)ã€‚

æœ¬æ–‡ä¸º **ç¬¬6ç«   æ·±åº¦å­¦ä¹ ç”¨äºæ–‡æœ¬å’Œåºåˆ—** (Chapter 6. *Deep learning for text and sequences*) çš„ç¬”è®°ã€‚

[TOC]

## 6.2 Understanding recurrent neural networks

> ç†è§£å¾ªç¯ç¥ç»ç½‘ç»œ

ä¹‹å‰æˆ‘ä»¬ç”¨çš„å…¨è¿æ¥ç½‘ç»œå’Œå·ç§¯ç¥ç»ç½‘ç»œéƒ½æœ‰æ˜¯è¢«å«åš feedforward networks (å‰é¦ˆç½‘ç»œ) çš„ã€‚è¿™ç§ç½‘ç»œæ˜¯æ— è®°å¿†çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬å•ç‹¬å¤„ç†æ¯ä¸ªè¾“å…¥ï¼Œåœ¨è¾“å…¥ä¸è¾“å…¥ä¹‹é—´æ²¡æœ‰ä¿å­˜ä»»ä½•çŠ¶æ€ã€‚åœ¨è¿™ç§ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬è¦å¤„ç†æ—¶é—´/æ–‡æœ¬ç­‰åºåˆ—ï¼Œå°±å¿…é¡»æŠŠä¸€ä¸ªå®Œæ•´çš„åºåˆ—å¤„ç†æˆä¸€ä¸ªå¤§å¼ é‡ï¼Œæ•´ä¸ªçš„ä¼ åˆ°ç½‘ç»œä¸­ï¼Œè®©æ¨¡å‹ä¸€æ¬¡çœ‹å®Œæ•´ä¸ªåºåˆ—ã€‚

è¿™ä¸ªæ˜¾ç„¶å’Œæˆ‘ä»¬äººç±»é˜…è¯»ã€å­¦ä¹ æ–‡æœ¬ç­‰ä¿¡æ¯çš„æ–¹å¼æœ‰æ‰€åŒºåˆ«ã€‚æˆ‘ä»¬ä¸æ˜¯ä¸€çœ¼çœ‹å®Œæ•´æœ¬ä¹¦çš„ï¼Œæˆ‘ä»¬è¦ä¸€ä¸ªè¯ä¸€ä¸ªè¯åœ°çœ‹ï¼Œçœ¼ç›ä¸åœç§»åŠ¨è·å–æ–°çš„æ•°æ®çš„åŒæ—¶ï¼Œè®°ä½ä¹‹å‰çš„å†…å®¹ï¼Œå°†æ–°çš„ã€æ—§çš„å†…å®¹è”ç³»åœ¨ä¸€èµ·æ¥ç†è§£æ•´å¥è¯çš„æ„æ€ã€‚è¯´æŠ½è±¡ä¸€äº›ï¼Œæˆ‘ä»¬ä¼šä¿å­˜ä¸€ä¸ªå…³äºæ‰€å¤„ç†å†…å®¹çš„å†…éƒ¨æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹æ ¹æ®è¿‡å»çš„ä¿¡æ¯æ„å»ºï¼Œå¹¶éšç€æ–°ä¿¡æ¯çš„è¿›å…¥è€Œä¸æ–­æ›´æ–°ã€‚æˆ‘ä»¬éƒ½æ˜¯ä»¥è¿™ç§æ¸è¿›çš„æ–¹å¼å¤„ç†ä¿¡æ¯çš„ã€‚

æŒ‰ç…§è¿™ç§æ€æƒ³ï¼Œæˆ‘ä»¬åˆå¾—åˆ°ä¸€ç§æ–°çš„æ¨¡å‹ï¼Œå«åš**å¾ªç¯ç¥ç»ç½‘ç»œ**(recurrent neural network, RNN)ï¼Œè¿™ç½‘ç»œä¼šéå†å¤„ç†æ‰€æœ‰åºåˆ—å…ƒç´ ï¼Œå¹¶ä¿å­˜ä¸€ä¸ªè®°å½•å·²æŸ¥çœ‹å†…å®¹ç›¸å…³ä¿¡æ¯çš„çŠ¶æ€(state)ã€‚è€Œåœ¨å¤„ç†ä¸‹ä¸€æ¡åºåˆ—ä¹‹æ—¶ï¼ŒRNN çŠ¶æ€ä¼šè¢«é‡ç½®ã€‚ä½¿ç”¨ RNN æ—¶ï¼Œæˆ‘ä»¬ä»å¯ä»¥å°†ä¸€ä¸ªåºåˆ—æ•´ä¸ªçš„è¾“å‡ºç½‘ç»œï¼Œä¸è¿‡åœ¨ç½‘ç»œå†…éƒ¨ï¼Œæ•°æ®ä¸å†æ˜¯ç›´æ¥è¢«æ•´ä¸ªå¤„ç†ï¼Œè€Œæ˜¯è‡ªåŠ¨å¯¹åºåˆ—å…ƒç´ è¿›è¡Œéå†ã€‚

![å¾ªç¯ç½‘ç»œ:å¸¦æœ‰ç¯çš„ç½‘ç»œ](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghgvghuih9j30iy0ict9s.jpg)

ä¸ºäº†ç†è§£å¾ªç¯ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬ç”¨ Numpy æ‰‹å†™ä¸€ä¸ªç©å…·ç‰ˆçš„ RNN å‰å‘ä¼ é€’ã€‚è€ƒè™‘å¤„ç†å½¢çŠ¶ä¸º `(timesteps, input_features)` çš„ä¸€æ¡åºåˆ—ï¼ŒRNN åœ¨ timesteps ä¸Šåšè¿­ä»£ï¼Œå°†å½“å‰ timestep çš„ input_features ä¸å‰ä¸€æ­¥å¾—åˆ°çš„çŠ¶æ€ç»“åˆç®—å‡ºè¿™ä¸€æ­¥çš„è¾“å‡ºï¼Œç„¶åå°†è¿™ä¸ªè¾“å‡ºä¿å­˜ä¸ºæ–°çš„çŠ¶æ€ä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨ã€‚ç¬¬ä¸€æ­¥æ—¶ï¼Œæ²¡æœ‰çŠ¶æ€ï¼Œå› æ­¤å°†çŠ¶æ€åˆå§‹åŒ–ä¸ºä¸€ä¸ªå…¨é›¶å‘é‡ï¼Œç§°ä¸ºç½‘ç»œçš„åˆå§‹çŠ¶æ€ã€‚

ä¼ªä»£ç ï¼š

```python
state_t = 0
for input_t in input_sequence:
    output_t = f(input_t, state_t)
    state_t = output_t
```

è¿™é‡Œçš„ `f(...)` å…¶å®å’Œæˆ‘ä»¬çš„ Dense å±‚æ¯”è¾ƒç±»ä¼¼ï¼Œä½†è¿™é‡Œä¸ä»…å¤„ç†è¾“å‡ºï¼Œè¿˜è¦åŒæ—¶åŠ å…¥çŠ¶æ€çš„å½±å“ã€‚æ‰€ä»¥å®ƒå°±éœ€è¦åŒ…å« 3 ä¸ªå‚æ•°ï¼šåˆ†åˆ«ä½œç”¨ä¸è¾“å‡ºå’ŒçŠ¶æ€çš„çŸ©é˜µ Wã€Uï¼Œä»¥åŠåç§»å‘é‡ b:

```python
def f(input_t, state_t):
    return activation(
        dot(W, input_t) + dot(U, state_t) + b
    )
```

ç”»ä¸ªå›¾æ¥è¡¨ç¤ºè¿™ä¸ªç¨‹åºï¼š

![ä¸€ä¸ªç®€å•çš„ RNNï¼Œæ²¿æ—¶é—´å±•å¼€](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghh2b81pn9j31520j276a.jpg)

ä¸‹é¢æŠŠå®ƒå†™æˆçœŸå®çš„ä»£ç ï¼š


```python
import numpy as np

# å®šä¹‰å„ç§ç»´åº¦å¤§å°
timesteps = 100
input_features = 32
output_features = 64

inputs = np.random.random((timesteps, input_features))

state_t = np.zeros((output_features))

W = np.random.random((output_features, input_features))
U = np.random.random((output_features, output_features))
b = np.random.random((output_features))

successive_outputs = []

for input_t in inputs:    # input_t: (input_features, )
    output_t = np.tanh(   # output_t: (output_features, )
        np.dot(W, input_t) + np.dot(U, state_t) + b
    )
    successive_outputs.append(output_t)
    
    state_t = output_t
    
final_output_sequence = np.stack(successive_outputs, axis=0)  # (timesteps, output_features)

print(successive_outputs[-1].shape)
print(final_output_sequence.shape)

```

    (64,)
    (100, 64)


åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ€ç»ˆè¾“å‡ºæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (timesteps, output_features) ï¼Œæ˜¯æ‰€æœ‰ timesteps çš„ç»“æœæ‹¼èµ·æ¥çš„ã€‚ä½†å®é™…ä¸Šï¼Œæˆ‘ä»¬ä¸€èˆ¬åªç”¨æœ€åä¸€ä¸ªç»“æœ `successive_outputs[-1]` å°±è¡Œäº†ï¼Œè¿™ä¸ªé‡Œé¢å·²ç»åŒ…å«äº†ä¹‹å‰æ‰€æœ‰æ­¥éª¤çš„ç»“æœï¼Œå³åŒ…å«äº†æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ã€‚

### Keras ä¸­çš„å¾ªç¯å±‚

æŠŠåˆšæ‰è¿™ä¸ªç©å…·ç‰ˆæœ¬å†åŠ å·¥ä¸€ä¸‹ï¼Œè®©å®ƒèƒ½æ¥æ”¶å½¢çŠ¶ä¸º `(batch_size, timesteps, input_features)` çš„è¾“å…¥ï¼Œæ‰¹é‡å»å¤„ç†ï¼Œå°±å¾—åˆ°äº† keras ä¸­çš„ `SimpleRNN` å±‚ï¼š

```python
from tensorflow.keras.layers import SimpleRNN
```

è¿™ä¸ª SimpleRNN å±‚å’Œ keras ä¸­çš„å…¶ä»–å¾ªç¯å±‚éƒ½æœ‰ä¸¤ç§å¯é€‰çš„è¾“å‡ºæ¨¡å¼ï¼š

| è¾“å‡ºå½¢çŠ¶ | è¯´æ˜ | ä½¿ç”¨ |
| --- | --- | --- |
| `(batch_size, timesteps, output_features)` | è¾“å‡ºæ¯ä¸ª timestep è¾“å‡ºçš„å®Œæ•´åºåˆ— | return_sequences=True |
| `(batch_size, output_features)` | åªè¿”å›æ¯ä¸ªåºåˆ—çš„æœ€ç»ˆè¾“å‡º | return_sequences=False (é»˜è®¤) |




```python
# åªè¿”å›æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN

model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32))
model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding (Embedding)        (None, None, 32)          320000    
    _________________________________________________________________
    simple_rnn (SimpleRNN)       (None, 32)                2080      
    =================================================================
    Total params: 322,080
    Trainable params: 322,080
    Non-trainable params: 0
    _________________________________________________________________



```python
# è¿”å›å®Œæ•´çš„çŠ¶æ€åºåˆ—

model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32, return_sequences=True))
model.summary()
```

    Model: "sequential_2"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding_2 (Embedding)      (None, None, 32)          320000    
    _________________________________________________________________
    simple_rnn_2 (SimpleRNN)     (None, None, 32)          2080      
    =================================================================
    Total params: 322,080
    Trainable params: 322,080
    Non-trainable params: 0
    _________________________________________________________________


å¦‚æœæˆ‘ä»¬è¦å †å ä½¿ç”¨å¤šä¸ª RNN å±‚çš„æ—¶å€™ï¼Œä¸­é—´çš„å±‚å¿…é¡»è¿”å›å®Œæ•´çš„çŠ¶æ€åºåˆ—ï¼š


```python
# å †å å¤šä¸ª RNN å±‚ï¼Œä¸­é—´å±‚è¿”å›å®Œæ•´çš„çŠ¶æ€åºåˆ—

model = Sequential()
model.add(Embedding(10000, 32))
model.add(SimpleRNN(32, return_sequences=True))
model.add(SimpleRNN(32, return_sequences=True))
model.add(SimpleRNN(32, return_sequences=True))
model.add(SimpleRNN(32))    # æœ€åä¸€å±‚è¦æœ€åä¸€ä¸ªè¾“å‡ºå°±è¡Œäº†
model.summary()
```

    Model: "sequential_3"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding_3 (Embedding)      (None, None, 32)          320000    
    _________________________________________________________________
    simple_rnn_3 (SimpleRNN)     (None, None, 32)          2080      
    _________________________________________________________________
    simple_rnn_4 (SimpleRNN)     (None, None, 32)          2080      
    _________________________________________________________________
    simple_rnn_5 (SimpleRNN)     (None, None, 32)          2080      
    _________________________________________________________________
    simple_rnn_6 (SimpleRNN)     (None, 32)                2080      
    =================================================================
    Total params: 328,320
    Trainable params: 328,320
    Non-trainable params: 0
    _________________________________________________________________


æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°è¯•ç”¨ RNN å†æ¬¡å¤„ç† IMDB é—®é¢˜ã€‚é¦–å…ˆï¼Œå‡†å¤‡æ•°æ®ï¼š


```python
# å‡†å¤‡ IMDB æ•°æ®

from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing import sequence

max_features = 10000
maxlen = 500
batch_size = 32

print('Loading data...')
(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)
print(len(input_train), 'train sequences')
print(len(input_test), 'test sequences')

print('Pad sequences (samples x time)')
input_train = sequence.pad_sequences(input_train, maxlen=maxlen)
input_test = sequence.pad_sequences(input_test, maxlen=maxlen)
print('input_train shape:', input_train.shape)
print('input_train shape:', input_test.shape)

```

    Loading data...
    25000 train sequences
    25000 test sequences
    Pad sequences (samples x time)
    input_train shape: (25000, 500)
    input_train shape: (25000, 500)


æ„å»ºå¹¶è®­ç»ƒç½‘ç»œï¼š


```python
# ç”¨ Embedding å±‚å’Œ SimpleRNN å±‚æ¥è®­ç»ƒæ¨¡å‹

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

model = Sequential()
model.add(Embedding(max_features, 32))
model.add(SimpleRNN(32))
model.add(Dense(1, activation='sigmoid'))

model.summary()

model.compile(optimizer='rmsprop', 
              loss='binary_crossentropy', 
              metrics=['acc'])
history = model.fit(input_train, y_train, 
                    epochs=10, 
                    batch_size=128, 
                    validation_split=0.2)

```

    Model: "sequential_4"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding_4 (Embedding)      (None, None, 32)          320000    
    _________________________________________________________________
    simple_rnn_7 (SimpleRNN)     (None, 32)                2080      
    _________________________________________________________________
    dense (Dense)                (None, 1)                 33        
    =================================================================
    Total params: 322,113
    Trainable params: 322,113
    Non-trainable params: 0
    _________________________________________________________________
    Epoch 1/10
    157/157 [==============================] - 17s 107ms/step - loss: 0.6445 - acc: 0.6106 - val_loss: 0.6140 - val_acc: 0.6676
    Epoch 2/10
    157/157 [==============================] - 20s 129ms/step - loss: 0.4139 - acc: 0.8219 - val_loss: 0.4147 - val_acc: 0.8194
    Epoch 3/10
    157/157 [==============================] - 20s 124ms/step - loss: 0.3041 - acc: 0.8779 - val_loss: 0.4529 - val_acc: 0.8012
    Epoch 4/10
    157/157 [==============================] - 18s 115ms/step - loss: 0.2225 - acc: 0.9151 - val_loss: 0.3957 - val_acc: 0.8572
    Epoch 5/10
    157/157 [==============================] - 18s 115ms/step - loss: 0.1655 - acc: 0.9391 - val_loss: 0.4416 - val_acc: 0.8246
    Epoch 6/10
    157/157 [==============================] - 17s 111ms/step - loss: 0.1167 - acc: 0.9601 - val_loss: 0.4614 - val_acc: 0.8606
    Epoch 7/10
    157/157 [==============================] - 17s 109ms/step - loss: 0.0680 - acc: 0.9790 - val_loss: 0.4754 - val_acc: 0.8408
    Epoch 8/10
    157/157 [==============================] - 15s 95ms/step - loss: 0.0419 - acc: 0.9875 - val_loss: 0.5337 - val_acc: 0.8352
    Epoch 9/10
    157/157 [==============================] - 16s 99ms/step - loss: 0.0246 - acc: 0.9935 - val_loss: 0.5796 - val_acc: 0.8468
    Epoch 10/10
    157/157 [==============================] - 15s 96ms/step - loss: 0.0174 - acc: 0.9952 - val_loss: 0.7274 - val_acc: 0.7968


ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹çœ‹çœ‹ï¼š


```python
# ç»˜åˆ¶ç»“æœ

import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo-', label='Training acc')
plt.plot(epochs, val_acc, 'rs-', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'rs-', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnskm7t47j30al07caac.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnskna7p7j30af07c74j.jpg)


Emmmmï¼Œå…¶å®å§ï¼Œè¿™ä¸ªæ¨¡å‹çš„ç»“æœè¿˜æ²¡æœ‰ç¬¬ä¸‰ç« é‡Œé¢çš„ç”¨å‡ ä¸ªå…¨è¿æ¥å±‚å †å èµ·æ¥çš„æ¨¡å‹å¥½ã€‚åŸå› æœ‰å¥½å‡ ä¸ªï¼Œä¸€ä¸ªæ˜¯æˆ‘ä»¬è¿™é‡Œåªè€ƒè™‘äº†æ¯ä¸ªåºåˆ—çš„å‰ 500 ä¸ªå•è¯ï¼Œè¿˜æœ‰ä¸€ä¸ªæ˜¯ SimpleRNN å…¶å®å¹¶ä¸æ“…é•¿å¤„ç†å¾ˆé•¿çš„åºåˆ—ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼šçœ‹å‡ ä¸ªèƒ½è¡¨ç°çš„æ›´å¥½çš„å¾ªç¯å±‚ã€‚

#### LSTM å±‚å’Œ GRU å±‚

åœ¨ Keras ä¸­çš„å¾ªç¯å±‚ï¼Œé™¤äº† SimpleRNNï¼Œè¿˜æœ‰æ›´â€œä¸simpleâ€ä¸€äº›çš„ LSTM å±‚å’Œ GRU å±‚ï¼Œåé¢è¿™ä¸¤ç§ä¼šæ›´å¸¸ç”¨ã€‚

SimpleRNN æ˜¯æœ‰ä¸€äº›é—®é¢˜çš„ï¼Œç†è®ºä¸Šï¼Œåœ¨éå†åˆ°æ—¶é—´æ­¥ t çš„æ—¶å€™ï¼Œå®ƒåº”è¯¥æ˜¯èƒ½ç•™å­˜ç€ä¹‹å‰è®¸å¤šæ­¥ä»¥æ¥è§è¿‡çš„ä¿¡æ¯çš„ï¼Œä½†å®é™…çš„åº”ç”¨ä¸­ï¼Œç”±äºæŸç§å«åš vanishing gradient problemï¼ˆæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼‰çš„ç°è±¡ï¼Œå®ƒå¹¶ä¸èƒ½å­¦åˆ°è¿™ç§é•¿æœŸä¾èµ–ã€‚

æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å…¶å®åœ¨å±‚æ•°æ¯”è¾ƒå¤šçš„å‰é¦ˆç½‘ç»œé‡Œé¢ä¹Ÿä¼šæœ‰å‘ç”Ÿï¼Œä¸»è¦è¡¨ç°å°±æ˜¯éšç€å±‚æ•°å¤šäº†ä¹‹åï¼Œç½‘ç»œæ— æ³•è®­ç»ƒäº†ã€‚LSTM å±‚å’Œ GRU å±‚å°±æ˜¯å¯¹æŠ—è¿™ç§é—®é¢˜è€Œç”Ÿçš„ã€‚

**LSTM** å±‚æ˜¯åŸºäº LSTM (é•¿çŸ­æœŸè®°å¿†ï¼Œlong short-term memory) ç®—æ³•çš„ï¼Œè¿™ç®—æ³•å°±æ˜¯ä¸“é—¨ç ”ç©¶äº†å¤„ç†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„ã€‚å…¶å®å®ƒçš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯è¦ä¿å­˜ä¿¡æ¯ä»¥ä¾¿åé¢ä½¿ç”¨ï¼Œé˜²æ­¢å‰é¢å¾—åˆ°çš„ä¿¡æ¯åœ¨åé¢çš„å¤„ç†ä¸­é€æ¸æ¶ˆå¤±ã€‚

LSTM åœ¨ SimpleRNN çš„åŸºç¡€ä¸Šï¼Œå¢åŠ äº†ä¸€ç§è·¨è¶Šå¤šä¸ªæ—¶é—´æ­¥ä¼ é€’ä¿¡æ¯çš„æ–¹æ³•ã€‚è¿™ä¸ªæ–°æ–¹æ³•åšçš„äº‹æƒ…å°±åƒä¸€æ¡åœ¨åºåˆ—æ—è¾¹çš„è¾…åŠ©ä¼ é€å¸¦ï¼Œåºåˆ—ä¸­çš„ä¿¡æ¯å¯ä»¥åœ¨ä»»æ„ä½ç½®è·³ä¸Šä¼ é€å¸¦ï¼Œ ç„¶åè¢«ä¼ é€åˆ°æ›´æ™šçš„æ—¶é—´æ­¥ï¼Œå¹¶åœ¨éœ€è¦æ—¶åŸå°ä¸åŠ¨åœ°è·³å›æ¥ã€‚


![å‰–æ LSTMï¼Œä» SimpleRNN åˆ° LSTM:æ·»åŠ ä¸€ä¸ªæºå¸¦è½¨é“](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghhebke5inj31p80rq78o.jpg)

è¿™é‡ŒæŠŠä¹‹å‰ SimpleRNN é‡Œé¢çš„æƒé‡ Wã€U é‡å‘½åä¸º Woã€Uo äº†ï¼ˆo è¡¨ç¤º outputï¼‰ã€‚ç„¶ååŠ äº†ä¸€ä¸ªâ€œæºå¸¦è½¨é“â€æ•°æ®æµï¼Œè¿™ä¸ªæºå¸¦è½¨é“å°±æ˜¯ç”¨æ¥æºå¸¦ä¿¡æ¯è·¨è¶Šæ—¶é—´æ­¥çš„ã€‚è¿™ä¸ªæºå¸¦è½¨é“ä¸Šé¢æ”¾ç€æ—¶é—´æ­¥ t çš„ ct ä¿¡æ¯ï¼ˆc è¡¨ç¤º carryï¼‰ï¼Œè¿™äº›ä¿¡æ¯å°†ä¸è¾“å…¥ã€çŠ¶æ€ä¸€èµ·è¿›è¡Œè¿ç®—ï¼Œè€Œå½±å“ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€ï¼š

```pythoon
output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)

i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)
f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)
k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)

c_t_next = i_t * k_t + c_t * f_t
```

å…³äº LSTM æ›´å¤šçš„ç»†èŠ‚ã€å†…éƒ¨å®ç°å°±ä¸ä»‹ç»äº†ã€‚å’±å®Œå…¨ä¸éœ€è¦ç†è§£å…³äº LSTM å•å…ƒçš„å…·ä½“æ¶æ„ï¼Œç†è§£è¿™ä¸œè¥¿å°±ä¸æ˜¯äººå¹²çš„äº‹ã€‚æˆ‘ä»¬åªéœ€è¦è®°ä½ LSTM å•å…ƒçš„ä½œç”¨: å…è®¸æŠŠè¿‡å»çš„ä¿¡æ¯ç¨åå†æ¬¡æ‹¿è¿›æ¥ç”¨ï¼Œä»è€Œå¯¹æŠ—æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

(P.S. ä½œè€…è¯´è¿™é‡Œæ˜¯ç„å­¦ï¼Œä¿¡ä»–å°±è¡Œäº†ã€‚ğŸ¤ª Emmmï¼Œè¿™å¥æ˜¯æˆ‘èƒ¡ç¿»çš„ï¼ŒåŸè¯æ˜¯:â€œit may seem a bit arbitrary, but bear with me.â€)

**GRU**ï¼ˆGated Recurrent Unit, é—¨æ§å¾ªç¯å•å…ƒï¼‰ï¼Œä¹¦ä¸Šæçš„æ¯”è¾ƒå°‘ï¼Œå‚è€ƒè¿™ç¯‡ ã€Š[äººäººéƒ½èƒ½çœ‹æ‡‚çš„GRU](https://zhuanlan.zhihu.com/p/32481747)ã€‹ï¼Œè¯´ GRU å¤§æ¦‚æ˜¯ LSTM çš„ä¸€ç§å˜ç§å§ï¼ŒäºŒè€…åŸç†åŒºåˆ«ä¸å¤§ã€å®é™…æ•ˆæœä¸Šä¹Ÿå·®ä¸å¤šã€‚ä½† GRU æ¯” LSTM æ–°ä¸€äº›ï¼Œå®ƒåšäº†ä¸€äº›ç®€åŒ–ï¼Œæ›´å®¹æ˜“è®¡ç®—ä¸€äº›ï¼Œä½†ç›¸åº”è¡¨ç¤ºèƒ½åŠ›å¯èƒ½ç¨å·®ä¸€ç‚¹ç‚¹ã€‚

#### Keras ä¸­ä½¿ç”¨ LSTM

æˆ‘ä»¬è¿˜æ˜¯ç»§ç»­ç”¨ä¹‹å‰å¤„ç†å¥½çš„çš„ IMDB æ•°æ®æ¥è·‘ä¸€ä¸ª LSTMï¼š


```python
from tensorflow.keras.layers import LSTM

model = Sequential()
model.add(Embedding(max_features, 32))
model.add(LSTM(32))
model.add(Dense(1, activation='sigmoid'))

model.summary()

model.compile(optimizer='rmsprop', 
              loss='binary_crossentropy', 
              metrics=['acc'])

history = model.fit(input_train, y_train, 
                    epochs=10, 
                    batch_size=128, 
                    validation_split=0.2)
```

    Model: "sequential_5"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding_5 (Embedding)      (None, None, 32)          320000    
    _________________________________________________________________
    lstm (LSTM)                  (None, 32)                8320      
    _________________________________________________________________
    dense_1 (Dense)              (None, 1)                 33        
    =================================================================
    Total params: 328,353
    Trainable params: 328,353
    Non-trainable params: 0
    _________________________________________________________________
    Epoch 1/10
    157/157 [==============================] - 37s 236ms/step - loss: 0.5143 - acc: 0.7509 - val_loss: 0.3383 - val_acc: 0.8672
    Epoch 2/10
    157/157 [==============================] - 37s 235ms/step - loss: 0.3010 - acc: 0.8834 - val_loss: 0.2817 - val_acc: 0.8862
    Epoch 3/10
    157/157 [==============================] - 34s 215ms/step - loss: 0.2357 - acc: 0.9129 - val_loss: 0.2766 - val_acc: 0.8876
    Epoch 4/10
    157/157 [==============================] - 34s 215ms/step - loss: 0.2062 - acc: 0.9255 - val_loss: 0.4392 - val_acc: 0.8310
    Epoch 5/10
    157/157 [==============================] - 34s 215ms/step - loss: 0.1762 - acc: 0.9360 - val_loss: 0.3078 - val_acc: 0.8670
    Epoch 6/10
    157/157 [==============================] - 34s 215ms/step - loss: 0.1575 - acc: 0.9436 - val_loss: 0.3293 - val_acc: 0.8902
    Epoch 7/10
    157/157 [==============================] - 35s 222ms/step - loss: 0.1419 - acc: 0.9506 - val_loss: 0.2993 - val_acc: 0.8898
    Epoch 8/10
    157/157 [==============================] - 39s 246ms/step - loss: 0.1277 - acc: 0.9546 - val_loss: 0.4179 - val_acc: 0.8234
    Epoch 9/10
    157/157 [==============================] - 35s 225ms/step - loss: 0.1199 - acc: 0.9585 - val_loss: 0.4391 - val_acc: 0.8434
    Epoch 10/10
    157/157 [==============================] - 34s 217ms/step - loss: 0.1113 - acc: 0.9615 - val_loss: 0.4926 - val_acc: 0.8614



```python
# ç»˜åˆ¶ç»“æœ

import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo-', label='Training acc')
plt.plot(epochs, val_acc, 'rs-', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'rs-', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnsko5b0pj30al07cdg3.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnskmosogj30al07cdg5.jpg)


æ¯” SimpleRNN å¥½å¤šäº†ã€‚ä½†ä¹Ÿæ²¡æ¯”ä»¥å‰é‚£ç§ç”¨å…¨è¿æ¥å±‚çš„ç½‘ç»œå¥½å¤šå°‘ï¼Œè€Œä¸”è¿˜æ¯”è¾ƒæ…¢(è®¡ç®—ä»£ä»·å¤§)ï¼Œå…¶å®ä¸»è¦æ˜¯ç”±äºæƒ…æ„Ÿåˆ†æè¿™æ ·çš„é—®é¢˜ï¼Œç”¨ LSTM å»åˆ†æå…¨å±€çš„é•¿æœŸæ€§ç»“æ„å¸®åŠ©å¹¶ä¸æ˜¯å¾ˆå¤§ï¼ŒLSTM æ“…é•¿çš„æ˜¯æ›´å¤æ‚çš„è‡ªç„¶è¯­è¨€å¤„ç†é—®é¢˜ï¼Œæ¯”å¦‚æœºå™¨ç¿»è¯‘ã€‚ç”¨å…¨è¿æ¥çš„æ–¹æ³•ï¼Œå…¶å®æ˜¯åšäº†çœ‹å‡ºç°äº†å“ªäº›è¯åŠå…¶å‡ºç°é¢‘ç‡ï¼Œè¿™ä¸ªå¯¹è¿™ç§ç®€å•é—®é¢˜è¿˜æ¯”è¾ƒæœ‰æ•ˆã€‚

ç„¶åï¼Œæˆ‘ä»¬å†è¯•è¯•ä¹¦ä¸Šæ²¡æçš„ GRUï¼š


```python
# æŠŠ LSTM æ”¹æˆç”¨ GRU

from tensorflow.keras.layers import GRU

model = Sequential()
model.add(Embedding(max_features, 32))
model.add(GRU(32))
model.add(Dense(1, activation='sigmoid'))

model.summary()

model.compile(optimizer='rmsprop', 
              loss='binary_crossentropy', 
              metrics=['acc'])

history = model.fit(input_train, y_train, 
                    epochs=10, 
                    batch_size=128, 
                    validation_split=0.2)

# ç»˜åˆ¶ç»“æœ

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo-', label='Training acc')
plt.plot(epochs, val_acc, 'rs-', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'rs-', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
```

    Model: "sequential_6"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding_6 (Embedding)      (None, None, 32)          320000    
    _________________________________________________________________
    gru (GRU)                    (None, 32)                6336      
    _________________________________________________________________
    dense_2 (Dense)              (None, 1)                 33        
    =================================================================
    Total params: 326,369
    Trainable params: 326,369
    Non-trainable params: 0
    _________________________________________________________________
    Epoch 1/10
    157/157 [==============================] - 37s 238ms/step - loss: 0.5119 - acc: 0.7386 - val_loss: 0.3713 - val_acc: 0.8434
    Epoch 2/10
    157/157 [==============================] - 36s 232ms/step - loss: 0.2971 - acc: 0.8806 - val_loss: 0.3324 - val_acc: 0.8722
    Epoch 3/10
    157/157 [==============================] - 37s 235ms/step - loss: 0.2495 - acc: 0.9034 - val_loss: 0.3148 - val_acc: 0.8722
    Epoch 4/10
    157/157 [==============================] - 34s 217ms/step - loss: 0.2114 - acc: 0.9200 - val_loss: 0.3596 - val_acc: 0.8738
    Epoch 5/10
    157/157 [==============================] - 36s 231ms/step - loss: 0.1872 - acc: 0.9306 - val_loss: 0.5291 - val_acc: 0.8084
    Epoch 6/10
    157/157 [==============================] - 35s 226ms/step - loss: 0.1730 - acc: 0.9359 - val_loss: 0.3976 - val_acc: 0.8802
    Epoch 7/10
    157/157 [==============================] - 34s 217ms/step - loss: 0.1523 - acc: 0.9452 - val_loss: 0.4303 - val_acc: 0.8532
    Epoch 8/10
    157/157 [==============================] - 34s 217ms/step - loss: 0.1429 - acc: 0.9486 - val_loss: 0.4019 - val_acc: 0.8542
    Epoch 9/10
    157/157 [==============================] - 34s 217ms/step - loss: 0.1258 - acc: 0.9562 - val_loss: 0.3476 - val_acc: 0.8746
    Epoch 10/10
    157/157 [==============================] - 34s 216ms/step - loss: 0.1191 - acc: 0.9585 - val_loss: 0.3558 - val_acc: 0.8812



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnskl04gej30al07cjrn.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghnsklsn9gj30af07cglv.jpg)


æ‰€ä»¥è¯´ï¼Œç»“æœåŒºåˆ«ä¸å¤§ã€‚
