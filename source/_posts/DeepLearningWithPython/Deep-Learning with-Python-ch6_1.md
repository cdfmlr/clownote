---
date: 2020-08-11 15:00:52
title: æ·±åº¦å­¦ä¹ ä¹‹å¤„ç†æ–‡æœ¬æ•°æ®
---
# Deep Learning with Python

è¿™ç¯‡æ–‡ç« æ˜¯æˆ‘å­¦ä¹ ã€ŠDeep Learning with Pythonã€‹(ç¬¬äºŒç‰ˆï¼ŒFranÃ§ois Chollet è‘—) æ—¶å†™çš„ç³»åˆ—ç¬”è®°ä¹‹ä¸€ã€‚æ–‡ç« çš„å†…å®¹æ˜¯ä»  Jupyter notebooks è½¬æˆ Markdown çš„ï¼Œä½ å¯ä»¥å» [GitHub](https://github.com/cdfmlr/Deep-Learning-with-Python-Notebooks) æˆ– [Gitee](https://gitee.com/cdfmlr/Deep-Learning-with-Python-Notebooks) æ‰¾åˆ°åŸå§‹çš„ `.ipynb` ç¬”è®°æœ¬ã€‚

ä½ å¯ä»¥å»[è¿™ä¸ªç½‘ç«™åœ¨çº¿é˜…è¯»è¿™æœ¬ä¹¦çš„æ­£ç‰ˆåŸæ–‡](https://livebook.manning.com/book/deep-learning-with-python)(è‹±æ–‡)ã€‚è¿™æœ¬ä¹¦çš„ä½œè€…ä¹Ÿç»™å‡ºäº†é…å¥—çš„ [Jupyter notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks)ã€‚

æœ¬æ–‡ä¸º **ç¬¬6ç«   æ·±åº¦å­¦ä¹ ç”¨äºæ–‡æœ¬å’Œåºåˆ—** (Chapter 6. *Deep learning for text and sequences*) çš„ç¬”è®°ã€‚

[TOC]

## 6.1  Working with text data

> å¤„ç†æ–‡æœ¬æ•°æ®

è¦ç”¨æ·±åº¦å­¦ä¹ çš„ç¥ç»ç½‘ç»œå¤„ç†æ–‡æœ¬æ•°æ®ï¼Œå’Œå›¾ç‰‡ç±»ä¼¼ï¼Œä¹Ÿè¦æŠŠæ•°æ®å‘é‡åŒ–ï¼šæ–‡æœ¬ -> æ•°å€¼å¼ é‡ã€‚

è¦åšè¿™ç§äº‹æƒ…å¯ä»¥æŠŠæ¯ä¸ªå•è¯å˜æˆå‘é‡ï¼Œä¹Ÿå¯ä»¥æŠŠå­—ç¬¦å˜æˆå‘é‡ï¼Œè¿˜å¯ä»¥æŠŠå¤šä¸ªè¿ç»­å•è¯æˆ–å­—ç¬¦(ç§°ä¸º *N-grams*)å˜æˆå‘é‡ã€‚

åæ­£ä¸ç®¡å¦‚ä½•åˆ’åˆ†ï¼Œæˆ‘ä»¬æŠŠæ–‡æœ¬æ‹†åˆ†å‡ºæ¥çš„å•å…ƒå«åš *tokens*ï¼ˆæ ‡è®°ï¼‰ï¼Œæ‹†åˆ†æ–‡æœ¬çš„è¿‡ç¨‹å«åš *tokenization*(åˆ†è¯)ã€‚

> æ³¨ï¼štoken çš„ä¸­æ–‡ç¿»è¯‘æ˜¯â€œæ ‡è®°â€ğŸ˜‚ã€‚è¿™äº›ç¿»è¯‘éƒ½æ€ªæ€ªçš„ï¼Œè™½ç„¶ token ç¡®å®æœ‰æ ‡è®°è¿™ä¸ªæ„æ€ï¼Œä½†æŠŠè¿™é‡Œçš„ token ç¿»è¯‘æˆæ ‡è®°å°±æ²¡å†…å‘³å„¿äº†ã€‚æˆ‘è§‰å¾— token æ˜¯é‚£ç§ä»¥ä¸€ä¸ªä¸œè¥¿ä»£è¡¨å¦ä¸€ä¸ªä¸œè¥¿æ¥ä½¿ç”¨çš„æ„æ€ï¼Œè¿™ç§ token æ˜¯ä¸€ç§æœ‰å®ä½“çš„ä¸œè¥¿ï¼Œæ¯”å¦‚ä»£é‡‘åˆ¸ã€‚â€œæ ‡è®°â€è¿™ä¸ªè¯åœ¨å­—å…¸ä¸Šä½œåè¯æ˜¯ã€Œèµ·æ ‡ç¤ºä½œç”¨çš„è®°å·ã€çš„æ„æ€ï¼Œè€Œæˆ‘è§‰å¾—è®°å·ä¸æ˜¯ä¸ªå¾ˆå®ä½“çš„ä¸œè¥¿ã€‚ä»£é‡‘åˆ¸ä¸æ˜¯ä¸€ç§è®°å·ã€ä¹Ÿå°±èƒ½è¯´æ˜¯æ ‡è®°ï¼ŒåŒæ ·çš„ï¼Œè¿™é‡Œçš„ token ä¹Ÿæ˜¯ä¸€ç§å®ä½“çš„ä¸œè¥¿ï¼Œæˆ‘è§‰å¾—ä¸èƒ½æŠŠå®ƒè¯´æˆæ˜¯â€œæ ‡è®°â€ã€‚æˆ‘ä¸èµåŒè¿™ç§è¯‘æ³•ï¼Œæ‰€ä»¥ä¸‹æ–‡æ‰€æœ‰æ¶‰åŠ token çš„åœ°æ–¹ç»Ÿä¸€å†™æˆ â€œtokenâ€ï¼Œä¸ç¿»è¯‘æˆâ€œæ ‡è®°â€ã€‚


æ–‡æœ¬çš„å‘é‡åŒ–å°±æ˜¯å…ˆä½œåˆ†è¯ï¼Œç„¶åæŠŠç”Ÿæˆå‡ºæ¥çš„ token é€ä¸ªä¸æ•°å€¼å‘é‡å¯¹åº”èµ·æ¥ï¼Œæœ€åæ‹¿å¯¹åº”çš„æ•°å€¼å‘é‡åˆæˆä¸€ä¸ªè¡¨è¾¾äº†åŸæ–‡æœ¬çš„å¼ é‡ã€‚å…¶ä¸­ï¼Œæ¯”è¾ƒæœ‰æ„æ€çš„æ˜¯å¦‚ä½•å»ºç«‹ token å’Œ æ•°å€¼å‘é‡ çš„è”ç³»ï¼Œä¸‹é¢ä»‹ç»ä¸¤ç§æè¿™ä¸ªçš„æ–¹æ³•ï¼šone-hot encoding(one-hotç¼–ç ) å’Œ token embedding(æ ‡è®°åµŒå…¥)ï¼Œå…¶ä¸­ token embedding ä¸€èˆ¬éƒ½ç”¨äºå•è¯ï¼Œå«ä½œè¯åµŒå…¥ã€Œword embeddingã€ã€‚

![æ–‡æœ¬çš„å‘é‡åŒ–ï¼šä»æ–‡æœ¬åˆ°tokenå†åˆ°å¼ é‡](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghek3mhp38j31320mg0v8.jpg)


### n-grams å’Œè¯è¢‹(bag-of-words)

n-gram æ˜¯èƒ½ä»ä¸€ä¸ªå¥å­ä¸­æå–å‡ºçš„ â‰¤N ä¸ªè¿ç»­å•è¯çš„é›†åˆã€‚ä¾‹å¦‚ï¼šã€ŒThe cat sat on the mat.ã€

è¿™ä¸ªå¥å­åˆ†è§£æˆ 2-gram æ˜¯ï¼š

```
{"The", "The cat", "cat", "cat sat", "sat",
  "sat on", "on", "on the", "the", "the mat", "mat"}
```

è¿™ä¸ªé›†åˆè¢«å«åš bag-of-2-grams (äºŒå…ƒè¯­æ³•è¢‹)ã€‚

åˆ†è§£æˆ 3-gram æ˜¯ï¼š

```
{"The", "The cat", "cat", "cat sat", "The cat sat",
  "sat", "sat on", "on", "cat sat on", "on the", "the",
  "sat on the", "the mat", "mat", "on the mat"}
```

è¿™ä¸ªé›†åˆè¢«å«åš bag-of-3-grams (ä¸‰å…ƒè¯­æ³•è¢‹)ã€‚

æŠŠè¿™ä¸œè¥¿å«åšã€Œè¢‹ã€æ˜¯å› ä¸ºå®ƒåªæ˜¯ tokens ç»„æˆçš„é›†åˆï¼Œæ²¡æœ‰åŸæ¥æ–‡æœ¬çš„é¡ºåºå’Œæ„ä¹‰ã€‚æŠŠæ–‡æœ¬åˆ†æˆè¿™ç§è¢‹çš„åˆ†è¯æ–¹æ³•å«åšã€Œè¯è¢‹(bag-of-words)ã€ã€‚

ç”±äºè¯è¢‹æ˜¯ä¸ä¿å­˜é¡ºåºçš„ï¼ˆåˆ†å‡ºæ¥æ˜¯é›†åˆï¼Œä¸æ˜¯åºåˆ—ï¼‰ï¼Œæ‰€ä»¥ä¸€èˆ¬ä¸åœ¨æ·±åº¦å­¦ä¹ é‡Œé¢ç”¨ã€‚ä½†åœ¨è½»é‡çº§çš„æµ…å±‚æ–‡æœ¬å¤„ç†æ¨¡å‹é‡Œé¢ï¼Œn-gram å’Œè¯è¢‹è¿˜æ˜¯å¾ˆé‡è¦çš„æ–¹æ³•çš„ã€‚

### one-hot ç¼–ç 

one-hot æ˜¯æ¯”è¾ƒåŸºæœ¬ã€å¸¸ç”¨çš„ã€‚å…¶åšæ³•æ˜¯å°†æ¯ä¸ª token ä¸ä¸€ä¸ªå”¯ä¸€æ•´æ•°ç´¢å¼•å…³è”ï¼Œ ç„¶åå°†æ•´æ•°ç´¢å¼• i è½¬æ¢ä¸ºé•¿åº¦ä¸º N çš„äºŒè¿›åˆ¶å‘é‡(N æ˜¯è¯è¡¨å¤§å°)ï¼Œè¿™ä¸ªå‘é‡åªæœ‰ç¬¬ i ä¸ªå…ƒç´ ä¸º 1ï¼Œå…¶ä½™å…ƒç´ éƒ½ä¸º 0ã€‚

ä¸‹é¢ç»™å‡ºä¸¤ä¸ªç©å…·ç‰ˆæœ¬çš„ one-hot ç¼–ç ç¤ºä¾‹ï¼š


```python
# å•è¯çº§çš„ one-hot ç¼–ç 

import numpy as np

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

token_index = {}
for sample in samples:
    for word in sample.split():
        if word not in token_index:
            token_index[word] = len(token_index) + 1
            
# å¯¹æ ·æœ¬è¿›è¡Œåˆ†è¯ã€‚åªè€ƒè™‘æ¯ä¸ªæ ·æœ¬å‰ max_length ä¸ªå•è¯
max_length = 10

results = np.zeros(shape=(len(samples), 
                          max_length, 
                          max(token_index.values()) + 1))
for i, sample in enumerate(samples):
    for j, word in list(enumerate(sample.split()))[:max_length]:
        index = token_index.get(word)
        results[i, j, index] = 1.

print(results)
```

    [[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
    
     [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
      [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]



```python
# å­—ç¬¦çº§çš„ one-hot ç¼–ç 

import string

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

characters = string.printable    # æ‰€æœ‰å¯æ‰“å°çš„ ASCII å­—ç¬¦
token_index = dict(zip(range(1, len(characters) + 1), characters))

max_length = 50
results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))
for i, sample in enumerate(samples):
    for j, character in enumerate(sample):
        index = token_index.get(character)
        results[i, j, index] = 1.
        
print(results)
```

    [[[1. 1. 1. ... 1. 1. 1.]
      [1. 1. 1. ... 1. 1. 1.]
      [1. 1. 1. ... 1. 1. 1.]
      ...
      [0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]]
    
     [[1. 1. 1. ... 1. 1. 1.]
      [1. 1. 1. ... 1. 1. 1.]
      [1. 1. 1. ... 1. 1. 1.]
      ...
      [0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]]]


Keras å†…ç½®äº†æ¯”åˆšæ‰å†™çš„è¿™ç§ç©å…·ç‰ˆæœ¬å¼ºå¤§å¾—å¤šçš„ one-hot ç¼–ç å·¥å…·ï¼Œåœ¨ç°å®ä½¿ç”¨ä¸­ï¼Œä½ åº”è¯¥ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œè€Œä¸æ˜¯ä½¿ç”¨åˆšæ‰çš„ç©å…·ç‰ˆæœ¬ï¼š


```python
from tensorflow.keras.preprocessing.text import Tokenizer

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

tokenizer = Tokenizer(num_words=1000)    # åªè€ƒè™‘å‰ 1000 ä¸ªæœ€å¸¸è§çš„å•è¯
tokenizer.fit_on_texts(samples)

sequences = tokenizer.texts_to_sequences(samples)    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•ç»„æˆçš„åˆ—è¡¨
print('sequences:', sequences)

one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')  # ç›´æ¥å¾—åˆ° one-hot äºŒè¿›åˆ¶è¡¨ç¤º

word_index = tokenizer.word_index    # å•è¯ç´¢å¼•ï¼Œå°±æ˜¯è¯è¡¨å­—å…¸å•¦ï¼Œç”¨è¿™ä¸ªå°±å¯ä»¥è¿˜åŸæ•°æ®

print(f'one_hot_results: shape={one_hot_results.shape}:\n', one_hot_results, )
print(f'Found {len(word_index)} unique tokens.', 'word_index:', word_index)
```

    sequences: [[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]
    one_hot_results: shape=(2, 1000):
     [[0. 1. 1. ... 0. 0. 0.]
     [0. 1. 0. ... 0. 0. 0.]]
    Found 9 unique tokens. word_index: {'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}


è¿™ç§ one-hot ç¼–ç è¿˜æœ‰ä¸€ç§ç®€å•çš„å˜ç§å«åš *one-hot hashing trick*ï¼ˆone-hot æ•£åˆ—æŠ€å·§ï¼‰ï¼Œè¿™ä¸ªæ–¹æ³•çš„æ€æƒ³æ˜¯ä¸å¯¹æ¯ä¸ª token å…³è”å”¯ä¸€çš„æ•´æ•°ç´¢å¼•ï¼Œè€Œæ˜¯ç”¨å“ˆå¸Œå‡½æ•°å»ä½œç”¨ï¼ŒæŠŠæ–‡æœ¬ç›´æ¥æ˜ å°„æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡ã€‚

ç”¨è¿™ç§æ–¹æ³•å¯ä»¥èŠ‚çœç»´æŠ¤å•è¯ç´¢å¼•çš„å†…å­˜å¼€é”€ï¼Œè¿˜å¯ä»¥å®ç°åœ¨çº¿ç¼–ç ï¼ˆæ¥ä¸€ä¸ªç¼–ç ä¸€ä¸ªï¼Œä¸å½±å“ä¹‹ã€ä¹‹åçš„ï¼‰ï¼›ä½†ä¹Ÿæœ‰ä¸€äº›å¼Šç«¯ï¼šå¯èƒ½å‡ºç°æ•£åˆ—å†²çªï¼Œç¼–ç åçš„æ•°æ®ä¹Ÿä¸èƒ½å¤Ÿè¿˜åŸã€‚


```python
# ä½¿ç”¨æ•£åˆ—æŠ€å·§çš„å•è¯çº§çš„ one-hot ç¼–ç ï¼Œç©å…·ç‰ˆæœ¬

samples = ['The cat sat on the mat.', 'The dog ate my homework.']

dimensionality = 1000  # å°†å•è¯ä¿å­˜ä¸ºé•¿åº¦ä¸º 1000 çš„å‘é‡ï¼Œå•è¯è¶Šå¤šè¿™ä¸ªå€¼å°±è¦è¶Šå¤§ï¼Œä¸ç„¶æ•£åˆ—å†²çªå¯èƒ½ä¼šåŠ å¤§
max_length = 10

results = np.zeros((len(samples), max_length, dimensionality))
for i, sample in enumerate(samples):
    for j, word in list(enumerate(sample.split()))[:max_length]:
        index = abs(hash(word)) % dimensionality  # å°†å•è¯æ•£åˆ—åˆ° 0~dimensionality èŒƒå›´å†…çš„ä¸€ä¸ªéšæœºæ•´æ•°ç´¢å¼•
        results[i, j, index] = 1.

print(results.shape)
print(results)
```

    (2, 10, 1000)
    [[[0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]
      ...
      [0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]]
    
     [[0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]
      ...
      [0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]
      [0. 0. 0. ... 0. 0. 0.]]]


### è¯åµŒå…¥

ä»å‰é¢çš„ä¾‹å­ä¸­ä¹Ÿå¯ä»¥çœ‹åˆ° one-hot çš„è¿™ç§ç¡¬ç¼–ç å¾—åˆ°çš„ç»“æœå‘é‡ååˆ†ç¨€ç–ï¼Œå¹¶ä¸”ç»´åº¦æ¯”è¾ƒé«˜ã€‚

è¯åµŒå…¥ï¼ˆword embeddingï¼‰æ˜¯å¦ä¸€ç§å°†å•è¯ä¸å‘é‡ç›¸å…³è”çš„å¸¸ç”¨æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¾—åˆ°æ¯” one-hot æ›´åŠ å¯†é›†ã€ä½ç»´çš„ç¼–ç ã€‚è¯åµŒå…¥çš„ç»“æœæ˜¯è¦ä»æ•°æ®ä¸­å­¦ä¹ å¾—åˆ°çš„ã€‚

![one-hotç¼–ç ä¸è¯åµŒå…¥çš„åŒºåˆ«](https://tva1.sinaimg.cn/large/007S8ZIlgy1gheskpva1tj31fq0u0aeq.jpg)

è¿ç”¨è¯åµŒå…¥æœ‰ä¸¤ç§æ–¹æ³•ï¼š

1. åˆ©ç”¨ Embedding å±‚å­¦ä¹ è¯åµŒå…¥ï¼šåœ¨å®Œæˆç€æ‰‹è¿›è¡Œçš„ä¸»è¦ä»»åŠ¡(æ¯”å¦‚æ–‡æ¡£åˆ†ç±»æˆ–æƒ…æ„Ÿé¢„æµ‹)çš„åŒæ—¶å­¦ä¹ è¯åµŒå…¥ï¼šä¸€å¼€å§‹ä½¿ç”¨éšæœºçš„è¯å‘é‡ï¼Œç„¶åå¯¹è¯å‘é‡ç”¨ä¸å­¦ä¹ ç¥ç»ç½‘ç»œçš„æƒé‡ç›¸åŒçš„æ–¹æ³•è¿›è¡Œå­¦ä¹ ã€‚
2. åˆ©ç”¨é¢„è®­ç»ƒè¯åµŒå…¥(pretrained word embedding)ï¼šåœ¨ä¸åŒäºå¾…è§£å†³é—®é¢˜çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸Šé¢„è®­ç»ƒå¥½è¯åµŒå…¥ï¼Œç„¶åå°†å…¶åŠ è½½åˆ°æ¨¡å‹ä¸­ã€‚


#### åˆ©ç”¨ Embedding å±‚å­¦ä¹ è¯åµŒå…¥

ä¸€ä¸ªç†æƒ³çš„è¯åµŒå…¥ç©ºé—´åº”è¯¥æ˜¯å¯ä»¥æ¯”è¾ƒå®Œç¾åœ°æ˜ å°„äººç±»è¯­è¨€çš„ã€‚å®ƒæ˜¯æœ‰ç¬¦åˆç°å®çš„ç»“æ„çš„ï¼Œç›¸è¿‘çš„è¯åœ¨ç©ºé—´ä¸­å°±åº”è¯¥æ¯”è¾ƒæ¥è¿‘ï¼Œå¹¶ä¸”è¯åµŒå…¥ç©ºé—´ä¸­çš„æ–¹å‘ä¹Ÿæ˜¯è¦æœ‰æ„ä¹‰çš„ã€‚ä¾‹å¦‚ä¸€ä¸ªæ¯”è¾ƒç®€å•çš„ä¾‹å­ï¼š

![è¯åµŒå…¥ç©ºé—´çš„ç®€å•ç¤ºä¾‹](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghetlonkiij30i40lmmyf.jpg)

åœ¨è¿™ä¸ªè¯åµŒå…¥ç©ºé—´ä¸­ï¼Œå® ç‰©éƒ½åœ¨é ä¸‹çš„ä½ç½®ï¼Œé‡ç”ŸåŠ¨ç‰©éƒ½åœ¨é ä¸Šçš„ä½ç½®ï¼Œæ‰€ä»¥ä¸€ä¸ªä»ä¸‹åˆ°ä¸Šæ–¹å‘çš„å‘é‡å°±åº”è¯¥æ˜¯è¡¨ç¤ºä»å® ç‰©åˆ°é‡ç”ŸåŠ¨ç‰©çš„ï¼Œè¿™ä¸ªå‘é‡ä» cat åˆ° tiger æˆ–è€… dog -> wolfã€‚ç±»ä¼¼çš„ï¼Œä¸€ä¸ªä»å·¦åˆ°å³çš„å‘é‡å¯ä»¥è§£é‡Šä¸ºä»çŠ¬ç§‘åˆ°çŒ«ç§‘ï¼Œè¿™ä¸ªå‘é‡å¯ä»¥ä» dog åˆ° catï¼Œæˆ–è€…ä» wolf åˆ° tigerã€‚

å†å¤æ‚ä¸€ç‚¹çš„ï¼Œæ¯”å¦‚è¦è¡¨ç¤ºè¯çš„æ€§åˆ«å…³ç³»ï¼Œå°† king å‘é‡åŠ ä¸Š female å‘é‡ï¼Œåº”è¯¥å¾—åˆ°çš„æ˜¯ queen å‘é‡ï¼Œè¿˜æœ‰å¤æ•°å…³ç³»ï¼šking + plural == kings......

æ‰€ä»¥è¯´ï¼Œè¦æœ‰ä¸€ä¸ªè¿™æ ·å®Œç¾çš„è¯åµŒå…¥ç©ºé—´æ˜¯å¾ˆéš¾çš„ï¼Œç°åœ¨è¿˜æ²¡æœ‰ã€‚ä½†åˆ©ç”¨æ·±åº¦å­¦ä¹ ï¼Œæˆ‘ä»¬è¿˜æ˜¯å¯ä»¥å¾—åˆ°å¯¹äºç‰¹å®šé—®é¢˜æ¥è¯´æ¯”è¾ƒå¥½çš„è¯åµŒå…¥ç©ºé—´çš„ã€‚åœ¨ Keras ä½¿ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦è®©æ¨¡å‹å­¦ä¹ ä¸€ä¸ª Embedding å±‚çš„æƒé‡å°±å¯ä»¥å¾—åˆ°å¯¹å½“å‰ä»»åŠ¡çš„è¯åµŒå…¥ç©ºé—´ï¼š


```python
from tensorflow.keras.layers import Embedding

embedding_layer = Embedding(1000, 64)  # Embedding(å¯èƒ½çš„tokenä¸ªæ•°, åµŒå…¥çš„ç»´åº¦)
```

Embedding å±‚å…¶å®å°±ç›¸å½“äºæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå®ƒå°†ä¸€ä¸ªè¡¨ç¤ºç‰¹å®šå•è¯çš„æ•´æ•°ç´¢å¼•æ˜ å°„åˆ°ä¸€ä¸ªè¯å‘é‡ã€‚

![Embedding å±‚](https://tva1.sinaimg.cn/large/007S8ZIlgy1gheu1p4zqdj316i046aau.jpg)

Embedding å±‚çš„è¾“å…¥æ˜¯å½¢çŠ¶ä¸º `(samples, sequence_length)` çš„äºŒç»´æ•´æ•°å¼ é‡ã€‚è¿™ä¸ªè¾“å…¥å¼ é‡ä¸­çš„ä¸€ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªä»£è¡¨ä¸€ä¸ªæ–‡æœ¬åºåˆ—çš„æ•´æ•°åºåˆ—ï¼Œåº”è¯¥ä¿æŒè¾“å…¥çš„æ‰€æœ‰åºåˆ—é•¿åº¦ç›¸åŒï¼Œè¾ƒçŸ­çš„åºåˆ—åº”è¯¥ç”¨ 0 å¡«å……ï¼Œè¾ƒé•¿çš„åºåˆ—åº”è¯¥è¢«æˆªæ–­ã€‚

Embedding å±‚çš„è¾“å‡ºæ˜¯å½¢çŠ¶ä¸º `(samples, sequence_length, embedding_dimensionality)` çš„ä¸‰ç»´æµ®ç‚¹æ•°å¼ é‡ã€‚è¿™ä¸ªè¾“å‡ºå°±å¯ä»¥ç”¨ RNN æˆ–è€… Conv1D å»å¤„ç†åšå…¶ä»–äº‹æƒ…äº†ã€‚

Embedding å±‚ä¸€å¼€å§‹ä¹Ÿæ˜¯è¢«éšæœºåˆå§‹åŒ–çš„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šåˆ©ç”¨åå‘ä¼ æ’­æ¥é€æ¸è°ƒèŠ‚è¯å‘é‡ã€æ”¹å˜ç©ºé—´ç»“æ„ï¼Œä¸€æ­¥æ­¥æ¥è¿‘æˆ‘ä»¬ä¹‹å‰æåˆ°çš„é‚£ç§ç†æƒ³çš„çŠ¶æ€ã€‚

å®ä¾‹ï¼šç”¨ Embedding å±‚å¤„ç† IMDB ç”µå½±è¯„è®ºæƒ…æ„Ÿé¢„æµ‹ã€‚


```python
# åŠ è½½ IMDB æ•°æ®ï¼Œå‡†å¤‡ç”¨äº Embedding å±‚

from tensorflow.keras.datasets import imdb
from tensorflow.keras import preprocessing

max_features = 10000    # ä½œä¸ºç‰¹å¾çš„å•è¯ä¸ªæ•°
maxlen = 20    # åœ¨ maxlen ä¸ªç‰¹å¾å•è¯åæˆªæ–­æ–‡æœ¬

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

# å°†æ•´æ•°åˆ—è¡¨è½¬æ¢æˆå½¢çŠ¶ä¸º (samples, maxlen) çš„äºŒç»´æ•´æ•°å¼ é‡
x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)
```


```python
# åœ¨ IMDB æ•°æ®ä¸Šä½¿ç”¨ Embedding å±‚å’Œåˆ†ç±»å™¨

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense

model = Sequential()
model.add(Embedding(10000, 8, input_length=maxlen))  # (samples, maxlen, 8)
model.add(Flatten())  # (samles, maxlen*8)
model.add(Dense(1, activation='sigmoid'))  # top classifier

model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])
model.summary()

history = model.fit(x_train, y_train, 
                    epochs=10, 
                    batch_size=32, 
                    validation_split=0.2)

```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding_2 (Embedding)      (None, 20, 8)             80000     
    _________________________________________________________________
    flatten_1 (Flatten)          (None, 160)               0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 1)                 161       
    =================================================================
    Total params: 80,161
    Trainable params: 80,161
    Non-trainable params: 0
    _________________________________________________________________
    Epoch 1/10
    625/625 [==============================] - 1s 1ms/step - loss: 0.6686 - acc: 0.6145 - val_loss: 0.6152 - val_acc: 0.6952
    Epoch 2/10
    625/625 [==============================] - 1s 929us/step - loss: 0.5370 - acc: 0.7525 - val_loss: 0.5214 - val_acc: 0.7318
    Epoch 3/10
    625/625 [==============================] - 1s 878us/step - loss: 0.4573 - acc: 0.7895 - val_loss: 0.4979 - val_acc: 0.7456
    Epoch 4/10
    625/625 [==============================] - 1s 889us/step - loss: 0.4182 - acc: 0.8116 - val_loss: 0.4937 - val_acc: 0.7512
    Epoch 5/10
    625/625 [==============================] - 1s 902us/step - loss: 0.3931 - acc: 0.8224 - val_loss: 0.4935 - val_acc: 0.7568
    Epoch 6/10
    625/625 [==============================] - 1s 931us/step - loss: 0.3721 - acc: 0.8360 - val_loss: 0.4969 - val_acc: 0.7582
    Epoch 7/10
    625/625 [==============================] - 1s 878us/step - loss: 0.3534 - acc: 0.8482 - val_loss: 0.5050 - val_acc: 0.7570
    Epoch 8/10
    625/625 [==============================] - 1s 873us/step - loss: 0.3356 - acc: 0.8579 - val_loss: 0.5103 - val_acc: 0.7556
    Epoch 9/10
    625/625 [==============================] - 1s 999us/step - loss: 0.3183 - acc: 0.8670 - val_loss: 0.5161 - val_acc: 0.7560
    Epoch 10/10
    625/625 [==============================] - 1s 886us/step - loss: 0.3017 - acc: 0.8766 - val_loss: 0.5260 - val_acc: 0.7508


è¿™é‡Œæˆ‘ä»¬åªæŠŠè¯åµŒå…¥åºåˆ—å±•å¼€ä¹‹åç”¨ä¸€ä¸ª Dense å±‚å»å®Œæˆåˆ†ç±»ï¼Œä¼šå¯¼è‡´æ¨¡å‹å¯¹è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªè¯å•ç‹¬å¤„ç†ï¼Œè€Œä¸å»è€ƒè™‘å•è¯ä¹‹é—´çš„å…³ç³»å’Œå¥å­ç»“æ„ã€‚è¿™ä¼šå¯¼è‡´æ¨¡å‹è®¤ä¸ºã€Œthis movie is a bomb(è¿™ç”µå½±è¶…çƒ‚)ã€å’Œã€Œthis movie is the bomb(è¿™ç”µå½±è¶…èµ)ã€éƒ½æ˜¯è´Ÿé¢è¯„ä»·ã€‚è¦å­¦ä¹ å¥å­æ•´ä½“çš„è¯å°±è¦ç”¨åˆ° RNN æˆ–è€… Conv1D äº†ï¼Œè¿™äº›ä¹‹åå†ä»‹ç»ã€‚

#### ä½¿ç”¨é¢„è®­ç»ƒçš„è¯åµŒå…¥

å’Œæˆ‘ä»¬åœ¨åšè®¡ç®—æœºè§†è§‰çš„æ—¶å€™ä½¿ç”¨é¢„è®­ç»ƒç½‘ç»œä¸€æ ·ï¼Œåœ¨æ‰‹å¤´æ•°æ®å°‘çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è¯åµŒå…¥ï¼Œå€Ÿç”¨äººå®¶è®­ç»ƒå‡ºæ¥çš„å¯å¤ç”¨çš„æ¨¡å‹é‡Œçš„é€šç”¨ç‰¹å¾ã€‚

é€šç”¨çš„è¯åµŒå…¥é€šå¸¸æ˜¯åˆ©ç”¨è¯é¢‘ç»Ÿè®¡è®¡ç®—å¾—å‡ºçš„ï¼Œç°åœ¨ä¹Ÿæœ‰å¾ˆå¤šå¯ä¾›æˆ‘ä»¬é€‰ç”¨çš„äº†ï¼Œæ¯”å¦‚ word2vecã€GloVe ç­‰ç­‰ï¼Œå…·ä½“çš„åŸç†éƒ½æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä»¬å…ˆä¼šç”¨å°±è¡Œäº†ã€‚

æˆ‘ä»¬ä¼šåœ¨ä¸‹æ–‡çš„ä¾‹å­ä¸­å°è¯•ä½¿ç”¨ GloVeã€‚

### ä»åŸå§‹æ–‡æœ¬åˆ°è¯åµŒå…¥

æˆ‘ä»¬å°è¯•ä»åŸå§‹çš„ IMDB æ•°æ®ï¼ˆå°±æ˜¯ä¸€å¤§å †æ–‡æœ¬å•¦ï¼‰å¼€å§‹ï¼Œå¤„ç†æ•°æ®ï¼Œåšè¯åµŒå…¥ã€‚

#### ä¸‹è½½ IMDB æ•°æ®çš„åŸå§‹æ–‡æœ¬

åŸå§‹çš„ IMDB æ•°æ®é›†å¯ä»¥ä» [http://mng.bz/0tIo](http://mng.bz/0tIo) ä¸‹è½½ï¼ˆæœ€åæ˜¯è·³è½¬åˆ°ä»s3ä¸‹çš„ http://s3.amazonaws.com/text-datasets/aclImdb.zip ï¼Œä¸ç§‘å­¦ä¸Šç½‘å¾ˆæ…¢å“¦ï¼‰ã€‚

ä¸‹è½½çš„æ•°æ®é›†è§£å‹åæ˜¯è¿™æ ·çš„ï¼š

```
aclImdb
â”œâ”€â”€ test
â”‚Â Â  â”œâ”€â”€ neg
â”‚Â Â  â””â”€â”€ pos
â””â”€â”€ train
    â”œâ”€â”€ neg
    â””â”€â”€ pos
```
åœ¨æ¯ä¸ª neg/pos ç›®å½•ä¸‹é¢å°±æ˜¯ä¸€å¤§å † `.txt` æ–‡ä»¶äº†ï¼Œæ¯ä¸ªé‡Œé¢æ˜¯ä¸€æ¡è¯„è®ºã€‚

ä¸‹é¢ï¼Œæˆ‘ä»¬å°† train è¯„è®ºè½¬æ¢æˆå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¸€ä¸ªå­—ç¬¦ä¸²ä¸€æ¡è¯„è®ºï¼Œå¹¶æŠŠå¯¹åº”çš„æ ‡ç­¾(neg/pos)å†™åˆ° labels åˆ—è¡¨ã€‚


```python
# å¤„ç† IMDB åŸå§‹æ•°æ®çš„æ ‡ç­¾

import os

imdb_dir = '/Volumes/WD/Files/dataset/aclImdb'
train_dir = os.path.join(imdb_dir, 'train')

texts = []
labels = []

for label_type in ['neg', 'pos']:
    dir_name = os.path.join(train_dir, label_type)
    for fname in os.listdir(dir_name):
        if fname.endswith('.txt'):
            with open(os.path.join(dir_name, fname)) as f:
                texts.append(f.read())
            labels.append(0 if label_type == 'neg' else 1)

```


```python
print(labels[0], texts[0], sep=' --> ')
print(labels[-1], texts[-1], sep=' --> ')
print(len(texts), len(labels))
```

    0 --> Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.
    1 --> Enchanted April is a tone poem, an impressionist painting, a masterpiece of conveying a message with few words. It has been one of my 10 favorite films since it came out. I continue to wait, albeit less patiently, for the film to come out in DVD format. Apparently, I am not alone.<br /><br />If parent company Amazon's listings are correct, there are many people who want this title in DVD format. Many people want to go to Italy with this cast and this script. Many people want to keep a permanent copy of this film in their libraries. The cast is spectacular, the cinematography and direction impeccable. The film is a definite keeper. Many have already asked. Please add our names to the list.
    25000 25000


#### å¯¹æ•°æ®è¿›è¡Œåˆ†è¯

ç°åœ¨æ¥åˆ†è¯ï¼Œé¡ºä¾¿åˆ’åˆ†ä¸€ä¸‹è®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚ä¸ºäº†ä½“éªŒé¢„è®­ç»ƒè¯åµŒå…¥ï¼Œæˆ‘ä»¬å†æŠŠè®­ç»ƒé›†æå°ä¸€ç‚¹ï¼Œåªç•™200æ¡æ•°æ®ç”¨æ¥è®­ç»ƒã€‚


```python
# å¯¹ IMDB åŸå§‹æ•°æ®çš„æ–‡æœ¬è¿›è¡Œåˆ†è¯

import numpy as np

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

maxlen = 100  # åªçœ‹æ¯æ¡è¯„è®ºçš„å‰100ä¸ªè¯
training_samples = 200
validation_samples = 10000
max_words = 10000

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens.')

data = pad_sequences(sequences, maxlen=maxlen)

labels = np.asarray(labels)

print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

# æ‰“ä¹±æ•°æ®
indices = np.arange(labels.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]

# åˆ’åˆ†è®­ç»ƒã€éªŒè¯é›†
x_train = data[:training_samples]
y_train = labels[:training_samples]
x_val = data[training_samples: training_samples + validation_samples]
y_val = labels[training_samples: training_samples + validation_samples]
```

    Found 88582 unique tokens.
    Shape of data tensor: (25000, 100)
    Shape of label tensor: (25000,)


#### ä¸‹è½½ GloVe è¯åµŒå…¥

ä¸‹è½½é¢„è®­ç»ƒå¥½çš„ GloVe è¯åµŒå…¥ï¼š [http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip)

å†™ä¸‹æ¥æŠŠå®ƒè§£å‹ï¼Œé‡Œé¢ç”¨çº¯æ–‡æœ¬ä¿å­˜äº†è®­ç»ƒå¥½çš„ 400000 ä¸ª tokens çš„ 100 ç»´è¯åµŒå…¥å‘é‡ã€‚

#### å¯¹åµŒå…¥è¿›è¡Œé¢„å¤„ç†

è§£æè§£å‹åçš„æ–‡ä»¶ï¼š


```python
glove_dir = '/Volumes/WD/Files/glove.6B'

embeddings_index = {}

with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

print(f'Found {len(embeddings_index)} word vectors.')
```

    Found 400000 word vectors.


ç„¶åï¼Œæˆ‘ä»¬è¦æ„å»ºä¸€ä¸ªå¯ä»¥åŠ è½½è¿› Embedding å±‚çš„åµŒå…¥çŸ©é˜µï¼Œå…¶å½¢çŠ¶ä¸º `(max_words, embedding_dim)`ã€‚


```python
embedding_dim = 100

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)  # æœ‰çš„å°±ç”¨ embeddings_index é‡Œçš„è¯å‘é‡
        if embedding_vector is not None:               # æ²¡æœ‰å°±ç”¨å…¨é›¶
            embedding_matrix[i] = embedding_vector
            
print(embedding_matrix)
```

    [[ 0.          0.          0.         ...  0.          0.
       0.        ]
     [-0.038194   -0.24487001  0.72812003 ... -0.1459      0.82779998
       0.27061999]
     [-0.071953    0.23127     0.023731   ... -0.71894997  0.86894
       0.19539   ]
     ...
     [-0.44036001  0.31821999  0.10778    ... -1.29849994  0.11824
       0.64845002]
     [ 0.          0.          0.         ...  0.          0.
       0.        ]
     [-0.54539001 -0.31817999 -0.016281   ... -0.44865     0.067047
       0.17975999]]


#### å®šä¹‰æ¨¡å‹


```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense

model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.summary()
```

    Model: "sequential_2"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding_3 (Embedding)      (None, 100, 100)          1000000   
    _________________________________________________________________
    flatten_2 (Flatten)          (None, 10000)             0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 32)                320032    
    _________________________________________________________________
    dense_3 (Dense)              (None, 1)                 33        
    =================================================================
    Total params: 1,320,065
    Trainable params: 1,320,065
    Non-trainable params: 0
    _________________________________________________________________


#### æŠŠ GloVe è¯åµŒå…¥åŠ è½½è¿›æ¨¡å‹


```python
model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False
```

#### è®­ç»ƒä¸è¯„ä¼°æ¨¡å‹


```python
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy', 
              metrics=['acc'])
history = model.fit(x_train, y_train, 
                    epochs=10, 
                    batch_size=32, 
                    validation_data=(x_val, y_val))
model.save_weights('pre_trained_glove_model.h5')
```

    Epoch 1/10
    7/7 [==============================] - 0s 64ms/step - loss: 1.3595 - acc: 0.5150 - val_loss: 0.6871 - val_acc: 0.5490
    Epoch 2/10
    7/7 [==============================] - 0s 42ms/step - loss: 0.6846 - acc: 0.7950 - val_loss: 0.7569 - val_acc: 0.5217
    Epoch 3/10
    7/7 [==============================] - 0s 42ms/step - loss: 0.3757 - acc: 0.8900 - val_loss: 0.8181 - val_acc: 0.5189
    Epoch 4/10
    7/7 [==============================] - 0s 41ms/step - loss: 0.3464 - acc: 0.8800 - val_loss: 0.8497 - val_acc: 0.4971
    Epoch 5/10
    7/7 [==============================] - 0s 41ms/step - loss: 0.2278 - acc: 0.9600 - val_loss: 0.8661 - val_acc: 0.5308
    Epoch 6/10
    7/7 [==============================] - 0s 42ms/step - loss: 0.1328 - acc: 0.9950 - val_loss: 0.6977 - val_acc: 0.5895
    Epoch 7/10
    7/7 [==============================] - 0s 42ms/step - loss: 0.1859 - acc: 0.9250 - val_loss: 0.6923 - val_acc: 0.5867
    Epoch 8/10
    7/7 [==============================] - 0s 42ms/step - loss: 0.0950 - acc: 0.9950 - val_loss: 0.7609 - val_acc: 0.5609
    Epoch 9/10
    7/7 [==============================] - 0s 50ms/step - loss: 0.0453 - acc: 1.0000 - val_loss: 0.7235 - val_acc: 0.5979
    Epoch 10/10
    7/7 [==============================] - 0s 49ms/step - loss: 0.0398 - acc: 1.0000 - val_loss: 1.1416 - val_acc: 0.5063



```python
# ç»˜åˆ¶ç»“æœ

import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo-', label='Training acc')
plt.plot(epochs, val_acc, 'rs-', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'rs-', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmvtdmhl3j30af07cmxf.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmvtbyf9tj30af07cglv.jpg)


åªç”¨ 200 ä¸ªè®­ç»ƒæ ·æœ¬è¿˜æ˜¯å¤ªéš¾äº†ï¼Œä½†ç”¨é¢„è®­ç»ƒè¯åµŒå…¥è¿˜æ˜¯å¾—åˆ°äº†ä¸é”™çš„æˆæœçš„ã€‚ä½œä¸ºå¯¹æ¯”ï¼Œçœ‹çœ‹å¦‚æœä¸ä½¿ç”¨é¢„è®­ç»ƒï¼Œä¼šæ˜¯ä»€ä¹ˆæ ·çš„ï¼š


```python
# æ„å»ºæ¨¡å‹

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense

model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.summary()

# ä¸ä½¿ç”¨ GloVe è¯åµŒå…¥

# è®­ç»ƒ

model.compile(optimizer='rmsprop', 
              loss='binary_crossentropy', 
              metrics=['acc'])
history = model.fit(x_train, y_train, 
                    epochs=10, 
                    batch_size=32, 
                    validation_data=(x_val, y_val))

# ç»˜åˆ¶ç»“æœ

import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo-', label='Training acc')
plt.plot(epochs, val_acc, 'rs-', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'rs-', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
```

    Model: "sequential_3"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding_4 (Embedding)      (None, 100, 100)          1000000   
    _________________________________________________________________
    flatten_3 (Flatten)          (None, 10000)             0         
    _________________________________________________________________
    dense_4 (Dense)              (None, 32)                320032    
    _________________________________________________________________
    dense_5 (Dense)              (None, 1)                 33        
    =================================================================
    Total params: 1,320,065
    Trainable params: 1,320,065
    Non-trainable params: 0
    _________________________________________________________________
    Epoch 1/10
    7/7 [==============================] - 1s 72ms/step - loss: 0.6972 - acc: 0.4600 - val_loss: 0.6921 - val_acc: 0.5150
    Epoch 2/10
    7/7 [==============================] - 0s 46ms/step - loss: 0.4991 - acc: 1.0000 - val_loss: 0.6901 - val_acc: 0.5347
    Epoch 3/10
    7/7 [==============================] - 0s 46ms/step - loss: 0.2795 - acc: 1.0000 - val_loss: 0.6914 - val_acc: 0.5401
    Epoch 4/10
    7/7 [==============================] - 0s 52ms/step - loss: 0.1171 - acc: 1.0000 - val_loss: 0.6977 - val_acc: 0.5389
    Epoch 5/10
    7/7 [==============================] - 0s 45ms/step - loss: 0.0535 - acc: 1.0000 - val_loss: 0.7115 - val_acc: 0.5343
    Epoch 6/10
    7/7 [==============================] - 0s 44ms/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.7133 - val_acc: 0.5348
    Epoch 7/10
    7/7 [==============================] - 0s 44ms/step - loss: 0.0149 - acc: 1.0000 - val_loss: 0.7146 - val_acc: 0.5382
    Epoch 8/10
    7/7 [==============================] - 0s 44ms/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.7192 - val_acc: 0.5410
    Epoch 9/10
    7/7 [==============================] - 0s 44ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.7266 - val_acc: 0.5398
    Epoch 10/10
    7/7 [==============================] - 0s 53ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.7378 - val_acc: 0.5380



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmvtd56u4j30af07c3yq.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghmvtcoi15j30af07cdg2.jpg)


å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œé¢„è®­ç»ƒè¯åµŒå…¥çš„æ€§èƒ½è¦ä¼˜äºä¸ä»»åŠ¡ä¸€èµ·å­¦ä¹ çš„è¯åµŒå…¥ã€‚ä½†å¦‚æœæœ‰å¤§é‡çš„å¯ç”¨æ•°æ®ï¼Œç”¨ä¸€ä¸ª Embedding å±‚å»ä¸ä»»åŠ¡ä¸€èµ·è®­ç»ƒï¼Œé€šå¸¸æ¯”ä½¿ç”¨é¢„è®­ç»ƒè¯åµŒå…¥æ›´åŠ å¼ºå¤§ã€‚

æœ€åï¼Œå†æ¥çœ‹ä¸€ä¸‹æµ‹è¯•é›†ä¸Šçš„ç»“æœï¼š


```python
# å¯¹æµ‹è¯•é›†æ•°æ®è¿›è¡Œåˆ†è¯

test_dir = os.path.join(imdb_dir, 'test')

texts = []
labels = []

for label_type in ['neg', 'pos']:
    dir_name = os.path.join(test_dir, label_type)
    for fname in sorted(os.listdir(dir_name)):
        if fname.endswith('.txt'):
            with open(os.path.join(dir_name, fname)) as f:
                texts.append(f.read())
            labels.append(0 if label_type == 'neg' else 1)

sequences = tokenizer.texts_to_sequences(texts)
x_test = pad_sequences(sequences, maxlen=maxlen)
y_test = np.asarray(labels)

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹

model.load_weights('pre_trained_glove_model.h5')
model.evaluate(x_test, y_test)
```

    782/782 [==============================] - 1s 983us/step - loss: 1.1397 - acc: 0.5127





    [1.1397335529327393, 0.512719988822937]



emmmï¼Œæœ€åçš„è¿›åº¦æ˜¯ä»¤äººæƒŠè®¶çš„ 50%+ ï¼åªç”¨å¦‚æ­¤å°‘çš„æ•°æ®æ¥è®­ç»ƒè¿˜æ˜¯éš¾å•Šã€‚

