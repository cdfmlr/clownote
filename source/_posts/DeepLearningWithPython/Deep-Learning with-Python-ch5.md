---
categories:
- Machine Learning
- Deep Learning with Python
date: 2020-07-29 21:09:00
tag:
- Machine Learning
- Deep Learning
title: Pythonæ·±åº¦å­¦ä¹ ä¹‹è®¡ç®—æœºè§†è§‰
---

# Deep Learning with Python for computer vision

è¿™ç¯‡æ–‡ç« æ˜¯æˆ‘å­¦ä¹ ã€ŠDeep Learning with Pythonã€‹(ç¬¬äºŒç‰ˆï¼ŒFranÃ§ois Chollet è‘—) æ—¶å†™çš„ç³»åˆ—ç¬”è®°ä¹‹ä¸€ã€‚æ–‡ç« çš„å†…å®¹æ˜¯ä»  Jupyter notebooks è½¬æˆ Markdown çš„ï¼Œä½ å¯ä»¥å» [GitHub](https://github.com/cdfmlr/Deep-Learning-with-Python-Notebooks) æˆ– [Gitee](https://gitee.com/cdfmlr/Deep-Learning-with-Python-Notebooks) æ‰¾åˆ°åŸå§‹çš„ `.ipynb` ç¬”è®°æœ¬ã€‚

ä½ å¯ä»¥å»[è¿™ä¸ªç½‘ç«™åœ¨çº¿é˜…è¯»è¿™æœ¬ä¹¦çš„æ­£ç‰ˆåŸæ–‡](https://livebook.manning.com/book/deep-learning-with-python)(è‹±æ–‡)ã€‚è¿™æœ¬ä¹¦çš„ä½œè€…ä¹Ÿç»™å‡ºäº†é…å¥—çš„ [Jupyter notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks)ã€‚

æœ¬æ–‡ä¸º **ç¬¬5ç«   æ·±åº¦å­¦ä¹ ç”¨äºè®¡ç®—æœºè§†è§‰** (Chapter 5. *Deep learning for computer vision*) çš„ç¬”è®°æ•´åˆã€‚

[TOC]

## å·ç§¯ç¥ç»ç½‘ç»œç®€ä»‹

> 5.1 Introduction to convnets

å·ç§¯ç¥ç»ç½‘ç»œå¤„ç†è®¡ç®—æœºè§†è§‰é—®é¢˜å¾ˆå‰å®³å•¦ã€‚

é¦–å…ˆçœ‹ä¸€ä¸ªæœ€ç®€å•çš„å·ç§¯ç¥ç»ç½‘ç»œå¤„ç† MNIST å®Œçˆ†ç¬¬äºŒç« é‡Œçš„å…¨è¿æ¥ç½‘ç»œçš„ä¾‹å­ï¼š


```python
from tensorflow.keras import layers
from tensorflow.keras import models

model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))

model.add(layers.Flatten())
model.add(layers.Dense(64, activation="relu"))
model.add(layers.Dense(10, activation="softmax"))
```

è¿™é‡Œæˆ‘ä»¬ç”¨çš„ Conv2D å±‚è¦çš„ `input_shape` æ˜¯ `(image_height, image_width, image_channels)` è¿™ç§æ ¼å¼çš„ã€‚

Conv2D å’Œ MaxPooling2D å±‚çš„è¾“å‡ºéƒ½æ˜¯ 3D å¼ é‡ `(height, width, channels)`ï¼Œ height å’Œ width ä¼šé€å±‚å‡å°ï¼Œchannels æ˜¯ç”± Conv2D çš„ç¬¬ä¸€ä¸ªå‚æ•°æ§åˆ¶çš„ã€‚

æœ€åçš„ä¸‰å±‚é‡Œï¼Œæˆ‘ä»¬æ˜¯æŠŠæœ€åä¸€ä¸ª Conv2D å±‚çš„ `(3, 3, 64)` çš„å¼ é‡ç”¨ä¸€ç³»åˆ—å…¨è¿æ¥å±‚å˜æˆæƒ³è¦çš„ç»“æœå‘é‡ï¼šFlatten å±‚æ˜¯ç”¨æ¥æŠŠæˆ‘ä»¬çš„ 3D å¼ é‡å±•å¹³(flattenï¼Œå…¶å®æˆ‘æƒ³å†™æˆâ€œå‹â€ã€â€œé™â€ä¹‹ç±»çš„ï¼Œè¿™æ‰æ˜¯flattençš„æœ¬æ„ï¼Œä½†æ ‡å‡†çš„ä¸­æ–‡ç¿»è¯‘æ˜¯å±•å¹³)åˆ° 1D çš„ã€‚ ç„¶ååé¢çš„ä¸¤ä¸ª Dense å±‚å°±è¡Œæˆ‘ä»¬åœ¨ç¬¬äºŒç« åšçš„é‚£ç§ï¼Œæœ€åå¾—åˆ°ä¸€ä¸ª 10 è·¯çš„åˆ†ç±»ã€‚

æœ€åï¼Œçœ‹ä¸€ä¸‹æ¨¡å‹ç»“æ„ï¼š


```python
model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       
    _________________________________________________________________
    max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         
    _________________________________________________________________
    conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     
    _________________________________________________________________
    max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         
    _________________________________________________________________
    conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     
    _________________________________________________________________
    flatten_1 (Flatten)          (None, 576)               0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 64)                36928     
    _________________________________________________________________
    dense_3 (Dense)              (None, 10)                650       
    =================================================================
    Total params: 93,322
    Trainable params: 93,322
    Non-trainable params: 0
    _________________________________________________________________


å¥½äº†ï¼Œç½‘ç»œå°±å»ºæˆè¿™æ ·äº†ï¼Œè¿˜æ˜¯å¾ˆç®€å•çš„ï¼Œæ¥ä¸‹æ¥å°±è®­ç»ƒå®ƒäº†ï¼Œå¤§è‡´å’Œä¹‹å‰ç¬¬äºŒç« é‡Œçš„æ˜¯ä¸€æ ·çš„ï¼ˆä½†æ³¨æ„reshapeçš„å½¢çŠ¶ä¸ä¸€æ ·ï¼‰ï¼š


```python
# Load the TensorBoard notebook extension
# TensorBoard å¯ä»¥å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
%load_ext tensorboard
# Clear any logs from previous runs
!rm -rf ./logs/ 
```


```python
# åœ¨ MNIST å›¾åƒä¸Šè®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œ

import datetime
import tensorflow as tf

from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((60000, 28, 28, 1))
train_images = train_images.astype('float32') / 255

test_images = test_images.reshape((10000, 28, 28, 1))
test_images = test_images.astype('float32') / 255

train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

# å‡†å¤‡ TensorBoard
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

model.compile(optimizer='rmsprop',
              loss="categorical_crossentropy",
              metrics=['accuracy'])
model.fit(train_images, train_labels, epochs=5, batch_size=64,
          callbacks=[tensorboard_callback])
```

    Train on 60000 samples
    Epoch 1/5
    60000/60000 [==============================] - 36s 599us/sample - loss: 0.0156 - accuracy: 0.9953
    Epoch 2/5
    60000/60000 [==============================] - 33s 554us/sample - loss: 0.0127 - accuracy: 0.9960
    Epoch 3/5
    60000/60000 [==============================] - 31s 524us/sample - loss: 0.0097 - accuracy: 0.9971
    Epoch 4/5
    60000/60000 [==============================] - 32s 529us/sample - loss: 0.0092 - accuracy: 0.9974
    Epoch 5/5
    60000/60000 [==============================] - 31s 523us/sample - loss: 0.0095 - accuracy: 0.9971





    <tensorflow.python.keras.callbacks.History at 0x1441fa9d0>




```python
%tensorboard --logdir logs/fit
```

(è¿™ç†ä¼šè¾“å‡º tensorboard æ˜¾ç¤ºçš„å¯è§†åŒ–æ¨¡å‹è®­ç»ƒæƒ…å†µ)

<iframe id="tensorboard-frame-f4ae12888a3a411e" width="100%" height="800" frameborder="0">
</iframe>
<script>
  (function() {
    const frame = document.getElementById("tensorboard-frame-f4ae12888a3a411e");
    const url = new URL("/", window.location);
    url.port = 6006;
    frame.src = url;
  })();
</script>



æ¥åœ¨æµ‹è¯•é›†çœ‹ä¸€ä¸‹ç»“æœï¼š


```python
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(test_loss, test_acc)
```

    10000/1 - 1s - loss: 0.0172 - accuracy: 0.9926
    0.03441549262946125 0.9926


### å·ç§¯

#### å·ç§¯ç¥ç»ç½‘ç»œ

æˆ‘ä»¬ä¹‹å‰ç”¨çš„*å¯†é›†è¿æ¥å±‚*æ˜¯åœ¨æ•´ä¸ªè¾“å…¥ç‰¹å¾ç©ºé—´ï¼ˆåœ¨ MNIST ä¸­å°±æ˜¯æ‰€æœ‰çš„åƒç´ ï¼‰ä¸­å­¦ä¹ å…¨å±€æ¨¡å¼çš„ï¼›è€Œè¿™é‡Œçš„å·ç§¯å±‚æ˜¯å­¦ä¹ å±€éƒ¨æ¨¡å¼çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒDense æ˜¯å­¦æ•´ä¸ªå›¾åƒçš„ï¼Œè€Œ Conv æ˜¯å­¦å›¾åƒçš„å±€éƒ¨ï¼Œæ¯”å¦‚åœ¨æˆ‘ä»¬åˆšæ‰å†™çš„ä»£ç é‡Œæ˜¯å­¦äº† 3x3 çš„çª—å£ï¼š

![å·ç§¯å±‚å­¦ä¹ å±€éƒ¨æ¨¡å¼](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggo1xjvr02j315g0o6n12.jpg)

è¿™ç§å·ç§¯ç¥ç»ç½‘ç»œå…·æœ‰ä¸¤ä¸ªæ€§è´¨ï¼š

- å·ç§¯ç¥ç»ç½‘ç»œå­¦åˆ°çš„æ¨¡å¼æ˜¯å¹³ç§»ä¸å˜çš„(translation invariant)ï¼šå·ç§¯ç¥ç»ç½‘ç»œå­¦ä¹ åˆ°æŸä¸ªæ¨¡å¼ä¹‹åï¼Œåœ¨å…¶ä»–åœ°æ–¹åˆçœ‹åˆ°äº†è¿™ä¸ªä¸€æ ·çš„æ¨¡å¼ï¼Œå®ƒå°±ä¼šè®¤å‡ºå®ƒå·²ç»å­¦è¿‡è¿™ä¸ªäº†ï¼Œä¸ç”¨å†å»å­¦ä¸€æ¬¡äº†ã€‚è€Œå¯¹äº Dense çš„ç½‘ç»œï¼Œå³ä½¿é‡åˆ°æœ‰ä¸€æ ·çš„å±€éƒ¨éƒ¨åˆ†ä¾ç„¶è¦å»é‡æ–°å­¦ä¹ ä¸€æ¬¡ã€‚è¿™ä¸ªæ€§è´¨è®©å·ç§¯ç¥ç»ç½‘ç»œå¯ä»¥é«˜æ•ˆåˆ©ç”¨æ•°æ®ï¼Œå®ƒåªéœ€è¦æ›´å°‘çš„è®­ç»ƒæ ·æœ¬å°±å¯ä»¥å­¦åˆ°æ³›åŒ–æ¯”è¾ƒå¥½çš„æ•°æ®è¡¨ç¤ºï¼ˆä¸€ä¸ªä¸ªå±€éƒ¨éƒ½è®°ä½äº†å˜›ï¼Œè€Œä¸æ˜¯é æ•´ä½“å»æ˜ å°„ï¼‰ã€‚

- å·ç§¯ç¥ç»ç½‘ç»œå¯ä»¥å­¦åˆ°æ¨¡å¼çš„ç©ºé—´å±‚æ¬¡ç»“æ„(spatial hierarchies of patterns)ï¼šå·ç§¯ç¥ç»ç½‘ç»œåœ¨ç¬¬ä¸€å±‚å­¦å®Œäº†ä¸€ä¸ªä¸€ä¸ªå°çš„å±€éƒ¨æ¨¡å¼ä¹‹åï¼Œä¸‹ä¸€å±‚åˆå¯ä»¥ç”¨è¿™äº›å°å±€éƒ¨æ‹¼å‡ºå¤§ä¸€äº›çš„æ¨¡å¼ã€‚ç„¶åè¿™æ ·å¤šæå‡ å±‚ï¼Œå·ç§¯ç¥ç»ç½‘ç»œå°±å¯ä»¥å­¦åˆ°è¶Šæ¥è¶Šå¤æ‚ã€è¶Šæ¥è¶ŠæŠ½è±¡çš„è§†è§‰æ¦‚å¿µäº†ï¼Œå°±æ˜¯ä¸‹é¢å›¾ç‰‡è¿™ä¸ªæ„æ€ï¼š

![å·ç§¯ç¥ç»ç½‘ç»œå¯ä»¥å­¦åˆ°æ¨¡å¼çš„ç©ºé—´å±‚æ¬¡ç»“æ„](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggo29tuqchj30zh0u0dn6.jpg)

#### å·ç§¯å±‚

æˆ‘ä»¬åˆšæ‰ä¾‹å­ä¸­ç”¨æ¥è¡¨ç¤ºå›¾ç‰‡çš„é‚£ç§ 3D å¼ é‡ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªç©ºé—´è½´ heightã€width å’Œä¸€ä¸ªæ·±åº¦è½´ depthï¼ˆä¹Ÿå« channels è½´ï¼‰ï¼Œå¯¹äº RGB å›¾ç‰‡ï¼Œæ·±åº¦è½´çš„ç»´åº¦å°±æ˜¯3ï¼Œåˆ†åˆ«è¡¨ç¤º3ç§é¢œè‰²å˜›ï¼›è€Œå¯¹äº MNIST è¿™ç§ç°åº¦å›¾ç‰‡ï¼Œæ·±åº¦å°±æ˜¯1ï¼Œåªç”¨ä¸€ä¸ªæ•°å»è¡¨ç¤ºç°åº¦å€¼ã€‚åœ¨è¿™ç§3Då¼ é‡å’Œåœ¨ä¸Šé¢åšçš„å·ç§¯è¿ç®—çš„ç»“æœè¢«ç§°ä½œ *feature map*ï¼ˆç‰¹å¾å›¾ï¼‰ã€‚

å·ç§¯è¿ç®—ä¼šä»è¾“å…¥ç‰¹å¾å›¾ä¸­æå–å‡ºä¸€ä¸ªä¸ªå°åˆ†å—ï¼Œå¹¶å¯¹æ‰€æœ‰è¿™äº›åˆ†å—æ–½åŠ ä¸€ä¸ªç›¸åŒçš„å˜æ¢ï¼Œå¾—åˆ°è¾“å‡ºç‰¹å¾å›¾ã€‚è¾“å‡ºç‰¹å¾å›¾ä»æ˜¯ä¸€ä¸ª 3D å¼ é‡ï¼šå…·æœ‰å®½åº¦å’Œé«˜åº¦ï¼Œå…¶æ·±åº¦å¯èƒ½æ˜¯ä»»æ„å€¼ï¼Œæ·±åº¦çš„å¤§å°æ˜¯è¯¥å±‚çš„ä¸€ä¸ªå‚æ•°ï¼Œæ·±åº¦è½´é‡Œçš„æ¯ä¸ª channel éƒ½ä»£è¡¨ä¸€ä¸ª filter (è¿‡æ»¤å™¨)ã€‚filter ä¼šå¯¹è¾“å…¥æ•°æ®çš„æŸä¸€æ–¹é¢è¿›è¡Œç¼–ç ï¼Œæ¯”å¦‚ï¼ŒæŸä¸ªè¿‡æ»¤å™¨å¯ä»¥ç¼–ç â€œè¾“å…¥ä¸­åŒ…å«ä¸€å¼ è„¸â€è¿™ç§æ¦‚å¿µã€‚

åœ¨åˆšæ‰çš„ MNIST ä¾‹å­ä¸­ï¼Œç¬¬ä¸€ä¸ªå·ç§¯å±‚æ¥å—å°ºå¯¸ä¸º `(28, 28, 1)` çš„è¾“å…¥ç‰¹å¾å›¾ï¼Œè¾“å‡ºä¸€ä¸ªå°ºå¯¸ä¸º `(26, 26, 32)` çš„ç‰¹å¾å›¾ã€‚è¿™ä¸ªè¾“å‡ºä¸­åŒ…å« 32 ä¸ª filterï¼Œåœ¨æ¯ä¸ªæ·±åº¦è½´ä¸­çš„ channel éƒ½åŒ…å«æœ‰ 26x26 çš„å€¼ï¼Œå«åš filter å¯¹è¾“å…¥çš„å“åº”å›¾(response map)ï¼Œè¡¨ç¤º filter åœ¨è¾“å…¥ä¸­ä¸åŒä½ç½®ä¸Šçš„è¿ç®—ç»“æœã€‚è¿™ä¹Ÿå°±æ˜¯ç‰¹å¾å›¾ä¸ºä»€ä¹ˆå«ç‰¹å¾å›¾çš„åŸå› äº†ï¼šæ·±åº¦è½´çš„æ¯ä¸ªç»´åº¦éƒ½æ˜¯ä¸€ä¸ªç‰¹å¾(æˆ–è¿‡æ»¤å™¨)ï¼Œè€Œ 2D å¼ é‡ `output[:, :, n]` æ˜¯è¿™ä¸ªè¿‡æ»¤å™¨åœ¨è¾“å…¥ä¸Šçš„å“åº”çš„äºŒç»´ç©ºé—´å›¾ã€‚

![å“åº”å›¾çš„ç¤ºæ„å›¾](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggo5m7eb85j31jy0jgagx.jpg)

#### å·ç§¯è¿ç®—

å…³äºå·ç§¯ï¼Œï¼Œemmmï¼Œå¤å˜æ²¡æ€ä¹ˆå¬æ‡‚ï¼Œæˆ‘ä¸»è¦æ˜¯çœ‹[ã€ŒçŸ¥ä¹: å¦‚ä½•é€šä¿—æ˜“æ‡‚åœ°è§£é‡Šå·ç§¯?ã€](https://www.zhihu.com/question/22298352)æ¥ç†è§£çš„ã€‚è¿™é‡Œæˆ‘ä»¬ä¸»è¦ç”¨çš„æ˜¯è¿™ç§ä½œç”¨ï¼š

![å·ç§¯](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggo6h1a5wfg30f30f4n2g.gif)

Keras çš„ Conv2D å±‚åˆå§‹åŒ–å†™æˆï¼š

```python
Conv2D(output_depth, (window_height, window_width))
```

å…¶ä¸­åŒ…å«äº†å·ç§¯è¿ç®—æœ‰ä¸¤ä¸ªæ ¸å¿ƒå‚æ•°ï¼š

- è¾“å‡ºç‰¹å¾å›¾çš„æ·±åº¦ï¼šåœ¨æˆ‘ä»¬åˆšæ‰çš„ MNIST ä¾‹å­é‡Œç”¨äº† 32 å’Œ 64ï¼›
- ä»è¾“å…¥ä¸­æå–çš„æ¯ä¸ªå—ï¼ˆæ»‘çª—ï¼‰çš„å°ºå¯¸ï¼šä¸€èˆ¬æ˜¯ 3x3 æˆ–è€… 5x5ï¼›

å·ç§¯è¿ç®—ä¼šåƒæ»‘åŠ¨çª—å£ä¸€æ ·çš„éå†æ‰€æœ‰å¯èƒ½çš„ä½ç½®ï¼ŒæŠŠè¾“å…¥ä¸­æ¯ä¸€å°å—çš„ç‰¹å¾ `(window_height, window_width, input_depth)` é€šè¿‡ä¸ä¸€ä¸ªç§°ä½œå·ç§¯æ ¸(convolution kernel)çš„è¦å­¦ä¹ çš„æƒé‡çŸ©é˜µåšç‚¹ä¹˜ï¼Œå˜åŒ–å¾—åˆ°ä¸€ä¸ªå‘é‡ `(output_depth, )`ã€‚æ‰€æœ‰çš„è¿™ç§ç»“æœå‘é‡æ‹¼åœ¨ä¸€èµ·å°±å¾—åˆ°äº†ä¸€ä¸ª 3D çš„æœ€ç»ˆè¾“å‡º `(height, width, output_depth)`ï¼Œå…¶ä¸­çš„æ¯ä¸ªå€¼å°±æ˜¯è¾“å…¥å¯¹åº”è¿‡æ¥çš„ï¼Œæ¯”å¦‚å– 3x3 çš„æ»‘çª—ï¼Œåˆ™ `output[i, j, :]` æ¥è‡ª `input[i-1:i+1, j-1:j+1, :]`ã€‚

![å·ç§¯çš„å·¥ä½œåŸç†](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggo6cg398tj30t40w8n2u.jpg)

å…³äºå·ç§¯å’Œ CNN å¯ä»¥å»çœ‹çœ‹è¿™ç¯‡æ–‡ç« ï¼š[Convolutional Neural Networks - Basics, An Introduction to CNNs and Deep Learning](https://mlnotebook.github.io/post/CNN1/)

æ³¨æ„ï¼Œå› ä¸ºè¾¹ç•Œæ•ˆåº”(border effects)å’Œä½¿ç”¨äº†æ­¥å¹…(strides)ï¼Œæˆ‘ä»¬è¾“å‡ºçš„å®½åº¦å’Œé«˜åº¦å¯èƒ½ä¸è¾“å…¥çš„å®½åº¦å’Œé«˜åº¦ä¸åŒã€‚

##### è¾¹ç•Œæ•ˆåº”å’Œå¡«å……

è¾¹ç•Œæ•ˆåº”å°±æ˜¯ä½ åœ¨åšæ»‘çª—ä¹‹åå¾—åˆ°çš„çŸ©é˜µå¤§å°ä¼šç¼©å°ä¸€åœˆï¼ˆè¾¹ç•Œæ²¡äº†ï¼‰ã€‚ä¾‹å¦‚ç°è¾“å…¥ä¸€ä¸ª 5x5 çš„å›¾ç‰‡ï¼Œå– 3x3 çš„å°å—åªèƒ½å–å‡º 9 å—æ¥ï¼Œå› æ­¤è¾“å‡ºçš„ç»“æœä¸º 3x3 çš„ï¼š

![è¾¹ç•Œæ•ˆåº”](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggo9oanxxcj31h60rc76w.jpg)

ä¹‹å‰æˆ‘ä»¬åšçš„ MNIST ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œä¸€å¼€å§‹è¾“å…¥ 28x28ï¼Œç¬¬ä¸€å±‚å– 3x3 çš„ï¼Œç»“æœå°±æ˜¯ 26x26 äº†ã€‚

å¦‚æœä¸å¸Œæœ›è¿™ç§å‡å°å‘ç”Ÿï¼Œå³å¸Œæœ›ä¿æŒè¾“å‡ºçš„ç©ºé—´ç»´åº¦ä¸è¾“å…¥çš„ä¸€è‡´ï¼Œåˆ™éœ€è¦åšå¡«å……(padding)ã€‚å¡«å……å°±æ˜¯åœ¨è¾“å…¥çš„å›¾ç‰‡è¾¹ç•Œä¸ŠåŠ ä¸€äº›è¡Œå’Œåˆ—ï¼Œ3x3 åŠ 1åœˆï¼Œ5x5 è¦åŠ 2åœˆï¼š

![å¡«å……](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggo9vytgefj31fa0kote4.jpg)

Keras çš„ Conv2D å±‚é‡Œå¯ä»¥ç”¨ `padding` å‚æ•°æ¥è®¾ç½®ä½¿ç”¨å¡«å……ã€‚`padding` å¯ä»¥è®¾ä¸ºï¼š

- `"valid"`ï¼ˆé»˜è®¤å€¼ï¼‰ï¼šä¸åšå¡«å……ï¼Œåªå–â€œæœ‰æ•ˆâ€çš„å—ã€‚ä¾‹å¦‚åœ¨ 5Ã—5 çš„è¾“å…¥ç‰¹å¾å›¾ä¸­ï¼Œå¯ä»¥æå– 3Ã—3 å›¾å—çš„æœ‰æ•ˆä½ç½®ï¼›
- `"same"`ï¼š åšå¡«å……ï¼Œä½¿è¾“å‡ºå’Œè¾“å…¥çš„ widthã€height ç›¸ç­‰ã€‚

##### å·ç§¯æ­¥å¹…

å·ç§¯çš„æ­¥å¹…å°±æ˜¯ä¸€æ¬¡æ»‘çª—ç§»å¤šå°‘ï¼Œä¹‹å‰æˆ‘ä»¬ä¸€ç›´åšçš„éƒ½æ˜¯æ­¥å¹…ä¸º1çš„ã€‚æˆ‘ä»¬æŠŠæ­¥å¹…å¤§äº 1 çš„å·ç§¯å«åš**æ­¥è¿›å·ç§¯**(strided convolution)ï¼Œæ¯”å¦‚ä¸‹é¢è¿™ä¸ªæ˜¯æ­¥å¹…ä¸º 2 çš„ï¼š

![æ­¥å¹…ä¸º 2 çš„æ­¥è¿›å·ç§¯](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggp4beuceuj31dw0pu76q.jpg)

ç„¶è€Œæ­¥è¿›å·ç§¯åœ¨å®é™…é‡Œé¢ç”¨çš„å¹¶ä¸å¤šğŸ˜‚ï¼Œè¦åšè¿™ç§å¯¹ç‰¹å¾å›¾çš„ä¸‹é‡‡æ ·ï¼ˆdownsampledï¼‰æˆ‘ä»¬ä¸€èˆ¬ç”¨æœ€å¤§æ± åŒ–ã€‚

> æ³¨ï¼š
>
> **ä¸‹é‡‡æ ·**ï¼šå¯¹äºä¸€ä¸ªæ ·å€¼åºåˆ—é—´éš”å‡ ä¸ªæ ·å€¼å–æ ·ä¸€æ¬¡ï¼Œè¿™æ ·å¾—åˆ°æ–°åºåˆ—å°±æ˜¯åŸåºåˆ—çš„ä¸‹é‡‡æ ·ã€‚
>
> From [ç™¾åº¦ç™¾ç§‘](https://baike.baidu.com/item/ä¸‹é‡‡æ ·)
> 


### æœ€å¤§æ± åŒ–

ä¸æ­¥è¿›å·ç§¯ç±»ä¼¼ï¼Œæœ€å¤§æ± åŒ–æ˜¯ç”¨æ¥å¯¹ç‰¹å¾å›¾è¿›è¡Œä¸‹é‡‡æ ·çš„ã€‚åœ¨ä¸€å¼€å§‹çš„ MNIST ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ç”¨äº† MaxPooling2D å±‚ä¹‹åï¼Œç‰¹å¾å›¾çš„å°ºå¯¸å°±å‡åŠäº†ã€‚

æœ€å¤§æ± åŒ–æ˜¯åœ¨ä¸€ä¸ªçª—å£é‡Œï¼Œä»è¾“å…¥ç‰¹å¾å›¾å–å‡ºæ¯ä¸ª channel çš„æœ€å¤§å€¼ï¼Œç„¶åè¾“å‡ºå‡ºæ¥ã€‚è¿™ä¸ªè¿ç®—å’Œå·ç§¯å¾ˆç±»ä¼¼ï¼Œä¸è¿‡æ–½åŠ çš„å‡½æ•°æ˜¯ä¸€ä¸ª maxã€‚

æœ€å¤§æ± åŒ–æˆ‘ä»¬ä¸€èˆ¬éƒ½æ˜¯ç”¨ 2x2 çš„çª—å£ï¼Œæ­¥å¹…ä¸º 2ï¼Œè¿™æ ·å–å¯ä»¥å°†ç‰¹å¾å›¾ä¸‹é‡‡æ ·2å€ã€‚ï¼ˆå·ç§¯æ˜¯ä¸€èˆ¬å–3x3çª—å£å’Œæ­¥å¹…1ï¼‰

å¦‚æœä¸ç”¨æœ€å¤§æ± åŒ–ï¼Œç›´æ¥æŠŠä¸€å¤§å †å·ç§¯å±‚å †èµ·æ¥ï¼Œä¼šæœ‰ä¸¤ä¸ªé—®é¢˜ï¼š

- ç‰¹å¾å›¾å°ºå¯¸ä¸‹é™çš„æ…¢ï¼Œæåˆ°åé¢å‚æ•°å¤ªå¤šäº†ï¼Œä¼šåŠ é‡è¿‡æ‹Ÿåˆï¼›
- ä¸åˆ©äºç©ºé—´å±‚çº§ç»“æ„çš„å­¦ä¹ ï¼šä¸€ç›´ä¸€å°ç‚¹ä¸€å°ç‚¹çš„ç”¨å·ç§¯å±‚å»å­¦ï¼Œä¸åˆ©äºçœ‹åˆ°æ›´æŠ½è±¡çš„æ•´ä½“ã€‚

é™¤äº†æœ€å¤§æ± åŒ–ï¼Œä¸‹é‡‡æ ·çš„æ–¹å¼è¿˜æœ‰å¾ˆå¤šï¼šæ¯”å¦‚æ­¥è¿›å·ç§¯ã€å¹³å‡æ± åŒ–ä¹‹ç±»çš„ã€‚ä½†ä¸€èˆ¬ç”¨æœ€å¤§æ± åŒ–æ•ˆæœæ¯”è¾ƒå¥½ï¼Œæˆ‘ä»¬è¦çš„æ˜¯çŸ¥é“æœ‰æ²¡æœ‰æŸä¸ªç‰¹å¾å˜›ï¼Œå¦‚æœç”¨å¹³å‡å»å°±æŠŠè¿™ä¸ªç‰¹å¾â€œå‡æ·¡â€äº†ï¼Œå¦‚æœç”¨æ­¥è¿›å·ç§¯åˆå¯èƒ½æŠŠè¿™ä¸ªä¿¡æ¯é”™è¿‡äº†ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œä½¿ç”¨æœ€å¤§æ± åŒ–/å…¶ä»–ä¸‹é‡‡æ ·çš„åŸå› ï¼Œä¸€æ˜¯å‡å°‘éœ€è¦å¤„ç†çš„ç‰¹å¾å›¾çš„å…ƒç´ ä¸ªæ•°ï¼ŒäºŒæ˜¯é€šè¿‡è®©ä¸€ç³»åˆ—çš„å·ç§¯å±‚è§‚å¯Ÿåˆ°è¶Šæ¥è¶Šå¤§çš„çª—å£(çœ‹åˆ°çš„è¦†ç›–è¶Šæ¥è¶Šå¤šæ¯”ä¾‹çš„åŸå§‹è¾“å…¥)ï¼Œä»è€Œå­¦åˆ°ç©ºé—´å±‚çº§ç»“æ„ã€‚




## åœ¨å°å‹æ•°æ®é›†ä¸Šä»å¤´è®­ç»ƒä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œ

> 5.2 Training a convnet from scratch on a small dataset

æˆ‘ä»¬æè®¡ç®—æœºè§†è§‰çš„æ—¶å€™ï¼Œç»å¸¸è¦å¤„ç†çš„é—®é¢˜å°±æ˜¯åœ¨å¾ˆå°çš„æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªå›¾åƒåˆ†ç±»çš„æ¨¡å‹ã€‚emmmï¼Œè¿™é‡Œçš„â€œå¾ˆå°â€å¯ä»¥æ˜¯å‡ ç™¾åˆ°å‡ ä¸‡ã€‚

ä»è¿™ä¸€èŠ‚å¼€å§‹åˆ°åé¢å‡ èŠ‚ï¼Œæˆ‘ä»¬è¦æçš„å°±æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªå°å‹æ¨¡å‹ã€ä½¿ç”¨é¢„è®­ç»ƒçš„ç½‘ç»œåšç‰¹å¾æå–ã€å¯¹é¢„è®­ç»ƒçš„ç½‘ç»œè¿›è¡Œå¾®è°ƒï¼Œè¿™äº›æ­¥éª¤åˆèµ·æ¥å°±å¯ä»¥ç”¨äºè§£å†³å°å‹æ•°æ®é›†çš„å›¾åƒåˆ†ç±»é—®é¢˜äº†ã€‚

æˆ‘ä»¬è¿™ä¸€èŠ‚è¦åšçš„æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªå°å‹æ¨¡å‹æ¥åˆ†ç±»çŒ«ç‹—çš„å›¾ç‰‡ã€‚ä¸åšæ­£åˆ™åŒ–ï¼Œå…ˆä¸ç®¡è¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚


### ä¸‹è½½æ•°æ®

æˆ‘ä»¬å°†ä½¿ç”¨ Dogs vs. Cats dataset æ¥è®­ç»ƒæ¨¡å‹ï¼Œè¿™ä¸ªæ•°æ®é›†é‡Œé¢æ˜¯ä¸€å¤§å †çŒ«ã€ç‹—çš„ç…§ç‰‡ã€‚è¿™ä¸ªæ•°æ®é›†å°±ä¸æ˜¯ Keras å†…ç½®çš„äº†ï¼Œæˆ‘ä»¬å¯ä»¥ä» Kaggle ä¸‹è½½ï¼š[https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data)

ä¸‹è½½ä¸‹æ¥è§£å‹ç¼©ï¼Œï¼Œï¼Œï¼ˆemmmmæœ‰ç‚¹å¤§æˆ‘çš„ MBP éƒ½è£…ä¸ä¸‹äº†ï¼Œæ”¾ç§»åŠ¨ç¡¬ç›˜ä¸Šæ‰è§£å‡ºæ¥çš„ğŸ˜‚ï¼Œemmmmï¼Œåˆæƒ³èµ·æ¥è¯¥ä¹°ä¸ªæ–°å›ºæ€äº†ï¼‰ã€‚

ç„¶åæ¥åˆ›å»ºæˆ‘ä»¬è¦ç”¨çš„æ•°æ®é›†ï¼šè®­ç»ƒé›†çŒ«ç‹—å„1000ä¸ªæ ·æœ¬ï¼ŒéªŒè¯é›†å„500ä¸ªï¼Œæµ‹è¯•é›†å„500ä¸ªã€‚ç¼–ç¨‹æ¥å®Œæˆè¿™ä¸ªå·¥ä½œï¼š


```python
# å°†å›¾åƒå¤åˆ¶åˆ°è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•çš„ç›®å½•
import os, shutil

original_dataset_dir = '/Volumes/WD/Files/dataset/dogs-vs-cats/dogs-vs-cats/train'    # åŸå§‹æ•°æ®é›†

base_dir = '/Volumes/WD/Files/dataset/dogs-vs-cats/cats_and_dogs_small'    # å°†è¦ä¿å­˜çš„è¾ƒå°æ•°æ®é›†çš„ä½ç½®
os.mkdir(base_dir)


# åˆ›å‡ ä¸ªç›®å½•æ”¾åˆ’åˆ†åçš„è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†
train_dir = os.path.join(base_dir, 'train')
os.mkdir(train_dir)
validation_dir = os.path.join(base_dir, 'validation')
os.mkdir(validation_dir)
test_dir = os.path.join(base_dir, 'test')
os.mkdir(test_dir)

# åˆ†å¼€æ”¾çŒ«ç‹—
train_cats_dir = os.path.join(train_dir, 'cats')
os.mkdir(train_cats_dir)
train_dogs_dir = os.path.join(train_dir, 'dogs')
os.mkdir(train_dogs_dir)

validation_cats_dir = os.path.join(validation_dir, 'cats')
os.mkdir(validation_cats_dir)
validation_dogs_dir = os.path.join(validation_dir, 'dogs')
os.mkdir(validation_dogs_dir)

test_cats_dir = os.path.join(test_dir, 'cats')
os.mkdir(test_cats_dir)
test_dogs_dir = os.path.join(test_dir, 'dogs')
os.mkdir(test_dogs_dir)

# å¤åˆ¶çŒ«çš„å›¾ç‰‡
fnames = [f'cat.{i}.jpg' for i in range(1000)]    # è¿™é‡Œç”¨äº† f-Stringï¼Œè¦æ±‚ Python >= 3.6ï¼Œè€ç‰ˆæœ¬å¯ä»¥ç”¨ 'cat.{}.jpg'.format(i)
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(train_cats_dir, fname)
    shutil.copyfile(src, dst)
    
fnames = [f'cat.{i}.jpg' for i in range(1000, 1500)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(validation_cats_dir, fname)
    shutil.copyfile(src, dst)
    
fnames = [f'cat.{i}.jpg' for i in range(1500, 2000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(test_cats_dir, fname)
    shutil.copyfile(src, dst)
    
# å¤åˆ¶ç‹—çš„å›¾ç‰‡
fnames = [f'dog.{i}.jpg' for i in range(1000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(train_dogs_dir, fname)
    shutil.copyfile(src, dst)
    
fnames = [f'dog.{i}.jpg' for i in range(1000, 1500)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(validation_dogs_dir, fname)
    shutil.copyfile(src, dst)
    
fnames = [f'dog.{i}.jpg' for i in range(1500, 2000)]
for fname in fnames:
    src = os.path.join(original_dataset_dir, fname)
    dst = os.path.join(test_dogs_dir, fname)
    shutil.copyfile(src, dst)
    
# æ£€æŸ¥
print('total training cat images:', len(os.listdir(train_cats_dir)))
print('total validation cat images:', len(os.listdir(validation_cats_dir)))
print('total test cat images:', len(os.listdir(test_cats_dir)))

print('total training dog images:', len(os.listdir(train_dogs_dir)))
print('total validation dog images:', len(os.listdir(validation_dogs_dir)))
print('total test dog images:', len(os.listdir(test_dogs_dir)))
```

    total training cat images: 1000
    total validation cat images: 500
    total test cat images: 500
    total training dog images: 1000
    total validation dog images: 500
    total test dog images: 500


### æ„å»ºç½‘ç»œ

åœ¨å‡ ä¹æ‰€æœ‰çš„å·ç§¯ç¥ç»ç½‘ç»œé‡Œï¼Œæˆ‘ä»¬éƒ½æ˜¯è®©ç‰¹å¾å›¾çš„æ·±åº¦é€æ¸å¢å¤§ï¼Œè€Œå°ºå¯¸é€æ¸å‡å°ã€‚æ‰€ä»¥è¿™æ¬¡æˆ‘ä»¬ä¹Ÿæ˜¯è¿™æ ·çš„ã€‚

æˆ‘ä»¬ç°åœ¨çš„è¿™ä¸ªé—®é¢˜æ˜¯ä¸ªäºŒåˆ†ç±»ï¼Œæ‰€ä»¥æœ€åä¸€å±‚ç”¨ä¸€ä¸ª1å•å…ƒçš„ sigmoid æ¿€æ´»çš„ Denseï¼š


```python
# å°†çŒ«ç‹—åˆ†ç±»çš„å°å‹å·ç§¯ç¥ç»ç½‘ç»œå®ä¾‹åŒ–

from tensorflow.keras import layers
from tensorflow.keras import models

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
```

çœ‹ä¸€ä¸‹ç½‘ç»œçš„ç»“æ„ï¼š


```python
model.summary()
```

    Model: "sequential_4"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_16 (Conv2D)           (None, 148, 148, 32)      896       
    _________________________________________________________________
    max_pooling2d_16 (MaxPooling (None, 74, 74, 32)        0         
    _________________________________________________________________
    conv2d_17 (Conv2D)           (None, 72, 72, 64)        18496     
    _________________________________________________________________
    max_pooling2d_17 (MaxPooling (None, 36, 36, 64)        0         
    _________________________________________________________________
    conv2d_18 (Conv2D)           (None, 34, 34, 128)       73856     
    _________________________________________________________________
    max_pooling2d_18 (MaxPooling (None, 17, 17, 128)       0         
    _________________________________________________________________
    conv2d_19 (Conv2D)           (None, 15, 15, 128)       147584    
    _________________________________________________________________
    max_pooling2d_19 (MaxPooling (None, 7, 7, 128)         0         
    _________________________________________________________________
    flatten_4 (Flatten)          (None, 6272)              0         
    _________________________________________________________________
    dense_8 (Dense)              (None, 512)               3211776   
    _________________________________________________________________
    dense_9 (Dense)              (None, 1)                 513       
    =================================================================
    Total params: 3,453,121
    Trainable params: 3,453,121
    Non-trainable params: 0
    _________________________________________________________________


ç„¶åå°±è¦ç¼–è¯‘è¿™ä¸ªç½‘ç»œäº†ï¼ŒåšäºŒåˆ†ç±»å˜›ï¼Œæ‰€ä»¥æŸå¤±å‡½æ•°ç”¨ binary crossentropyï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰ï¼Œä¼˜åŒ–å™¨è¿˜æ˜¯ç”¨ RMSprop (æˆ‘ä»¬ä¹‹å‰éƒ½æ˜¯å†™ `optimizer='rmsprop'`ï¼Œè¿™æ¬¡è¦ä¼ ç‚¹å‚æ•°ï¼Œæ‰€ä»¥ç”¨ `optimizers.RMSprop` å®ä¾‹)ï¼š


```python
from tensorflow.keras import optimizers

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics=['acc'])
```

### æ•°æ®é¢„å¤„ç†

æˆ‘ä»¬è¦æŠŠé‚£äº›å›¾ç‰‡ææˆæµ®ç‚¹æ•°å¼ é‡æ‰èƒ½å–‚ç»™ç¥ç»ç½‘ç»œã€‚æ­¥éª¤å¦‚ä¸‹ï¼š

1. è¯»å–å›¾ç‰‡æ–‡ä»¶
2. æŠŠ JPEG æ–‡ä»¶å†…å®¹è§£ç æˆ RGB åƒç´ ç½‘æ ¼
3. è½¬åŒ–æˆæµ®ç‚¹æ•°å¼ é‡
4. æŠŠåƒç´ çš„å€¼ä» `[0, 255]` ç¼©æ”¾åˆ° `[0, 1]`

Keras æä¾›äº†ä¸€äº›å·¥å…·å¯ä»¥è‡ªåŠ¨å®Œæˆè¿™äº›ï¼š


```python
# ä½¿ç”¨ ImageDataGenerator ä»ç›®å½•ä¸­è¯»å–å›¾åƒ

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')    # ç”¨äºŒåˆ†ç±»çš„æ ‡ç­¾

validation_generator = test_datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')
```

    Found 2000 images belonging to 2 classes.
    Found 1000 images belonging to 2 classes.


è¿™ä¸ªæå‡ºæ¥çš„ train_generator å’Œ validation_generator å°±æ˜¯ Python çš„é‚£ç§ Generatorï¼Œæƒ°æ€§è®¡ç®—çš„é‚£ç§ã€‚è¿™ä¸ªç”Ÿæˆå™¨ä¸€æ¬¡ yield å‡ºæ¥ä¸€ä¸ª batchï¼Œæ‰€ä»¥æŠŠå®ƒå«åšâ€œbatch generatorâ€ï¼Œè¿­ä»£å‡ºä¸€ä¸ªæ¥çœ‹ä¸€ä¸‹ï¼š


```python
for data_batch, labels_batch in train_generator:
    print('data batch shape:', data_batch.shape)
    print('labels batch shape:', labels_batch.shape)
    print('labels_batch:', labels_batch)
    break
```

    data batch shape: (20, 150, 150, 3)
    labels batch shape: (20,)
    labels_batch: [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1.]



```python
# åˆ©ç”¨ batch ç”Ÿæˆå™¨æ‹Ÿåˆæ¨¡å‹
history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=30,
    validation_data=validation_generator,
    validation_steps=50)
```

    Epoch 1/30
    100/100 [==============================] - 97s 967ms/step - loss: 0.6901 - acc: 0.5450 - val_loss: 0.6785 - val_acc: 0.5270
    Epoch 2/30
    100/100 [==============================] - 86s 865ms/step - loss: 0.6661 - acc: 0.5875 - val_loss: 0.6525 - val_acc: 0.6060
    ......
    Epoch 29/30
    100/100 [==============================] - 160s 2s/step - loss: 0.0533 - acc: 0.9860 - val_loss: 0.9298 - val_acc: 0.7310
    Epoch 30/30
    100/100 [==============================] - 162s 2s/step - loss: 0.0460 - acc: 0.9885 - val_loss: 1.0609 - val_acc: 0.7150


è¿™é‡Œå› ä¸ºæ˜¯ä» generator è¯»å– batch æ¥ fitï¼Œæ‰€ä»¥æŠŠæˆ‘ä»¬å¹³æ—¶ç”¨çš„ fit æ”¹æˆäº† `fit_generator`ã€‚é‡Œé¢ä¼ è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨ã€ä¸€ä¸ªè½®æ¬¡è¦ä» train_generator é‡Œ yield å‡ºæ¥çš„æ¬¡æ•°(steps_per_epoch)ã€è½®æ¬¡ã€éªŒè¯é›†ç”Ÿæˆå™¨ã€ä¸€ä¸ªè½®æ¬¡è¦ä» validation_generator é‡Œ yield å‡ºæ¥çš„æ¬¡æ•°(validation_steps)ã€‚

```
steps_per_epoch = è®­ç»ƒé›†æ•°æ®æ€»æ•° / æ„å»ºgeneratoræ—¶æŒ‡å®šçš„batch_size
```

validation_steps å’Œ steps_per_epoch ç±»ä¼¼ï¼Œåªæ˜¯æ˜¯å¯¹éªŒè¯é›†çš„ã€‚

ç”¨ä¸‹é¢è¿™è¡Œä»£ç æŠŠè®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜ä¸‹æ¥ï¼š


```python
# ä¿å­˜æ¨¡å‹
model.save('/Volumes/WD/Files/dataset/dogs-vs-cats/cats_and_dogs_small_1.h5')
```

ç„¶åæŠŠè®­ç»ƒè¿‡ç¨‹ç”»å‡ºå›¾æ¥çœ‹ä¸€ä¸‹ï¼š


```python
# ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ›²çº¿å’Œç²¾åº¦æ›²çº¿
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo-', label='Training acc')
plt.plot(epochs, val_acc, 'sr-', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'sr-', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh85zaz7caj30af07ct8s.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh85zd7wcrj30af07cwek.jpg)


ä¸å‡ºæ‰€æ–™ï¼Œè¿‡æ‹Ÿåˆäº†ï¼Œä»å·®ä¸å¤šç¬¬5è½®å°±å¼€å§‹è¿‡äº†ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¦ç”¨ data augmentation (æ•°æ®å¢å¼º) æ¥é™ä½è¿‡æ‹Ÿåˆã€‚

### æ•°æ®å¢å¼º

data augmentation (æ•°æ®å¢å¼º) æ˜¯ç”¨æ·±åº¦å­¦ä¹ å¤„ç†å›¾åƒä¸€èˆ¬éƒ½ä¼šç”¨åˆ°çš„ä¸€ä¸ªæ–¹æ³•ã€‚

è¿‡æ‹Ÿåˆæ˜¯ç”±äºè®­ç»ƒçš„æ•°æ®å¤ªå°‘å¯¼è‡´çš„ï¼ˆåªè¦æ ·æœ¬è¶³å¤Ÿå¤šï¼Œæ¨¡å‹å°±èƒ½çœ‹éå‡ ä¹æ‰€ä»¥å¯èƒ½ï¼Œä»è€Œå‡ ä¹ä¸ä¼šçŠ¯é”™ï¼‰ã€‚æ•°æ®å¢å¼ºæ˜¯ä¸€ç§ä»¥ç°æœ‰çš„æ ·æœ¬ä¸ºåŸºç¡€ç”Ÿæˆæ›´å¤šçš„è®­ç»ƒæ•°æ®çš„ä¸€ç§æ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•åˆ©ç”¨å¤šç§èƒ½å¤Ÿç”Ÿæˆå¯ä¿¡å›¾åƒçš„éšæœºå˜æ¢æ¥å¢åŠ ã€‚

åœ¨ Keras ä¸­ï¼Œæˆ‘ä»¬ç”¨ ImageDataGenerator çš„æ—¶å€™è®¾å‡ ä¸ªå‚æ•°å°±å¯ä»¥å®Œæˆæ•°æ®å¢å¼ºäº†ï¼š


```python
datagen = ImageDataGenerator(
      rotation_range=40,      # éšæœºæ—‹è½¬å›¾ç‰‡çš„èŒƒå›´ï¼Œ0~180
      width_shift_range=0.2,  # éšæœºæ°´å¹³ç§»åŠ¨çš„æ¯”ä¾‹
      height_shift_range=0.2, # éšæœºç«–ç›´ç§»åŠ¨çš„æ¯”ä¾‹
      shear_range=0.2,        # éšæœºé”™åˆ‡å˜æ¢(shearing transformations)çš„è§’åº¦
      zoom_range=0.2,         # éšæœºç¼©æ”¾çš„èŒƒå›´
      horizontal_flip=True,   # æ˜¯å¦åšéšæœºæ°´å¹³åè½¬
      fill_mode='nearest')    # å¡«å……æ–°åˆ›å»ºåƒç´ çš„æ–¹æ³•
```

æ‰¾å¼ å›¾ç‰‡å¢å¼ºäº†è¯•è¯•ï¼š


```python
from tensorflow.keras.preprocessing import image

fnames = [os.path.join(train_cats_dir, fname) for 
          fname in os.listdir(train_cats_dir)]

img_path = fnames[3]
img = image.load_img(img_path, target_size=(150, 150))    # è¯»å–å›¾ç‰‡

x = image.img_to_array(img)    # shape (150, 150, 3)
x = x.reshape((1,) + x.shape)  # shape (1, 150, 150, 3)

i=0
for batch in datagen.flow(x, batch_size=1):
    plt.figure(i)
    imgplot = plt.imshow(image.array_to_img(batch[0]))
    i += 1
    if i % 4 == 0:
        break

plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh85zfrydzj307907075h.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh85zk2m49j3079070myh.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh85zhn5dcj3079070aba.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh85zrkbkej30790703zv.jpg)


æ³¨æ„æ•°æ®å¢å¼ºå¹¶æ²¡æœ‰å¸¦æ¥æ–°çš„ä¿¡æ¯ï¼Œåªæ˜¯æŠŠåŸæœ¬å°±æœ‰çš„ä¿¡æ¯ remix ä¸€ä¸‹ã€‚æ‰€ä»¥åœ¨æ•°æ®ç‰¹åˆ«å°‘çš„æƒ…å†µä¸‹ï¼Œå…‰ç”¨æ•°æ®å¢å¼ºä¸è¶³ä»¥æ¶ˆé™¤è¿‡æ‹Ÿåˆï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜éœ€è¦åœ¨ Dense å±‚ä¹‹å‰ç”¨ä¸Š Dropoutã€‚


```python
# å®šä¹‰ä¸€ä¸ªåŒ…å« dropout çš„æ–°å·ç§¯ç¥ç»ç½‘ç»œ

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Flatten())

model.add(layers.Dropout(0.5))    # ğŸ‘ˆ æ–°å¢çš„ Dropout

model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', 
              optimizer=optimizers.RMSprop(lr=1e-4), 
              metrics=['acc'])
```


```python
# åˆ©ç”¨æ•°æ®å¢å¼ºç”Ÿæˆå™¨æ¥è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œ

# æ•°æ®ç”Ÿæˆå™¨
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,)

test_datagen = ImageDataGenerator(rescale=1./255)    # æµ‹è¯•é›†ä¸å¢å¼ºå“¦

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary')

# è®­ç»ƒ
history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=100,
    validation_data=validation_generator,
    validation_steps=50)

# ä¿å­˜æ¨¡å‹
model.save('/Volumes/WD/Files/dataset/dogs-vs-cats/cats_and_dogs_small_2.h5')
```

    Found 2000 images belonging to 2 classes.
    Found 1000 images belonging to 2 classes.
    Epoch 1/100
    100/100 [==============================] - 142s 1s/step - loss: 0.6909 - acc: 0.5265 - val_loss: 0.6799 - val_acc: 0.5127
    Epoch 2/100
    100/100 [==============================] - 123s 1s/step - loss: 0.6817 - acc: 0.5474 - val_loss: 0.6561 - val_acc: 0.6320
    ......
    100/100 [==============================] - 131s 1s/step - loss: 0.3225 - acc: 0.8612 - val_loss: 0.4465 - val_acc: 0.7976
    Epoch 99/100
    100/100 [==============================] - 134s 1s/step - loss: 0.3391 - acc: 0.8501 - val_loss: 0.5455 - val_acc: 0.7836
    Epoch 100/100
    100/100 [==============================] - 130s 1s/step - loss: 0.3140 - acc: 0.8624 - val_loss: 0.4295 - val_acc: 0.8274



```python
# ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±æ›²çº¿å’Œç²¾åº¦æ›²çº¿
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo-', label='Training acc')
plt.plot(epochs, val_acc, 'r-', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'r-', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh85zupxk2j30al07cjro.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh85zwpa4hj30al07c0t2.jpg)


ğŸ‘Œç”¨äº† Data Augmentation å’Œ Dropout ä¹‹åï¼Œè¿‡æ‹Ÿåˆå°±å¥½å¤šäº†ï¼Œç²¾åº¦ä¹Ÿæœ‰æ‰€æå‡ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿˜ä¼šç”¨ä¸€äº›æŠ€æœ¯æ¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ã€‚

## ä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œ

> 5.3 Using a pretrained convnet

å¯¹äºå°å‹å›¾åƒæ•°æ®é›†ï¼Œæˆ‘ä»¬å¸¸ç”¨çš„ä¸€ç§é«˜æ•ˆçš„æ–¹æ³•æ˜¯åˆ©ç”¨é¢„è®­ç»ƒç½‘ç»œï¼ˆpretrained networkï¼‰æ¥æ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚é¢„è®­ç»ƒç½‘ç»œæ˜¯ä¹‹å‰åœ¨å¤§å‹æ•°æ®é›†ä¸Šè®­ç»ƒå¥½çš„ç½‘ç»œï¼ˆé€šå¸¸æ˜¯åœ¨å¤§å‹å›¾ç‰‡åˆ†ç±»ä»»åŠ¡ä¸Šè®­ç»ƒå¥½çš„ï¼‰ã€‚å¦‚æœé¢„è®­ç»ƒé›†ç”¨çš„æ•°æ®è¶³å¤Ÿå¤šï¼Œæ¨¡å‹è¶³å¤Ÿæ³›åŒ–ï¼Œé‚£ä¹ˆé¢„è®­ç»ƒç½‘ç»œå­¦åˆ°çš„ç©ºé—´å±‚æ¬¡ç»“æ„å°±å¯ä»¥æœ‰æ•ˆåœ°ä½œä¸ºé€šç”¨æ¨¡å‹æ¥ååº”ç°å®çš„è§†è§‰ä¸–ç•Œï¼Œå› æ­¤å°±å¯ç”¨äºå„ç§ä¸åŒçš„è®¡ç®—æœºè§†è§‰é—®é¢˜ï¼Œå“ªæ€•æ–°çš„é—®é¢˜å’ŒåŸå§‹ä»»åŠ¡å®Œå…¨æ‰¯ä¸ä¸Šå…³ç³»ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªåœ¨ ImageNet (è¿™ä¸ªæ•°æ®é›†æœ‰140ä¸‡å¼ å›¾åƒï¼Œ1000ä¸ªä¸åŒçš„ç±»åˆ«ï¼Œä¸»è¦æ˜¯åŠ¨ç‰©å’Œå„ç§æ—¥å¸¸ç”¨å“)å®Œæˆé¢„è®­ç»ƒçš„ç½‘ç»œæ¥å¤„ç†çŒ«ç‹—åˆ†ç±»çš„é—®é¢˜ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„æ˜¯ VGG16 è¿™ä¸ªæ¶æ„ã€‚

ä½¿ç”¨é¢„è®­ç»ƒç½‘ç»œæœ‰ä¸¤ç§æ–¹æ³•ï¼š*ç‰¹å¾æå–*(feature extraction)å’Œ*å¾®è°ƒæ¨¡å‹*(fine-tuning)ã€‚

### ç‰¹å¾æå–

ç‰¹å¾æå–æ˜¯ä½¿ç”¨ä¹‹å‰çš„ç½‘ç»œå­¦åˆ°çš„è¡¨ç¤ºæ¥ä»æ–°æ ·æœ¬ä¸­æå–å‡ºéœ€è¦çš„ç‰¹å¾ï¼Œè¾“å…¥ä¸€ä¸ªæ–°çš„åˆ†ç±»å™¨ï¼Œä»å¤´å¼€å§‹è®­ç»ƒã€‚

åœ¨å‰é¢çš„å·ç§¯ç¥ç»ç½‘ç»œä¾‹å­ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“ï¼Œæˆ‘ä»¬ç”¨æ¥å›¾ç‰‡åˆ†ç±»çš„æ¨¡å‹å¯ä»¥åˆ†æˆä¸¤éƒ¨åˆ†ï¼š

- å·ç§¯åŸºï¼ˆconvolutional baseï¼‰ï¼šå‰é¢çš„å·ç§¯ã€æ± åŒ–å±‚ï¼›
- åˆ†ç±»å™¨ï¼ˆclassifierï¼‰ï¼šåé¢çš„å¯†è¿æ¥å±‚ï¼›

æ‰€ä»¥å¯¹äºå·ç§¯ç¥ç»ç½‘ç»œï¼Œ**ç‰¹å¾æå–**å°±æ˜¯å–å‡ºä¹‹å‰è®­ç»ƒå¥½çš„ç½‘ç»œçš„å·ç§¯åŸºï¼ŒæŠŠæ–°çš„æ•°æ®è¾“å…¥è¿›å»è·‘ï¼Œç„¶åæ‹¿å…¶è¾“å‡ºå»è®­ç»ƒä¸€ä¸ªæ–°çš„åˆ†ç±»å™¨ï¼š

![ä¿æŒå·ç§¯åŸºä¸å˜ï¼Œæ”¹å˜åˆ†ç±»å™¨](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggvimux4ttj314l0u0jv4.jpg)

æ³¨æ„ï¼Œåªå¤ç”¨å·ç§¯åŸºï¼Œè€Œä¸å¤ç”¨åˆ†ç±»å™¨ã€‚å·ç§¯åŸºæ˜¯ç”¨æ¥æå–ç‰¹å¾çš„ï¼Œè¿™ä¸ªå¯ä»¥ç›¸åŒï¼›ä½†ç”±äºæ¯ä¸ªé—®é¢˜çš„åˆ†ç±»ä¸åŒï¼Œç†åº”ä½¿ç”¨ä¸åŒçš„åˆ†ç±»å™¨ï¼›å¹¶ä¸”æœ‰çš„é—®é¢˜ä¸­ç‰¹å¾çš„ä½ç½®æ˜¯æœ‰ç”¨çš„ï¼Œè€Œæˆ‘ä»¬æŠŠç‰¹å¾å›¾è½¬åˆ° Dense å±‚çš„æ—¶å€™è¿™äº›ä½ç½®ç‰¹å¾å°±ä¸¢å¤±äº†ï¼Œæ‰€ä»¥å¹¶ä¸æ˜¯æ‰€æœ‰é—®é¢˜éƒ½æ˜¯ç”¨ Dense å»ç®€å•çš„å®Œæˆåˆ†ç±»çš„ï¼Œæ‰€ä»¥ä¸åº”è¯¥æ— è„‘å¥—ç”¨åˆ†ç±»å™¨ã€‚

åœ¨é¢„è®­ç»ƒç½‘ç»œä¸­ï¼Œèƒ½æå–çš„ç‰¹å¾è¡¨ç¤ºçš„é€šç”¨ç¨‹åº¦å–å†³äºäºå…¶æ·±åº¦ã€‚è¶Šå°‘çš„å±‚è¶Šé€šç”¨ï¼ˆæ¯”å¦‚ï¼Œå›¾ç‰‡çš„é¢œè‰²ã€è¾¹ç¼˜ã€çº¹ç†ä¹‹ç±»çš„ï¼‰ï¼Œè¶Šå¤šçš„å±‚å°±ä¼šåŒ…å«è¶Šå¤šçš„æŠ½è±¡ä¿¡æ¯ï¼ˆæ¯”å¦‚ï¼Œæœ‰ä¸€ä¸ªçŒ«çš„çœ¼ç›å•Šä¹‹ç±»çš„ï¼‰ã€‚æ‰€ä»¥å¦‚æœé¢„è®­ç»ƒç½‘ç»œå¤„ç†çš„åŸå§‹é—®é¢˜å’Œæˆ‘ä»¬å½“å‰è¦å¤„ç†çš„é—®é¢˜å·®è·å¤ªå¤§ï¼Œå°±åªå› è¯¥ä½¿ç”¨æ¯”è¾ƒå°‘çš„å‰å‡ å±‚ï¼Œè€Œä¸è¦ç”¨æ•´ä¸ªå·ç§¯åŸºã€‚

ç°åœ¨ï¼Œæ¥åšå®è·µäº†ï¼Œæˆ‘ä»¬è¦ç”¨åœ¨ ImageNet ä¸Šé¢„è®­ç»ƒçš„ VGG16 æ¨¡å‹æ¥å¤„ç†çŒ«ç‹—åˆ†ç±»çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¿æŒå·ç§¯åŸºä¸å˜ï¼Œæ”¹å˜åˆ†ç±»å™¨ã€‚

VGG16 æ¨¡å‹æ˜¯ Keras æœ‰å†…ç½®çš„ï¼Œç›´æ¥ç”¨å°±å¥½äº†ï¼š


```python
from tensorflow.keras.applications import VGG16

conv_base = VGG16(weights='imagenet',        # æŒ‡å®šæ¨¡å‹åˆå§‹åŒ–çš„æƒé‡æ£€æŸ¥ç‚¹
                  include_top=False,         # æ˜¯å¦åŒ…å«æœ€åçš„å¯†é›†è¿æ¥å±‚åˆ†ç±»å™¨
                  input_shape=(150, 150, 3)) # è¾“å…¥çš„å½¢çŠ¶ï¼Œä¸ä¼ çš„è¯èƒ½å¤„ç†ä»»æ„å½¢çŠ¶çš„è¾“å…¥
```

æ¨¡å‹æ˜¯è¦ä»è¿™é‡Œä¸‹è½½çš„ï¼š[https://github.com/fchollet/deep-learning-models/releases](https://github.com/fchollet/deep-learning-models/releases)

å¦‚æœè®©ä»–è‡ªå·±ä¸‹æ¯”è¾ƒæ…¢çš„è¯ï¼Œå¯ä»¥è€ƒè™‘æ‰‹åŠ¨ä¸‹è½½å®‰è£…ã€‚å‚è€ƒ vgg16 çš„æºç : `/usr/local/lib/python3.7/site-packages/keras_applications/vgg16.py`ï¼Œå‘ç°å®ƒæ˜¯è°ƒç”¨ä¸€ä¸ª get_file æ¥è·å–æ¨¡å‹çš„ï¼š

```python
weights_path = keras_utils.get_file(
    'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',
    WEIGHTS_PATH_NO_TOP,
    cache_subdir='models',
    file_hash='6d6bbae143d832006294945121d1f1fc')
```

è¿™é‡Œçœ‹åˆ°ä»–æ˜¯ä»ä¸€ä¸ªå­ç›®å½• `models` é‡Œè¯»å–æ¨¡å‹çš„ï¼Œè¿˜æœ‰è¿™ä¸ª `keras_utils.get_file` å°±æ˜¯ `/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/data_utils.py` ç¬¬ 150 è¡Œå·¦å³çš„ get_file å‡½æ•°ï¼Œæ–‡æ¡£æ³¨é‡Šé‡Œå†™äº†ï¼Œä¸œè¥¿é»˜è®¤æ˜¯æ”¾åˆ° `~/.keras` è¿™ä¸ªç›®å½•çš„ã€‚

æ€»ä¹‹ï¼Œå°±æ˜¯æŠŠæ¨¡å‹ä¸‹è½½ä¸‹æ¥æ”¾åˆ° `~/.keras/models` é‡Œå°±å¥½äº†ã€‚

è¿˜æœ‰æ³¨æ„åŠ ä¸åŠ top(æœ€åçš„åˆ†ç±»å™¨)ï¼Œä¸‹è½½çš„æ¨¡å‹å¤§å°å·®è·è¿˜æ˜¯å¾ˆå¤§çš„ï¼š

![ä¸åŒé…ç½®çš„VGG16é¢„è®­ç»ƒæ¨¡å‹çš„å¤§å°](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggw35kvawgj30t407uta8.jpg)

æ€»ä¹‹ï¼Œæœ€åå®Œäº†ä¼šå¾—åˆ°ä¸€ä¸ª conv_base æ¨¡å‹ï¼Œè¿™ä¸ªæ¨¡å‹è¿˜æ˜¯å¾ˆå®¹æ˜“ç†è§£çš„ï¼Œç”¨åˆ°çš„éƒ½æ˜¯æˆ‘ä»¬ä¹‹å‰å·²ç»å­¦è¿‡çš„ä¸œè¥¿ï¼š


```python
conv_base.summary()
```

    Model: "vgg16"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         [(None, 150, 150, 3)]     0         
    _________________________________________________________________
    block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
    _________________________________________________________________
    block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
    _________________________________________________________________
    block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
    _________________________________________________________________
    block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
    _________________________________________________________________
    block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
    _________________________________________________________________
    block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
    _________________________________________________________________
    block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
    _________________________________________________________________
    block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
    _________________________________________________________________
    block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
    _________________________________________________________________
    block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
    _________________________________________________________________
    block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
    _________________________________________________________________
    block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
    _________________________________________________________________
    block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
    _________________________________________________________________
    block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
    _________________________________________________________________
    block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
    _________________________________________________________________
    block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
    _________________________________________________________________
    block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
    _________________________________________________________________
    block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
    =================================================================
    Total params: 14,714,688
    Trainable params: 14,714,688
    Non-trainable params: 0
    _________________________________________________________________


æ³¨æ„ï¼Œæœ€å‰é¢çš„è¾“å…¥å·²ç»è°ƒæˆäº†æˆ‘ä»¬å¸Œæœ›çš„`(150, 150, 3)`ï¼Œæœ€åçš„è¾“å‡ºæ˜¯`(4, 4, 512)`ï¼Œåœ¨åé¢æˆ‘ä»¬è¦è¿ä¸Šæˆ‘ä»¬è‡ªå·±çš„åˆ†ç±»å™¨ã€‚æœ‰ä¸¤ç§åŠæ³•ï¼š

1. åœ¨è¿™ä¸ªå·ç§¯åŸºä¸Šè·‘æˆ‘ä»¬ç°åœ¨çš„æ•°æ®é›†ï¼Œç„¶åæŠŠç»“æœæ”¾åˆ°ä¸ª Numpy æ•°ç»„é‡Œé¢ï¼Œä¿å­˜åˆ°ç£ç›˜ï¼Œç„¶åä»¥è¿™ä¸ªæ•°ç»„ä¸ºè¾“å…¥ï¼Œæ‰”åˆ°ä¸ªå¯†è¿æ¥çš„ç½‘ç»œé‡Œé¢å»è®­ç»ƒã€‚è¿™ç§åŠæ³•æ¯”è¾ƒç®€å•ï¼Œä¹Ÿåªéœ€è¦è®¡ç®—ä¸€æ¬¡æ¶ˆè€—æœ€å¤§çš„å·ç§¯åŸºéƒ¨åˆ†ã€‚ä½†æ˜¯ï¼Œè¿™ç§åŠæ³•ä¸èƒ½ä½¿ç”¨æ•°æ®å¢å¼ºã€‚
2. æ‹“å±• conv_baseï¼Œå¾€å®ƒåé¢åŠ  Dense å±‚ï¼Œç„¶ååœ¨è¾“å…¥æ•°æ®ä¸Šç«¯åˆ°ç«¯è¿è¡Œæ•´ä¸ªç½‘ç»œï¼Œè¿™æ ·å¯ä»¥ä½¿ç”¨æ•°æ®å¢å¼ºï¼Œä½†è®¡ç®—ä»£ä»·æ¯”è¾ƒå¤§ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬åšç¬¬ä¸€ç§ã€‚

#### ä¸æ•°æ®å¢å¼ºçš„å¿«é€Ÿç‰¹å¾æå–

é‡è¿°ä¸€èˆ¬ï¼Œè¿™ç§æ–¹æ³•ä¿å­˜æˆ‘ä»¬çš„æ•°æ®é€šè¿‡ conv_base åçš„è¾“å‡ºï¼Œç„¶åå°†è¿™äº›è¾“å‡ºä½œä¸ºè¾“å…¥æ”¾åˆ°ä¸€ä¸ªæ–°æ¨¡å‹é‡Œã€‚

è¿™é‡Œè¿˜æ˜¯ç”¨ ImageDataGenerator æŠŠå›¾ç‰‡ã€æ ‡ç­¾æå–åˆ° Numpy æ•°ç»„ã€‚ç„¶åè°ƒç”¨ conv_base çš„ predict æ–¹æ³•æ¥ç”¨é¢„è®­ç»ƒæ¨¡å‹æå–ç‰¹å¾ã€‚


```python
# ä½¿ç”¨é¢„è®­ç»ƒçš„å·ç§¯åŸºæå–ç‰¹å¾

import os
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator

base_dir = '/Volumes/WD/Files/dataset/dogs-vs-cats/cats_and_dogs_small'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'validation')
test_dir = os.path.join(base_dir, 'test')

datagen = ImageDataGenerator(rescale=1./255)
batch_size = 20

def extract_features(directory, sample_count):
    features = np.zeros(shape=(sample_count, 4, 4, 512))    # è¿™ä¸ªè¦ç¬¦åˆä¹‹å‰ conv_base.summary() æœ€åä¸€å±‚çš„è¾“å‡ºçš„å½¢çŠ¶
    labels = np.zeros(shape=(sample_count))
    generator = datagen.flow_from_directory(
        directory,
        target_size=(150, 150),
        batch_size=batch_size,
        class_mode='binary')
    i = 0
    for inputs_batch, labels_batch in generator:    # ä¸€æ‰¹ä¸€æ‰¹åŠ æ•°æ®
        features_batch = conv_base.predict(inputs_batch)
        features[i * batch_size : (i + 1) * batch_size] = features_batch
        labels[i * batch_size : (i + 1) * batch_size] = labels_batch
        i += 1
        if i * batch_size >= sample_count:    # generator ä¼šç”Ÿæˆæ— é™çš„æ•°æ®å“¦ï¼Œbreak è¦è‡ªå·±æ§åˆ¶
            break
    return features, labels

train_features, train_labels = extract_features(train_dir, 2000)
validation_features, validation_labels = extract_features(validation_dir, 1000)
test_features, test_labels = extract_features(test_dir, 1000)
```

    Found 2000 images belonging to 2 classes.
    Found 1000 images belonging to 2 classes.
    Found 1000 images belonging to 2 classes.


ä¹‹åæˆ‘ä»¬æ˜¯è¦æ¥å¯†è¿æ¥å±‚çš„åˆ†ç±»å™¨æ¥ç€ï¼Œæ‰€ä»¥è¿™é‡Œå…ˆæŠŠå¼ é‡å‹æ‰äº†ï¼š


```python
train_features = np.reshape(train_features, (2000, 4 * 4 * 512))
validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))
test_features = np.reshape(test_features, (1000, 4 * 4 * 512))
```

ç„¶åå°±æ˜¯åšå¯†è¿æ¥çš„åˆ†ç±»å™¨äº†ï¼Œè¿˜æ˜¯ç”¨ dropout æ­£åˆ™åŒ–ï¼š


```python
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers

model = models.Sequential()
model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer=optimizers.RMSprop(lr=2e-5),
              loss='binary_crossentropy',
              metrics=['acc'])

history = model.fit(train_features, train_labels,
                    epochs=30,
                    batch_size=20,
                    validation_data=(validation_features, validation_labels))
```

    Train on 2000 samples, validate on 1000 samples
    Epoch 1/30
    2000/2000 [==============================] - 3s 1ms/sample - loss: 0.5905 - acc: 0.6840 - val_loss: 0.4347 - val_acc: 0.8430
    Epoch 2/30
    2000/2000 [==============================] - 1s 640us/sample - loss: 0.4339 - acc: 0.8060 - val_loss: 0.3566 - val_acc: 0.8620
    ......
    Epoch 29/30
    2000/2000 [==============================] - 1s 686us/sample - loss: 0.0928 - acc: 0.9755 - val_loss: 0.2390 - val_acc: 0.9010
    Epoch 30/30
    2000/2000 [==============================] - 1s 687us/sample - loss: 0.0889 - acc: 0.9715 - val_loss: 0.2401 - val_acc: 0.9010


çœ‹ç»“æœäº†ï¼š


```python
import matplotlib.pyplot as plt

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo-', label='Training acc')
plt.plot(epochs, val_acc, 'rs-', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'rs-', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh861ydynej30al07cwek.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh862078xaj30af07cglo.jpg)


å¯ä»¥çœ‹åˆ°ï¼Œæ•ˆæœè¿˜æ˜¯å¾ˆä¸é”™çš„æ¥è¿‘ 90% çš„å‡†ç¡®ç‡ã€‚ä½†é—®é¢˜ä¹Ÿè¿˜æ˜¯æœ‰ï¼Œå› ä¸ºæ²¡æœ‰æ•°æ®å¢å¼ºï¼Œè¿‡æ‹Ÿåˆè¿˜æ˜¯ä¸¥é‡çš„ï¼ŒåŸºæœ¬ä»ä¸€å¼€å§‹å°±åœ¨è¿‡æ‹Ÿåˆäº†ã€‚å¯¹è¿™ç§å¤ªå°çš„å›¾ç‰‡æ•°æ®é›†ï¼Œæ²¡æœ‰æ•°æ®å¢å¼ºï¼Œä¸€èˆ¬éƒ½æ˜¯ä¸å¤ªè¡Œçš„ã€‚

#### å¸¦æ•°æ®å¢å¼ºçš„ç‰¹å¾æå–

ç¬¬äºŒç§åŠæ³•ï¼Œæ‹“å±• conv_baseï¼Œå¾€å®ƒåé¢åŠ  Dense å±‚ï¼Œç„¶ååœ¨è¾“å…¥æ•°æ®ä¸Šç«¯åˆ°ç«¯è¿è¡Œæ•´ä¸ªç½‘ç»œã€‚

è¿™ä¸ªåŠæ³•è®¡ç®—ä»£ä»·éï¼å¸¸ï¼é«˜ï¼ï¼ŒåŸºæœ¬åªèƒ½ç”¨ GPU è·‘ã€‚æ²¡ GPU å°±è¢«ç”¨è¿™ä¸ªäº†ã€‚

åœ¨ Keras é‡Œï¼Œæˆ‘ä»¬å¯ä»¥åƒæ·»åŠ å±‚ä¸€æ ·æŠŠä¸€ä¸ªæ¨¡å‹æ·»åŠ åˆ° Sequential model é‡Œï¼š


```python
# åœ¨å·ç§¯åŸºçš„åŸºç¡€ä¸Šæ·»åŠ å¯†é›†è¿æ¥åˆ†ç±»å™¨

from tensorflow.keras import models
from tensorflow.keras import layers

model = models.Sequential()
model.add(conv_base)
model.add(layers.Flatten())
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
```

çœ‹ä¸€ä¸‹æ¨¡å‹çš„æ ·å­ï¼š


```python
model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    vgg16 (Model)                (None, 4, 4, 512)         14714688  
    _________________________________________________________________
    flatten (Flatten)            (None, 8192)              0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 256)               2097408   
    _________________________________________________________________
    dense_3 (Dense)              (None, 1)                 257       
    =================================================================
    Total params: 16,812,353
    Trainable params: 16,812,353
    Non-trainable params: 0
    _________________________________________________________________


æ³¨æ„ï¼Œåœ¨ç”¨è¿™ç§ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•æ—¶ï¼Œä¸€å®šè¦**å†»ç»“å·ç§¯åŸº**ï¼Œä¹Ÿå°±æ˜¯å‘Šè¯‰ç½‘ç»œï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ç§ä¸è¦å»æ›´æ–°å·ç§¯åŸºçš„å‚æ•°ï¼è¿™ä¸ªå¾ˆé‡è¦ï¼Œä¸è¿™ä¹ˆåšçš„è¯é¢„è®­ç»ƒå¥½çš„æ¨¡å‹å°±ä¼šè¢«ç ´åï¼Œåˆ°å¤´æ¥ç›¸å½“äºä½ æ˜¯ä»å¤´è®­ç»ƒçš„ï¼Œé‚£æ ·å°±å¤±å»ç”¨é¢„è®­ç»ƒçš„æ„ä¹‰äº†ã€‚


```python
# å†»ç»“å·ç§¯åŸº
conv_base.trainable = False
```

è¿™æ ·åšä¹‹åï¼Œè®­ç»ƒæ¨¡å‹æ—¶å°±åªä¼šå»ä¸æ–­æ›´æ–° Dense å±‚çš„æƒé‡äº†ï¼š


```python
model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    vgg16 (Model)                (None, 4, 4, 512)         14714688  
    _________________________________________________________________
    flatten (Flatten)            (None, 8192)              0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 256)               2097408   
    _________________________________________________________________
    dense_3 (Dense)              (None, 1)                 257       
    =================================================================
    Total params: 16,812,353
    Trainable params: 2,097,665
    Non-trainable params: 14,714,688
    _________________________________________________________________


æ¥ä¸‹æ¥ï¼Œå°±å¯ä»¥ä¸Šæ•°æ®å¢å¼ºï¼Œè®­ç»ƒæ¨¡å‹äº†ï¼š


```python
# åˆ©ç”¨å†»ç»“çš„å·ç§¯åŸºç«¯åˆ°ç«¯åœ°è®­ç»ƒæ¨¡å‹

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import optimizers

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')

test_datagen = ImageDataGenerator(rescale=1./255)    # æ³¨æ„ test ä¸å¢å¼º

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(lr=2e-5),
              metrics=['acc'])

history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=30,
    validation_data=validation_generator,
    validation_steps=50)
```

P.S. è¿™ä¸ªåœ¨æˆ‘çš„ CPU ä¸Šè·‘ï¼Œä¸€è½®å¤§æ¦‚è¦15åˆ†é’Ÿï¼Œ30è½®ï¼Œï¼Œæˆ‘æ”¾å¼ƒäº†ã€‚æˆ‘ç”¨ Kaggle è·‘äº†è¿™ä¸ªï¼Œä¸€è½®æ‰30ç§’ğŸ˜­ï¼š

```
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Epoch 1/30
100/100 [==============================] - 30s 300ms/step - loss: 0.5984 - acc: 0.6855 - val_loss: 0.4592 - val_acc: 0.8250
Epoch 2/30
100/100 [==============================] - 30s 302ms/step - loss: 0.4854 - acc: 0.7815 - val_loss: 0.3660 - val_acc: 0.8710
......
Epoch 29/30
100/100 [==============================] - 30s 300ms/step - loss: 0.2774 - acc: 0.8825 - val_loss: 0.2402 - val_acc: 0.9040
Epoch 30/30
100/100 [==============================] - 30s 302ms/step - loss: 0.2693 - acc: 0.8900 - val_loss: 0.2401 - val_acc: 0.9070
```

![è¿è¡Œç»“æœå›¾](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh5lk985mvj30m60vqtcp.jpg)

### Fine-tuning å¾®è°ƒæ¨¡å‹

ï¼ˆæˆ‘æ¯”è¾ƒå–œæ¬¢ Fine-tuning è¿™ä¸ªè¯ï¼Œâ€œå¾®è°ƒâ€åè€Œæ²¡å†…å‘³å„¿äº†ã€‚ï¼‰

Fine-tuning æ˜¯è¡¥å……ç‰¹å¾æå–ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹çš„ã€‚Fine-tuning åšçš„æ˜¯å°†å·ç§¯åŸºé¡¶éƒ¨(é åé¢çš„)çš„å‡ å±‚**è§£å†»**ï¼ŒæŠŠè§£å†»çš„å‡ å±‚å’Œæ–°å¢åŠ çš„éƒ¨åˆ†(å…¨è¿æ¥åˆ†ç±»å™¨)è”åˆè®­ç»ƒã€‚è¿™ä¸ªæ–¹æ³•ç¨å¾®è°ƒæ•´äº†é¢„è®­ç»ƒæ¨¡å‹é‡Œé¢é‚£äº›é«˜çº§æŠ½è±¡çš„è¡¨ç¤º(ä¹Ÿå°±æ˜¯é‚£äº›æ¥è¿‘é¡¶å±‚çš„)ï¼Œä½¿ä¹‹æ›´é€‚åˆæˆ‘ä»¬æ‰‹å¤´çš„é—®é¢˜ã€‚

![Fine-tuning ç¤ºæ„å›¾ï¼Œå¾®è°ƒäº† VGG16 çš„æœ€åä¸€ä¸ªå·ç§¯å—](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh5nq823wfj30ik0t0tbf.jpg)

æ³¨æ„ï¼Œå¿…é¡»å…ˆæŠŠæœ€åçš„å…¨è¿æ¥åˆ†ç±»å™¨è®­ç»ƒå¥½ï¼Œæ‰èƒ½å» Fine-tune å·ç§¯åŸºé¡¶éƒ¨çš„ Conv å—ï¼Œä¸ç„¶å°±ä¼šæŠŠé¢„è®­ç»ƒçš„æˆæœå®Œå…¨ç ´åè°ƒã€‚

æ‰€ä»¥ï¼ŒFine-tuning éœ€è¦æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ï¼š

1. åœ¨å·²ç»è®­ç»ƒå¥½çš„ç½‘ç»œ(base networkï¼ŒåŸºç½‘ç»œ)ä¸Šæ·»åŠ æˆ‘ä»¬è‡ªå·±çš„ç½‘ç»œ(æ¯”å¦‚åˆ†ç±»å™¨);
2. å†»ç»“åŸºç½‘ç»œ;
3. è®­ç»ƒè‡ªå·±æ·»åŠ çš„é‚£ä¸€éƒ¨åˆ†;
4. è§£å†»åŸºç½‘ç»œçš„éƒ¨åˆ†å±‚;
5. è”åˆè®­ç»ƒè§£å†»çš„å±‚å’Œè‡ªå·±çš„é‚£éƒ¨åˆ†;

å‰ä¸‰æ­¥éƒ½å’Œç‰¹å¾æå–åšçš„æ˜¯ä¸€æ ·çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»ç¬¬å››æ­¥å¼€å§‹ã€‚é¦–å…ˆå†çœ‹ä¸€ä¸‹æˆ‘ä»¬çš„ VGG16 å·ç§¯åŸºï¼š


```python
conv_base.summary()
```

    Model: "vgg16"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         [(None, 150, 150, 3)]     0         
    _________________________________________________________________
    block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
    _________________________________________________________________
    block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
    _________________________________________________________________
    block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
    _________________________________________________________________
    block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
    _________________________________________________________________
    block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
    _________________________________________________________________
    block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
    _________________________________________________________________
    block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
    _________________________________________________________________
    block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
    _________________________________________________________________
    block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
    _________________________________________________________________
    block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
    _________________________________________________________________
    block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
    _________________________________________________________________
    block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
    _________________________________________________________________
    block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
    _________________________________________________________________
    block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
    _________________________________________________________________
    block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
    _________________________________________________________________
    block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
    _________________________________________________________________
    block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
    _________________________________________________________________
    block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
    =================================================================
    Total params: 14,714,688
    Trainable params: 0
    Non-trainable params: 14,714,688
    _________________________________________________________________


æˆ‘ä»¬å°†è§£å†» block5_conv1, block5_conv2 å’Œ block5_conv3 æ¥å®Œæˆ Fine-turningï¼š


```python
# å†»ç»“ç›´åˆ°æŸä¸€å±‚çš„æ‰€æœ‰å±‚

conv_base.trainable = True

set_trainable = False
for layer in conv_base.layers:
    if layer.name == 'block5_conv1':
        set_trainable = True
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False
        
# å¾®è°ƒæ¨¡å‹

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-5),    # è¿™é‡Œç”¨çš„å­¦ä¹ ç‡(lr)å¾ˆå°ï¼Œæ˜¯å¸Œæœ›è®©å¾®è°ƒçš„ä¸‰å±‚è¡¨ç¤ºå˜åŒ–èŒƒå›´ä¸è¦å¤ªå¤§
              metrics=['acc'])

history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=100,
    validation_data=validation_generator,
    validation_steps=50)
```

è¿™ä¸ªä¹Ÿæ˜¯åœ¨ Kaggle è·‘çš„ï¼š

![åœ¨ Kaggle è®­ç»ƒçš„æœ€åå‡ è½®è¾“å‡º](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh6fkf1r4hj31jw0gaq87.jpg)

![è®­ç»ƒå†å²å›¾çº¿](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh6flamoy7j30o60wkdl5.jpg)

æˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘åšå‡ºæ¥çš„å’Œä¹¦ä¸Šå·®è·å¾ˆå¤§ï¼Œæˆ‘åå¤çœ‹äº†å¥½è¿‡é”™éƒ½æ²¡å‘ç°æœ‰ä»€ä¹ˆé—®é¢˜ï¼Œå³ä½¿æˆ‘ç”¨ä½œè€…ç»™çš„ Notebook æ¥è·‘ä¹Ÿæ˜¯ğŸ˜‚ï¼Œä¸çŸ¥é“å“ªé‡Œå‡ºé—®é¢˜äº†ã€‚ä¸ç®¡äº†ï¼Ÿå°±è¿™æ ·å§ã€‚

æœ€åï¼Œæ¥çœ‹çœ‹åœ¨æµ‹è¯•é›†ä¸Šçš„ç»“æœï¼š

```python
test_generator = test_datagen.flow_from_directory(
        test_dir,
        target_size=(150, 150),
        batch_size=20,
        class_mode='binary')
test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)
print('test acc:', test_acc)
```

ä¹¦ä¸Šçš„ç»“æœå‡†ç¡®ç‡æœ‰ 97% äº†ï¼Œï¼Œæˆ‘ï¼Œæˆ‘çš„è¿˜ä¸åˆ° 95%ï¼Œå®³ã€‚

## å·ç§¯ç¥ç»ç½‘ç»œçš„å¯è§†åŒ–

> 5.4 Visualizing what convnets learn

æˆ‘ä»¬å¸¸è¯´æ·±åº¦å­¦ä¹ æ˜¯é»‘ç®±ï¼Œæˆ‘ä»¬å¾ˆéš¾ä»å…¶å­¦ä¹ çš„è¿‡ç¨‹ä¸­æå–å‡º human-readable çš„è¡¨ç¤ºã€‚ä½†æ˜¯ï¼Œåšè®¡ç®—æœºè§†è§‰çš„å·ç§¯ç¥ç»ç½‘ç»œä¸æ˜¯è¿™æ ·ï¼Œå·ç§¯ç¥ç»ç½‘ç»œæ˜¯èƒ½å¯è§†åŒ–çš„ï¼Œæˆ‘ä»¬çœ‹å¾—æ‡‚çš„ï¼Œå› ä¸ºå·ç§¯ç¥ç»ç½‘ç»œæœ¬æ¥å°±æ˜¯æå–â€œè§†è§‰æ¦‚å¿µçš„è¡¨ç¤ºâ€çš„å˜›ã€‚

ç°åœ¨å·²ç»æœ‰å¾ˆå¤šç§ä¸åŒçš„æ–¹æ³•å¯ä»¥ä»ä¸åŒè§’åº¦æŠŠå·ç§¯ç¥ç»ç½‘ç»œå¯è§†åŒ–ï¼Œå¹¶åˆç†è§£é‡Šå…¶æ„ä¹‰ã€‚ä¸‹é¢ä»‹ç»å…¶ä¸­å‡ ç§ã€‚

### å¯è§†åŒ–ä¸­é—´æ¿€æ´»

**å¯è§†åŒ–ä¸­é—´æ¿€æ´»**ï¼ˆVisualizing intermediate activationsï¼‰ï¼Œä¹Ÿå°±æ˜¯å¯è§†åŒ–å·ç§¯ç¥ç»ç½‘ç»œçš„ä¸­é—´è¾“å‡ºã€‚è¿™ä¸ªå¯è§†åŒ–å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£ä¸€ç³»åˆ—è¿ç»­å¾—å¤šå·ç§¯å±‚æ˜¯å¦‚ä½•å˜æ¢å¤„ç†è¾“å…¥æ•°æ®çš„ã€ä»¥åŠæ¯ä¸ªè¿‡æ»¤å™¨çš„åŸºæœ¬æ„ä¹‰ã€‚

è¿™ä¸ªå…¶å®å°±æ˜¯æŠŠå·ç§¯/æ± åŒ–å±‚è¾“å‡ºçš„ feature maps æ˜¾ç¤ºå‡ºæ¥çœ‹çœ‹ï¼ˆå±‚çš„è¾“å‡ºä¹Ÿå¯ä»¥å«åš activationï¼Œæ¿€æ´»ï¼‰ã€‚å…·ä½“çš„åšæ³•å°±æ˜¯æŠŠè¾“å‡ºçš„å„ä¸ª channel éƒ½åšæˆäºŒç»´çš„å›¾åƒæ˜¾ç¤ºå‡ºæ¥çœ‹ã€‚

æˆ‘ä»¬ç”¨ä¸€ä¸ªä¹‹å‰ç”Ÿæˆçš„æ¨¡å‹æ¥ä½œä¸ºä¾‹å­ï¼š


```python
from tensorflow.keras.models import load_model

model = load_model('/Volumes/WD/Files/dataset/dogs-vs-cats/cats_and_dogs_small_2.h5')
model.summary()
```

    Model: "sequential_5"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d_20 (Conv2D)           (None, 148, 148, 32)      896       
    _________________________________________________________________
    max_pooling2d_20 (MaxPooling (None, 74, 74, 32)        0         
    _________________________________________________________________
    conv2d_21 (Conv2D)           (None, 72, 72, 64)        18496     
    _________________________________________________________________
    max_pooling2d_21 (MaxPooling (None, 36, 36, 64)        0         
    _________________________________________________________________
    conv2d_22 (Conv2D)           (None, 34, 34, 128)       73856     
    _________________________________________________________________
    max_pooling2d_22 (MaxPooling (None, 17, 17, 128)       0         
    _________________________________________________________________
    conv2d_23 (Conv2D)           (None, 15, 15, 128)       147584    
    _________________________________________________________________
    max_pooling2d_23 (MaxPooling (None, 7, 7, 128)         0         
    _________________________________________________________________
    flatten_5 (Flatten)          (None, 6272)              0         
    _________________________________________________________________
    dropout (Dropout)            (None, 6272)              0         
    _________________________________________________________________
    dense_10 (Dense)             (None, 512)               3211776   
    _________________________________________________________________
    dense_11 (Dense)             (None, 1)                 513       
    =================================================================
    Total params: 3,453,121
    Trainable params: 3,453,121
    Non-trainable params: 0
    _________________________________________________________________


ç„¶åæˆ‘ä»¬æ‰¾ä¸€å¼ è®­ç»ƒçš„æ—¶å€™ç½‘ç»œæ²¡è§è¿‡çš„å›¾ç‰‡ï¼š


```python
img_path = '/Volumes/WD/Files/dataset/dogs-vs-cats/cats_and_dogs_small/test/cats/cat.1750.jpg'

from tensorflow.keras.preprocessing import image    # æŠŠå›¾ç‰‡ææˆ 4D å¼ é‡
import numpy as np

img = image.load_img(img_path, target_size=(150, 150))
img_tensor = image.img_to_array(img)
img_tensor = np.expand_dims(img_tensor, axis=0)
img_tensor /= 255.

print(img_tensor.shape)
```

    (1, 150, 150, 3)


æ˜¾ç¤ºå›¾ç‰‡ï¼š


```python
import matplotlib.pyplot as plt

plt.imshow(img_tensor[0])
plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh8669nh8uj3079070js1.jpg)


è¦æå–ç‰¹å¾å›¾ï¼Œå°±è¦ç”¨ä¸€ä¸ªè¾“å…¥å¼ é‡å’Œä¸€ä¸ªè¾“å‡ºå¼ é‡åˆ—è¡¨æ¥å®ä¾‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼š


```python
from tensorflow.keras import models

layer_outputs = [layer.output for layer in model.layers[:8]]    # æå–å‰ 8 å±‚çš„è¾“å‡º
activation_model = models.Model(inputs=model.input, outputs=layer_outputs)
```

å½“è¾“å…¥ä¸€å¼ å›¾åƒæ—¶ï¼Œè¿™ä¸ªæ¨¡å‹å°±ä¼šè¿”å›åŸå§‹æ¨¡å‹å‰ 8 å±‚çš„æ¿€æ´»å€¼ã€‚æˆ‘ä»¬ä¹‹å‰çš„æ¨¡å‹éƒ½æ˜¯ç»™ä¸€ä¸ªè¾“å…¥ï¼Œè¿”ä¸€ä¸ªè¾“å‡ºçš„ï¼Œä½†å…¶å®ä¸€ä¸ªæ¨¡å‹æ˜¯å¯ä»¥ç»™ä»»æ„ä¸ªè¾“å…¥ï¼Œè¿”ä»»æ„ä¸ªè¾“å‡ºçš„ã€‚

æ¥ä¸‹æ¥åœ¨æŠŠåˆšæ‰æ‰¾çš„å›¾ç‰‡è¾“å…¥è¿›å»ï¼š


```python
activations = activation_model.predict(img_tensor)
# è¿”å› 8 ä¸ª Numpy æ•°ç»„ç»„æˆçš„åˆ—è¡¨ï¼Œæ¯å±‚ä¸€ä¸ªï¼Œé‡Œé¢æ”¾ç€æ¿€æ´»
```


```python
first_layer_activation = activations[0]
print(first_layer_activation.shape)
```

    (1, 148, 148, 32)


è¿™ä¸œè¥¿æœ‰ 32 ä¸ª channelï¼Œæˆ‘ä»¬éšä¾¿æ‰“ä¸€ä¸ªå‡ºæ¥çœ‹çœ‹ï¼š


```python
# å°†ç¬¬ 4 ä¸ª channel å¯è§†åŒ–

import matplotlib.pyplot as plt
plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')
```




    <matplotlib.image.AxesImage at 0x13f7b3d10>




![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh8676bpw7j307f076wex.jpg)


è¿™ä¸ªæ¯ä¸ª channel æ˜¯å¹²å˜›çš„åŸºæœ¬æ˜¯éšæœºçš„ï¼Œæˆ‘è¿™ä¸ªå°±å’Œä¹¦ä¸Šä¸ä¸€æ ·ã€‚

ä¸‹é¢ï¼Œæ¥ç»˜åˆ¶æ•´ä¸ªç½‘ç»œä¸­å®Œæ•´çš„ã€æ‰€æœ‰æ¿€æ´»çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬å°† 8 ä¸ª feature mapsï¼Œæ¯ä¸ªå…¶ä¸­çš„æ‰€æœ‰ channels ç”»å‡ºæ¥äº†ï¼Œæ’åˆ°ä¸€å¼ å¤§å›¾ä¸Šï¼š


```python
# å°†æ¯ä¸ªä¸­é—´æ¿€æ´»çš„æ‰€æœ‰é€šé“å¯è§†åŒ–

layer_names = []
for layer in model.layers[:8]:
    layer_names.append(layer.name)
    
images_per_row = 16

for layer_name, layer_activation in zip(layer_names, activations):
    # å¯¹äºæ¯ä¸ª layer_activationï¼Œå…¶å½¢çŠ¶ä¸º (1, size, size, n_features)
    n_features = layer_activation.shape[-1]    # ç‰¹å¾å›¾é‡Œç‰¹å¾(channel)çš„ä¸ªæ•°
    
    size = layer_activation.shape[1]    # å›¾çš„å¤§å°
    
    n_cols = n_features // images_per_row
    display_grid = np.zeros((size * n_cols, images_per_row * size))    # åˆ†æ ¼
    
    for col in range(n_cols):
        for row in range(images_per_row):
            channel_image = layer_activation[0, :, :, col * images_per_row + row]
            
            # æŠŠå›¾ç‰‡æå¥½çœ‹ä¸€ç‚¹
            channel_image -= channel_image.mean()
            channel_image /= channel_image.std()
            channel_image *= 64
            channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype('uint8')
            display_grid[col * size : (col + 1) * size,
                         row * size : (row + 1) * size] = channel_image

    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1], 
                        scale * display_grid.shape[0]))

    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, aspect='auto', cmap='viridis')

```

    /usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh866jmct8j30q004c40i.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh8674iooxj30q004cdhw.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh866chv4nj30q007cdjl.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh866c10xej30q007cgpf.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh866ewppaj30q00dejxf.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh8666s9lfj30q00den33.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh866djarhj30q00dejul.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh866go1kzj30pu0dewhm.jpg)


å¯ä»¥çœ‹åˆ°ï¼Œç‰¹å¾è¶Šæ¥è¶ŠæŠ½è±¡ã€‚åŒæ—¶ï¼Œé»‘çš„ä¹Ÿè¶Šæ¥è¶Šå¤šï¼ˆç¨€ç–åº¦è¶Šæ¥è¶Šå¤§ï¼‰ï¼Œé‚£äº›éƒ½æ˜¯è¾“å…¥å›¾åƒä¸­æ‰¾ä¸åˆ°è¿™äº›è¿‡æ»¤å™¨æ‰€ç¼–ç çš„æ¨¡å¼ï¼Œæ‰€ä»¥å°±ç©ºç™½äº†ã€‚

è¿™é‡Œè¡¨ç°å‡ºæ¥ä¸€ä¸ªé‡è¦æ™®éç‰¹å¾ï¼šéšç€å±‚æ•°çš„åŠ æ·±ï¼Œå±‚æ‰€æå–çš„ç‰¹å¾å˜å¾—è¶Šæ¥è¶ŠæŠ½è±¡ã€‚æ›´é«˜çš„å±‚æ¿€æ´»åŒ…å«å…³äºç‰¹å®šè¾“å…¥çš„ä¿¡æ¯è¶Šæ¥è¶Šå°‘ï¼Œè€Œå…³äºç›®æ ‡çš„ä¿¡æ¯è¶Šæ¥è¶Šå¤šã€‚

ä¸äººç±»å’ŒåŠ¨ç‰©æ„ŸçŸ¥ä¸–ç•Œçš„æ–¹å¼ç±»ä¼¼ï¼šäººç±»è§‚å¯Ÿä¸€ä¸ªåœºæ™¯å‡ ç§’é’Ÿåï¼Œå¯ä»¥è®°ä½å…¶ä¸­æœ‰å“ªäº›æŠ½è±¡ç‰©ä½“(æ¯”å¦‚è‡ªè¡Œè½¦ã€æ ‘)ï¼Œä½†è®°ä¸ä½è¿™äº›ç‰©ä½“çš„å…·ä½“å¤–è§‚ã€‚ä½ çš„å¤§è„‘ä¼šè‡ªåŠ¨å°†è§†è§‰è¾“å…¥å®Œå…¨æŠ½è±¡åŒ–ï¼Œå³å°†å…¶è½¬æ¢ä¸ºæ›´é«˜å±‚æ¬¡çš„è§†è§‰æ¦‚å¿µï¼ŒåŒæ—¶è¿‡æ»¤æ‰ä¸ç›¸å…³çš„è§†è§‰ç»†èŠ‚ã€‚

æ·±åº¦ç¥ç»ç½‘ç»œåˆ©ç”¨ä¿¡æ¯è’¸é¦ç®¡é“ (information distillation pipeline)ï¼Œå°†è¾“å…¥åŸå§‹æ•°æ®(æœ¬ä¾‹ä¸­æ˜¯ RGB å›¾åƒ)ï¼Œåå¤è¿›è¡Œå˜æ¢ï¼Œå°†æ— å…³ä¿¡æ¯è¿‡æ»¤æ‰(æ¯”å¦‚å›¾åƒçš„å…·ä½“å¤–è§‚)ï¼Œå¹¶æ”¾å¤§å’Œç»†åŒ–æœ‰ç”¨çš„ä¿¡æ¯(æ¯”å¦‚å›¾åƒçš„ç±»åˆ«)ï¼Œæœ€ç»ˆå®Œæˆå¯¹ä¿¡æ¯çš„åˆ©ç”¨ï¼ˆæ¯”å¦‚åˆ¤æ–­å‡ºå›¾ç‰‡æ˜¯çŒ«è¿˜æ˜¯ç‹—ï¼‰ã€‚


### å¯è§†åŒ–å·ç§¯ç¥ç»ç½‘ç»œçš„è¿‡æ»¤å™¨

**å¯è§†åŒ–å·ç§¯ç¥ç»ç½‘ç»œçš„è¿‡æ»¤å™¨**ï¼ˆVisualizing convnets filtersï¼‰ï¼Œå¸®åŠ©ç†è§£å„ä¸ªè¿‡æ»¤å™¨å–„äºæ¥å—ä»€ä¹ˆæ ·çš„è§†è§‰æ¨¡å¼(patternï¼Œæˆ‘è§‰å¾—â€œæ¨¡å¼â€è¿™ä¸ªè¯å¹¶ä¸èƒ½å¾ˆå¥½çš„è¯ é‡Š pattern çš„æ„æ€)æˆ–æ¦‚å¿µã€‚

è¦è§‚å¯Ÿå·ç§¯ç¥ç»ç½‘ç»œå­¦åˆ°çš„è¿‡æ»¤å™¨ï¼Œæˆ‘ä»¬å¯ä»¥æ˜¾ç¤ºæ¯ä¸ªè¿‡æ»¤å™¨æ‰€å“åº”çš„è§†è§‰æ¨¡å¼ã€‚è¿™ä¸ªå¯ä»¥åœ¨è¾“å…¥ç©ºé—´ä¸Šç”¨ gradient ascentï¼ˆæ¢¯åº¦ä¸Šå‡ï¼‰æ¥å®ç°ï¼š

ä¸ºäº†ä»ç©ºç™½è¾“å…¥å›¾åƒå¼€å§‹ï¼Œè®©æŸä¸ªè¿‡æ»¤å™¨çš„å“åº”æœ€å¤§åŒ–ï¼Œå¯ä»¥å°†æ¢¯åº¦ä¸‹é™åº”ç”¨äºå·ç§¯ç¥ç»ç½‘ç»œè¾“å…¥å›¾åƒçš„å€¼ã€‚å¾—åˆ°çš„è¾“å…¥å›¾åƒå³ä¸ºå¯¹è¿‡æ»¤å™¨å…·æœ‰æœ€å¤§å“åº”çš„å›¾åƒã€‚

è¿™ä¸ªåšèµ·æ¥å¾ˆç®€å•ï¼Œæ„å»ºä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œå…¶ç›®çš„æ˜¯è®©æŸä¸ªå·ç§¯å±‚çš„æŸä¸ªè¿‡æ»¤å™¨çš„å€¼æœ€å¤§åŒ–ã€‚ç„¶åï¼Œä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ¥è°ƒèŠ‚è¾“å…¥å›¾åƒçš„å€¼ï¼Œä½¿ä¹‹æ¿€æ´»å€¼æœ€å¤§åŒ–ã€‚

ä¾‹å¦‚ï¼Œåœ¨ ImageNet ä¸Šé¢„è®­ç»ƒçš„ VGG16 ç½‘ç»œçš„ block3_conv1 å±‚ç¬¬ 0 ä¸ªè¿‡æ»¤å™¨æ¿€æ´»çš„æŸå¤±å°±æ˜¯ï¼š


```python
from tensorflow.keras.applications import VGG16
from tensorflow.keras import backend as K

model = VGG16(weights='imagenet', include_top=False)

layer_name = 'block3_conv1'
filter_index = 0

layer_output = model.get_layer(layer_name).output
loss = K.mean(layer_output[:, :, :, filter_index])

print(loss)
```

    Tensor("Mean_11:0", shape=(), dtype=float32)


è¦åšæ¢¯åº¦ä¸‹é™å˜›ï¼Œæ‰€ä»¥è¦å¾—åˆ° loss ç›¸å¯¹äºæ¨¡å‹è¾“å…¥çš„æ¢¯åº¦ã€‚ç”¨ Keras çš„ backend çš„ gradients å‡½æ•°æ¥å®Œæˆè¿™ä¸ªï¼š


```python
import tensorflow as tf
tf.compat.v1.disable_eager_execution()  # See https://github.com/tensorflow/tensorflow/issues/33135

grads = K.gradients(loss, model.input)[0]
# è¿™ä¸œè¥¿è¿”å›çš„æ˜¯ä¸€ä¸ªè£…ç€ä¸€ç³»åˆ—å¼ é‡çš„ listï¼Œåœ¨æ­¤å¤„ï¼Œlist é•¿åº¦ä¸º1ï¼Œæ‰€ä»¥å– [0] å°±æ˜¯ä¸€ä¸ªå¼ é‡
print(grads)
```

    Tensor("gradients_1/block1_conv1_4/Conv2D_grad/Conv2DBackpropInput:0", shape=(None, None, None, 3), dtype=float32)


è¿™é‡Œå¯ä»¥ç”¨ä¸€ä¸ªå°æŠ€å·§æ¥è®©æ¢¯åº¦ä¸‹é™è¿‡ç¨‹å¹³é¡ºåœ°è¿›è¡Œï¼šå°†æ¢¯åº¦å¼ é‡é™¤ä»¥å…¶ L2 èŒƒæ•°(å¼ é‡çš„å¹³æ–¹çš„å¹³å‡å€¼çš„å¹³æ–¹æ ¹)æ¥æ ‡å‡†åŒ–ã€‚è¿™ç§æ“ä½œå¯ä»¥ä½¿è¾“å…¥å›¾åƒæ›´æ–°åœ°å¤§å°å§‹ç»ˆä¿æŒåœ¨åŒä¸€ä¸ªèŒƒå›´é‡Œï¼š


```python
grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)    # + 1e-5 é˜²æ­¢é™¤ä»¥0
print(grads)
```

    Tensor("truediv_1:0", shape=(None, None, None, 3), dtype=float32)


ç°åœ¨éœ€è¦æ¥è¿‘çš„é—®é¢˜æ˜¯ï¼Œç»™å®šè¾“å…¥å›¾åƒï¼Œè®¡ç®—å‡º loss å’Œ grads çš„å€¼ã€‚è¿™ä¸ªå¯ä»¥ç”¨ iterate å‡½æ¥åšï¼šå®ƒå°†ä¸€ä¸ª Numpy å¼ é‡è½¬æ¢ä¸ºä¸¤ä¸ª Numpy å¼ é‡ç»„æˆçš„åˆ—è¡¨ï¼Œè¿™ä¸¤ä¸ªå¼ é‡åˆ†åˆ«æ˜¯æŸå¤±å€¼å’Œæ¢¯åº¦å€¼ï¼š


```python
iterate = K.function([model.input], [loss, grads])

import numpy as np
loss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])
print(loss_value, grads_value)
```

    0.0 [[[[0. 0. 0.]
       [0. 0. 0.]
       [0. 0. 0.]
       ...
       ...
       [0. 0. 0.]
       [0. 0. 0.]
       [0. 0. 0.]]]]


ç„¶åï¼Œå°±æ¥å†™æ¢¯åº¦ä¸‹é™äº†ï¼š


```python
# é€šè¿‡éšæœºæ¢¯åº¦ä¸‹é™è®©æŸå¤±æœ€å¤§åŒ–

input_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.   # éšä¾¿ä¸€ä¸ªæœ‰ç‡¥ç‚¹çš„ç°åº¦å›¾

plt.imshow(input_img_data[0, :, :, :] / 225.)
plt.figure()

step = 1.    # æ¢¯åº¦æ›´æ–°çš„æ­¥é•¿
for i in range(40):
    loss_value, grads_value = iterate([input_img_data])
    input_img_data += grads_value * step    # æ²¿ç€è®©æŸå¤±æœ€å¤§åŒ–çš„æ–¹å‘è°ƒèŠ‚è¾“å…¥å›¾åƒ
    
plt.imshow(input_img_data[0, :, :, :] / 225.)
plt.show()
```

    Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh8673z1cwj30790700sy.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh86752ufkj307907074x.jpg)

Emmmï¼Œåˆšæ‰è¿™ä¸ªç”»å›¾æ˜¯æˆ‘éšä¾¿å†™çš„å•¦ï¼Œæ‰€ä»¥çˆ†äº†ä¸ª Warningï¼Œä¸ºäº†æ­£è§„åœ°ç”»å‡ºå›¾æ¥,ä¸‹é¢å¥½å¥½å¤„ç†ä¸€ä¸‹ï¼š

```python
# å°†å¼ é‡è½¬æ¢ä¸ºæœ‰æ•ˆå›¾åƒçš„ utility å‡½æ•°

def deprocess_image(x):
    # æ ‡å‡†åŒ–ï¼Œå‡å€¼ä¸º 0ï¼Œ æ ‡å‡†å·®ä¸º 0.1
    x -= x.mean()
    x /= (x.std() + 1e-5)
    x *= 0.1
    
    # è£å‰ªåˆ° [0, 1]
    x += 0.5
    x = np.clip(x, 0, 1)
    
    # å°† x è½¬æ¢ä¸º RGB æ•°ç»„
    x *= 255
    x = np.clip(x, 0, 255).astype('uint8')
    return x

```

æŠŠä¸Šé¢é‚£äº›ä¸œè¥¿å…¨éƒ¨æ‹¼èµ·æ¥å°±å¯ä»¥å¾—åˆ°å®Œæ•´çš„ç”Ÿæˆè¿‡æ»¤å™¨å¯è§†åŒ–çš„å‡½æ•°äº†ï¼š


```python
# ç”Ÿæˆè¿‡æ»¤å™¨å¯è§†åŒ–çš„å‡½æ•°

import tensorflow as tf
tf.compat.v1.disable_eager_execution()  # See https://github.com/tensorflow/tensorflow/issues/33135

def generate_pattern(layer_name, filter_index, size=150):
    # æ„å»ºä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œå°† layer_name å±‚ç¬¬ filter_index ä¸ªè¿‡æ»¤å™¨çš„æ¿€æ´»æœ€å¤§åŒ–
    layer_output = model.get_layer(layer_name).output
    loss = K.mean(layer_output[:, :, :, filter_index])
    
    # è®¡ç®—æŸå¤±ç›¸å¯¹äºè¾“å…¥å›¾åƒçš„æ¢¯åº¦ï¼Œå¹¶å°†æ¢¯åº¦æ ‡å‡†åŒ–
    grads = K.gradients(loss, model.input)[0]
    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)
    
    # è¿”å›ç»™å®šè¾“å…¥å›¾åƒçš„æŸå¤±å’Œæ¢¯åº¦
    iterate = K.function([model.input], [loss, grads])
    
    # ä»å¸¦æœ‰å™ªå£°çš„ç°åº¦å›¾å¼€å§‹ï¼Œæ¢¯åº¦ä¸Šå‡ 40 æ¬¡
    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.
    step = 1.
    for i in range(40):
        loss_value, grads_value = iterate([input_img_data])
        input_img_data += grads_value * step
    
    # è¾“å‡ºç»“æœ
    img = input_img_data[0]
    return deprocess_image(img)
```

ç„¶åå°±å¯ä»¥ç”¨äº†ï¼Œè¿˜æ˜¯é‚£åˆšæ‰é‚£ä¸ªä¾‹å­æ˜¯ä¸€ä¸‹|ï¼š


```python
plt.imshow(generate_pattern('block3_conv1', 1))
```




    <matplotlib.image.AxesImage at 0x167d78950>




![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh866foirwj3079070t9a.jpg)


è¿™ä¸ªå›¾å°±æ˜¯ block3_conv1 å±‚ç¬¬ 0 ä¸ªè¿‡æ»¤å™¨çš„å“åº”äº†ã€‚è¿™ç§ pattern å«åš polka-dotï¼ˆæ³¢å°”å¡ç‚¹å›¾æ¡ˆï¼‰ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬æŠŠæ¯ä¸€å±‚çš„æ¯ä¸ªè¿‡æ»¤å™¨éƒ½å¯è§†åŒ–ã€‚ä¸ºäº†å¿«ä¸€ç‚¹ï¼Œæˆ‘ä»¬åªå¯è§†åŒ–æ¯ä¸ªå·ç§¯å—çš„ç¬¬ä¸€å±‚çš„å‰64ä¸ªè¿‡æ»¤å™¨ï¼ˆé˜²æ­¢ä¹Ÿæ²¡ä»€ä¹ˆæ„ä¹‰ï¼Œå°±æ˜¯çœ‹çœ‹ï¼Œéšä¾¿æå‡ ä¸ªå°±è¡Œäº†ï¼‰ï¼š


```python
# ç”Ÿæˆä¸€å±‚ä¸­æ‰€æœ‰è¿‡æ»¤å™¨å“åº”æ¨¡å¼ç»„æˆçš„ç½‘æ ¼
def generate_patterns_in_layer(layer_name, size=64, margin=5):
    results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3), dtype=np.int)
    for i in range(8):
        for j in range(8):
            filter_img = generate_pattern(layer_name, i + (j * 8), size=size)
            
            horizontal_start = i * size + i * margin
            horizontal_end = horizontal_start + size
            
            vertical_start = j * size + j * margin
            vertical_end = vertical_start + size
            
            results[horizontal_start: horizontal_end,
                    vertical_start: vertical_end, :] = filter_img
            
    return results


layer_names = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1']
for layer in layer_names:
    img = generate_patterns_in_layer(layer)
    plt.figure(figsize=(20, 20))
    plt.title(layer)
    plt.imshow(img)
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh86676843j30q00den33.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh8668h3n8j30u00u44d3.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh866aay9sj30u00u4du6.jpg)



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh866b9rihj30u00u4nbg.jpg)


è¿™äº›å›¾é‡Œå°±è¡¨ç°äº†å·ç§¯ç¥ç»ç½‘ç»œçš„å±‚å¦‚ä½•è§‚å¯Ÿå›¾ç‰‡ä¸­çš„ä¿¡æ¯çš„ï¼šå·ç§¯ç¥ç»ç½‘ç»œä¸­æ¯ä¸€å±‚éƒ½å­¦ä¹ ä¸€ç»„è¿‡æ»¤å™¨ï¼Œç„¶åè¾“å…¥ä¼šè¢«è¡¨ç¤ºæˆè¿‡æ»¤å™¨çš„ç»„åˆï¼Œè¿™ä¸ªå…¶å®ç±»ä¼¼äºå‚…é‡Œå¶å˜æ¢çš„è¿‡ç¨‹ã€‚éšç€å±‚æ•°çš„åŠ æ·±ï¼Œå·ç§¯ç¥ç»ç½‘ç»œä¸­çš„è¿‡æ»¤å™¨å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œè¶Šæ¥è¶Šç²¾ç»†ï¼Œæ‰€ä»¥å°±å¯ä»¥æå–ã€â€œç†è§£â€æ ¹æ®æŠ½è±¡çš„ä¿¡æ¯äº†ã€‚

### å¯è§†åŒ–å›¾åƒä¸­ç±»æ¿€æ´»çš„çƒ­åŠ›å›¾

**å¯è§†åŒ–å›¾åƒä¸­ç±»æ¿€æ´»çš„çƒ­åŠ›å›¾**ï¼ˆVisualizing heatmaps of class activation in an imageï¼‰ï¼Œç”¨æ¥äº†è§£ç½‘ç»œæ˜¯é å“ªéƒ¨åˆ†å›¾åƒæ¥è¯†åˆ«ä¸€ä¸ªç±»çš„ï¼Œä¹Ÿæœ‰åŠ©äºçŸ¥é“ç‰©ä½“åœ¨å›¾ç‰‡çš„å“ªä¸ªä½ç½®ã€‚

ç”¨æ¥å¯¹è¾“å…¥å›¾åƒç”Ÿæˆç±»æ¿€æ´»çš„çƒ­åŠ›å›¾çš„ä¸€ç§æŠ€æœ¯å« CAM (class activation map ç±»æ¿€æ´»å›¾)å¯è§†åŒ–ã€‚ç±»æ¿€æ´»çƒ­åŠ›å›¾å¯¹ä»»æ„è¾“å…¥çš„å›¾åƒçš„æ¯ä¸ªä½ç½®è¿›è¡Œè®¡ç®—ï¼Œè¡¨ç¤ºå‡ºæ¯ä¸ªä½ç½®å¯¹è¯¥ç±»åˆ«çš„é‡è¦ç¨‹åº¦ã€‚æ¯”å¦‚æˆ‘ä»¬çš„çŒ«ç‹—åˆ†ç±»ç½‘ç»œé‡Œï¼Œå¯¹ä¸€ä¸ªçŒ«çš„å›¾ç‰‡ç”Ÿæˆç±»æ¿€æ´»çƒ­åŠ›å›¾ï¼Œå¯ä»¥å¾—åˆ°è¿™ä¸ªå›¾ç‰‡çš„å„ä¸ªä¸åŒçš„éƒ¨ä½æœ‰å¤šåƒçŒ«ï¼ˆå¯¹æ¨¡å‹è®¤ä¸ºè¿™æ˜¯çŒ«çš„å›¾ç‰‡èµ·äº†è¿‡å¤§çš„ä½œç”¨ï¼‰ã€‚

å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨ [Grad-CAM](https://arxiv.org/abs/ 1610.02391.) è¿™ä¸ªæ–¹æ³•ï¼šç»™å®šä¸€å¼ è¾“å…¥å›¾åƒï¼Œå¯¹äºä¸€ä¸ªå·ç§¯å±‚çš„è¾“å‡ºç‰¹å¾å›¾ï¼Œç”¨ç±»åˆ«ç›¸å¯¹äºé€šé“çš„æ¢¯åº¦å¯¹è¿™ä¸ªç‰¹å¾å›¾ä¸­çš„æ¯ä¸ªé€šé“è¿›è¡ŒåŠ æƒã€‚è¯´äººè¯å°±æ˜¯ç”¨ã€Œæ¯ä¸ªé€šé“å¯¹åˆ†ç±»çš„é‡è¦ç¨‹åº¦ã€å¯¹ã€Œè¾“å…¥å›¾åƒå¯¹ä¸åŒé€šé“çš„æ¿€æ´»çš„å¼ºå¼±ç¨‹åº¦ã€çš„ç©ºé—´å›¾è¿›è¡ŒåŠ æƒï¼Œä»è€Œå¾—åˆ°ã€Œè¾“å…¥å›¾åƒå¯¹ç±»åˆ«çš„æ¿€æ´»å¼ºåº¦ã€çš„ç©ºé—´å›¾ã€‚ï¼ˆemmmï¼Œè¿™ç§è¶…é•¿çš„å¥å­çœ‹åŸæ–‡æ¯”è¾ƒå¥½ğŸ˜­ï¼šIntuitively, one way to understand this trick is that youâ€™re weighting a spatial map of â€œhow intensely the input image activates different channelsâ€ by â€œhow important each channel is with regard to the class,â€ resulting in a spatial map of â€œhow intensely the input image activates the class.â€ï¼‰

æˆ‘ä»¬åœ¨ VGG16 æ¨¡å‹æ¥æ¼”ç¤ºè¿™ä¸ªæ–¹æ³•ï¼š


```python
# åŠ è½½å¸¦æœ‰é¢„è®­ç»ƒæƒé‡çš„ VGG16 ç½‘ç»œ

from tensorflow.keras.applications.vgg16 import VGG16

model = VGG16(weights='imagenet')    # æ³¨æ„è¿™ä¸ªæ˜¯å¸¦æœ‰åˆ†ç±»å™¨çš„ï¼Œæ¯”è¾ƒå¤§ï¼Œä¸‹è½½ç¨æ…¢ï¼ˆæœ‰500+MBï¼‰
```



```python
model.summary()
```

    Model: "vgg16"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         [(None, 224, 224, 3)]     0         
    _________________________________________________________________
    block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      
    _________________________________________________________________
    block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     
    _________________________________________________________________
    block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         
    _________________________________________________________________
    block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     
    _________________________________________________________________
    block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    
    _________________________________________________________________
    block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         
    _________________________________________________________________
    block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    
    _________________________________________________________________
    block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    
    _________________________________________________________________
    block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    
    _________________________________________________________________
    block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         
    _________________________________________________________________
    block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   
    _________________________________________________________________
    block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   
    _________________________________________________________________
    block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   
    _________________________________________________________________
    block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         
    _________________________________________________________________
    block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   
    _________________________________________________________________
    block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   
    _________________________________________________________________
    block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   
    _________________________________________________________________
    block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         
    _________________________________________________________________
    flatten (Flatten)            (None, 25088)             0         
    _________________________________________________________________
    fc1 (Dense)                  (None, 4096)              102764544 
    _________________________________________________________________
    fc2 (Dense)                  (None, 4096)              16781312  
    _________________________________________________________________
    predictions (Dense)          (None, 1000)              4097000   
    =================================================================
    Total params: 138,357,544
    Trainable params: 138,357,544
    Non-trainable params: 0
    _________________________________________________________________


ç„¶åæˆ‘ä»¬ç…§ä¸€å¼ ç”¨æ¥æµ‹è¯•çš„å›¾ç‰‡ï¼š

![creative_commons_elephant](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh83vzik31j30oz0goq7t.jpg)

è¿™æ˜¯ä¸¤åªäºšæ´²è±¡ğŸ˜å“¦ï¼ŒæŠŠè¿™ä¸ªå›¾ç‰‡å¤„ç†æˆ VGG16 æ¨¡å‹éœ€è¦çš„æ ·å­ï¼š

```python
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions
import numpy as np

img_path = './creative_commons_elephant.jpg'
img = image.load_img(img_path, target_size=(224, 224))

img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
```

é¢„æµ‹ä¸€ä¸‹å›¾ç‰‡é‡Œçš„æ˜¯å•¥ï¼š


```python
preds = model.predict(x)
print('Predicted:', decode_predictions(preds, top=3)[0])
```

    Predicted: [('n02504458', 'African_elephant', 0.909421), ('n01871265', 'tusker', 0.086182885), ('n02504013', 'Indian_elephant', 0.0043545826)]


æœ‰ 90% çš„æŠŠæ¡æ˜¯äºšæ´²è±¡ï¼Œä¸é”™ã€‚æ¥ä¸‹æ¥å°±è¦ç”¨ Grad-CAM ç®—æ³•æ¥æ˜¾ç¤ºå›¾åƒä¸­å“ªäº›éƒ¨åˆ†æœ€åƒéæ´²è±¡äº†ã€‚


```python
from tensorflow.keras import backend as K

import tensorflow as tf
tf.compat.v1.disable_eager_execution()  # See https://github.com/tensorflow/tensorflow/issues/33135

african_elephant_output = model.output[:, 386]    # è¿™ä¸ªæ˜¯è¾“å‡ºå‘é‡ä¸­ä»£è¡¨â€œéæ´²è±¡â€çš„å…ƒç´ 

last_conv_layer = model.get_layer('block5_conv3')   # VGG16 æœ€åä¸€ä¸ªå·ç§¯å±‚çš„è¾“å‡ºç‰¹å¾å›¾

grads = K.gradients(african_elephant_output, last_conv_layer.output)[0]

pooled_grads = K.mean(grads, axis=(0, 1, 2))    # ç‰¹å®šç‰¹å¾å›¾é€šé“çš„æ¢¯åº¦å¹³å‡å¤§å°ï¼Œå½¢çŠ¶ä¸º (512,) 

iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])

pooled_grads_value, conv_layer_output_value = iterate([x])    # å¯¹åˆšæ‰çš„æµ‹è¯•ğŸ˜å›¾ç‰‡è®¡ç®—å‡ºæ¢¯åº¦å’Œç‰¹å¾å›¾

for i in range(512):
    # å°†ç‰¹å¾å›¾æ•°ç»„çš„æ¯ä¸ª channel ä¹˜ä»¥ã€Œè¿™ä¸ª channel å¯¹â€˜å¤§è±¡â€™ç±»åˆ«çš„é‡è¦ç¨‹åº¦ã€
    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]
    
heatmap = np.mean(conv_layer_output_value, axis=-1)  # å¤„ç†åçš„ç‰¹å¾å›¾çš„é€é€šé“å¹³å‡å€¼å³ä¸ºç±»æ¿€æ´»çš„çƒ­åŠ›å›¾
```

æŠŠå®ƒç”»å‡ºæ¥çœ‹çœ‹ï¼š


```python
import matplotlib.pyplot as plt

heatmap = np.maximum(heatmap, 0)
heatmap /= np.max(heatmap)
plt.matshow(heatmap)
```




    <matplotlib.image.AxesImage at 0x13d935210>




![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh8669908hj3078076mx8.jpg)


emmmmï¼Œçœ‹ä¸æ‡‚å•Šï¼Œæ‰€ä»¥ï¼Œæˆ‘ä»¬ç”¨ OpenCV æŠŠè¿™ä¸ªå åŠ åˆ°åŸå›¾ä¸Šå»çœ‹ï¼š


```python
import cv2

img = cv2.imread(img_path)    # åŠ è½½åŸå›¾
heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))    # è°ƒæ•´çƒ­åŠ›å›¾å¤§å°ï¼Œç¬¦åˆåŸå›¾
heatmap = np.uint8(255 * heatmap)    # è½¬æ¢ä¸º RGB æ ¼å¼
heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
superimposed_img = heatmap * 0.4 + img    # å åŠ 
cv2.imwrite('./elephant_cam.jpg', superimposed_img)    # ä¿å­˜
```




    True



å¾—åˆ°çš„å›¾åƒå¦‚ä¸‹ï¼š

![å åŠ åˆ°åŸå›¾çš„çƒ­åŠ›å›¾](https://tva1.sinaimg.cn/large/007S8ZIlgy1gh84yvepspj30oz0go7a9.jpg)

å¯ä»¥çœ‹å‡ºï¼ŒVGG16 ç½‘ç»œå…¶å®åªè¯†åˆ«äº†é‚£åªå°çš„è±¡ï¼Œæ³¨æ„ï¼Œå°è±¡å¤´éƒ¨çš„æ¿€æ´»å¼ºåº¦å¾ˆå¤§ï¼Œè¿™å¯èƒ½å°±æ˜¯ç½‘ç»œæ‰¾åˆ°çš„éæ´²è±¡çš„ç‹¬ç‰¹ä¹‹å¤„ã€‚

