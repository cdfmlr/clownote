---
categories:
- Machine Learning
- Deep Learning with Python
date: 2020-07-08 11:35:00
tag:
- Machine Learning
- Deep Learning
title: Pythonæ·±åº¦å­¦ä¹ ä¹‹ç¥ç»ç½‘ç»œå…¥é—¨
---

# Deep Learning with Python

è¿™ç¯‡æ–‡ç« æ˜¯æˆ‘å­¦ä¹ ã€ŠDeep Learning with Pythonã€‹(ç¬¬äºŒç‰ˆï¼ŒFranÃ§ois Chollet è‘—) æ—¶å†™çš„ç³»åˆ—ç¬”è®°ä¹‹ä¸€ã€‚æ–‡ç« çš„å†…å®¹æ˜¯ä»  Jupyter notebooks è½¬æˆ Markdown çš„ï¼Œå½“æˆ‘å®Œæˆæ‰€ä»¥æ–‡ç« åï¼Œä¼šåœ¨ GitHub å‘å¸ƒæˆ‘å†™çš„æ‰€æœ‰  Jupyter notebooksã€‚

ä½ å¯ä»¥åœ¨è¿™ä¸ªç½‘å€åœ¨çº¿é˜…è¯»è¿™æœ¬ä¹¦çš„æ­£ç‰ˆåŸæ–‡(è‹±æ–‡)ï¼šhttps://livebook.manning.com/book/deep-learning-with-python

è¿™æœ¬ä¹¦çš„ä½œè€…ä¹Ÿç»™å‡ºäº†ä¸€å¥— Jupyter notebooksï¼šhttps://github.com/fchollet/deep-learning-with-python-notebooks

---

æœ¬æ–‡ä¸º **ç¬¬3ç«  ç¥ç»ç½‘ç»œå…¥é—¨** (Chapter 3. Getting started with neural networks) çš„ç¬”è®°æ•´åˆã€‚

æœ¬æ–‡ç›®å½•ï¼š

[TOC]


## ç”µå½±è¯„è®ºåˆ†ç±»:äºŒåˆ†ç±»é—®é¢˜

[åŸæ–‡é“¾æ¥](https://livebook.manning.com/book/deep-learning-with-python/chapter-3/101)

### IMDB æ•°æ®é›†

IMDB æ•°æ®é›†é‡Œæ˜¯ 50,000 æ¡ç”µå½±è¯„è®ºã€‚ä¸€åŠæ˜¯è®­ç»ƒé›†ï¼Œä¸€åŠæ˜¯æµ‹è¯•é›†ã€‚
æ•°æ®é‡Œ 50% æ˜¯ç§¯æè¯„ä»·ï¼Œ50% æ˜¯æ¶ˆæè¯„ä»·ã€‚

Keras å†…ç½®äº†åšè¿‡é¢„å¤„ç†çš„ IMDB æ•°æ®é›†ï¼ŒæŠŠå•è¯åºåˆ—è½¬åŒ–æˆäº†æ•´æ•°åºåˆ—ï¼ˆä¸€ä¸ªæ•°å­—å¯¹åº”å­—å…¸é‡Œçš„è¯ï¼‰ï¼š


```python
from tensorflow.keras.datasets import imdb

# æ•°æ®é›†
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
    num_words=10000)
```

`num_words=10000` æ˜¯åªä¿ç•™å‡ºç°é¢‘ç‡å‰ 10000 çš„å•è¯ã€‚

å…ˆæ¥éšä¾¿çœ‹ä¸€æ¡è¯„è®ºï¼Œè¿™æ˜¯æ¡å¥½è¯„ï¼š


```python
# å­—å…¸
index_word = {v: k for k, v in imdb.get_word_index().items()}

# è¿˜åŸä¸€æ¡è¯„è®ºçœ‹çœ‹
text = ' '.join([index_word[i] for i in train_data[0]])

print(f"{train_labels[0]}:", text)
```

    1: the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an


### æ•°æ®å‡†å¤‡

å…ˆçœ‹ä¸€ä¸‹ train_data ç°åœ¨çš„å½¢çŠ¶ï¼š


```python
train_data.shape
```

è¾“å‡ºï¼š


    (25000,)



æˆ‘ä»¬è¦æŠŠå®ƒå˜æˆ `(samples, word_indices)` çš„æ ·å­ï¼Œå¤§æ¦‚æ˜¯ä¸‹é¢è¿™ç§ï¼š

```
[[0, 0, ..., 1, ..., 0, ..., 1],
 [0, 1, ..., 0, ..., 1, ..., 0],
 ...
]
```

æœ‰è¿™ä¸ªè¯å°±æ˜¯ 1ï¼Œæ²¡æœ‰å°±æ˜¯ 0ã€‚


```python
import numpy as np

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

```


```python
x_train
```

è¾“å‡ºï¼š


    array([[0., 1., 1., ..., 0., 0., 0.],
           [0., 1., 1., ..., 0., 0., 0.],
           [0., 1., 1., ..., 0., 0., 0.],
           ...,
           [0., 1., 1., ..., 0., 0., 0.],
           [0., 1., 1., ..., 0., 0., 0.],
           [0., 1., 1., ..., 0., 0., 0.]])



labels ä¹Ÿéšä¾¿æä¸€ä¸‹ï¼š


```python
train_labels
```

è¾“å‡ºï¼š


    array([1, 0, 0, ..., 0, 1, 0])

å¤„ç†ä¸€ä¸‹ï¼š


```python
y_train = np.asarray(train_labels).astype('float32')
y_test = np.asarray(test_labels).astype('float32')
```


```python
y_train
```

è¾“å‡ºï¼š


    array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)



ç°åœ¨è¿™äº›æ•°æ®å°±å¯ä»¥å®‰å…¨æŠ•å–‚æˆ‘ä»¬ä¸€ä¼šå„¿å»ºçš„ç¥ç»ç½‘ç»œäº†ã€‚

### å»ºç«‹ç½‘ç»œ

å¯¹äºè¿™ç§è¾“å…¥æ˜¯å‘é‡ã€æ ‡ç­¾æ˜¯æ ‡é‡ï¼ˆç”šè‡³æ˜¯ 0 æˆ– 1ï¼‰çš„é—®é¢˜ï¼š
ä½¿ç”¨ relu æ¿€æ´»çš„ Dense (å…¨è¿æ¥)å †èµ·æ¥çš„ç½‘ç»œï¼š

```python
Dense(16, activation='relu')
```

è¿™ç§å±‚çš„ä½œç”¨æ˜¯ `output = relu(dot(W, input) + b)`ã€‚

16 æ˜¯æ¯å±‚é‡Œéšè—å•å…ƒ(hidden unit)çš„ä¸ªæ•°ã€‚ä¸€ä¸ª hidden unit å°±æ˜¯åœ¨è¿™å±‚çš„è¡¨ç¤ºç©ºé—´é‡Œçš„ä¸€ä¸ªç»´åº¦ã€‚
W çš„å½¢çŠ¶ä¹Ÿæ˜¯ `(input_dimension, 16)`ï¼Œdot å‡ºæ¥å°±æ˜¯ä¸ª 16 ç»´çš„å‘é‡ï¼Œä¹Ÿå°±æŠŠæ•°æ®æŠ•å½±åˆ°äº† 16 ç»´çš„è¡¨ç¤ºç©ºé—´ã€‚

è¿™ä¸ªç»´åº¦ (hidden unit çš„æ•°é‡) å¯ä»¥çœ‹æˆå¯¹ç½‘ç»œå­¦ä¹ çš„è‡ªç”±åº¦çš„æ§åˆ¶ã€‚
ç»´åº¦è¶Šé«˜ï¼Œèƒ½å­¦çš„ä¸œè¥¿è¶Šå¤æ‚ï¼Œä½†è®¡ç®—æ¶ˆè€—ä¹Ÿè¶Šå¤§ï¼Œè€Œä¸”å¯èƒ½å­¦åˆ°ä¸€äº›ä¸é‡è¦çš„ä¸œè¥¿å¯¼è‡´è¿‡æ‹Ÿåˆã€‚

è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸¤å±‚è¿™ç§16ä¸ªéšè—å•å…ƒçš„å±‚ï¼Œ
æœ€åè¿˜æœ‰ä¸€ä¸ª sigmoid æ¿€æ´»çš„å±‚æ¥è¾“å‡ºç»“æœï¼ˆåœ¨$[0, 1]$å†…çš„å€¼ï¼‰ï¼Œ
è¿™ä¸ªç»“æœè¡¨ç¤ºé¢„æµ‹æœ‰å¤šå¯èƒ½è¿™ä¸ªæ•°æ®çš„æ ‡ç­¾æ˜¯1ï¼Œå³ä¸€æ¡å¥½è¯„ã€‚

relu æ˜¯è¿‡æ»¤æ‰è´Ÿå€¼ï¼ˆæŠŠè¾“å…¥çš„è´Ÿå€¼è¾“å‡ºæˆ0ï¼‰ï¼Œsigmoid æ˜¯æŠŠå€¼æŠ•åˆ° `[0, 1]`ï¼š

![relu and sigmoid](https://tva1.sinaimg.cn/large/007S8ZIlgy1ge8cohvw31j31hi0lu49o.jpg)

åœ¨ Keras ä¸­å®ç°è¿™ä¸ªç½‘ç»œï¼š


```python
from tensorflow.keras import models
from tensorflow.keras import layers

model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
```

#### æ¿€æ´»å‡½æ•°çš„ä½œç”¨

æˆ‘ä»¬ä¹‹å‰åœ¨ MNIST é‡Œç”¨è¿‡ relu æ¿€æ´»å‡½æ•°ï¼Œæ‰€ä»¥*æ¿€æ´»å‡½æ•°*åˆ°åº•æ˜¯å¹²å˜›çš„ï¼Ÿ

ä¸€ä¸ªæ²¡æœ‰æ¿€æ´»å‡½æ•°çš„ Dense å±‚çš„ä½œç”¨åªæ˜¯ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼š

```
output = dot(W, input) + b
```

å¦‚æœæ¯ä¸€å±‚éƒ½æ˜¯è¿™ç§çº¿æ€§å˜æ¢ï¼ŒæŠŠå¤šä¸ªè¿™ç§å±‚å åœ¨ä¸€èµ·ï¼Œå‡è®¾ç©ºé—´å¹¶ä¸ä¼šå˜å¤§ï¼Œæ‰€ä»¥èƒ½å­¦åˆ°çš„ä¸œè¥¿å¾ˆæœ‰é™ã€‚

è€Œæ¿€æ´»å‡½æ•°å°±æ˜¯åœ¨ `dot(W, input) + b` å¤–é¢å¥—çš„ä¸€ä¸ªå‡½æ•°ï¼Œæ¯”å¦‚ relu æ¿€æ´»æ˜¯ `output = relu(dot(W, input) + b)`ã€‚
åˆ©ç”¨è¿™ç§æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥æ‹“å±•è¡¨ç¤ºç©ºé—´ï¼Œä¹Ÿå°±å¯ä»¥è®©ç½‘ç»œå­¦ä¹ åˆ°æ›´å¤æ‚çš„â€œçŸ¥è¯†â€ã€‚

### ç¼–è¯‘æ¨¡å‹

ç¼–è¯‘æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦é€‰æ‹©æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’ŒæŒ‡æ ‡ã€‚

å¯¹è¿™ç§æœ€åè¾“å‡º 0 æˆ– 1 çš„äºŒå…ƒåˆ†ç±»é—®é¢˜ï¼ŒæŸå¤±å‡½æ•°å¯ä»¥ä½¿ç”¨ `binary_crossentropy`(ä»åå­—å°±å¯ä»¥çœ‹å¾—å‡ºæ¥å¾ˆåˆé€‚å•¦)ã€‚

è¿™ä¸ª *crossentropy* ä¸­æ–‡å«äº¤å‰ç†µï¼Œæ˜¯ä¿¡æ¯è®ºé‡Œçš„ï¼Œæ˜¯ç”¨æ¥è¡¡é‡æ¦‚ç‡åˆ†å¸ƒç›´æ¥çš„è·ç¦»çš„ã€‚
æ‰€ä»¥è¾“å‡ºæ¦‚ç‡çš„æ¨¡å‹ç»å¸¸æ˜¯ç”¨è¿™ç§ crossentropy åšæŸå¤±çš„ã€‚

è‡³äºä¼˜åŒ–å™¨ï¼Œå’Œ MNIST ä¸€æ ·ï¼Œæˆ‘ä»¬ç”¨ `rmsprop` ï¼ˆä¹¦é‡Œè¿˜æ²¡å†™ä¸ºä»€ä¹ˆï¼‰ï¼ŒæŒ‡æ ‡ä¹Ÿè¿˜æ˜¯å‡†ç¡®åº¦ï¼š


```python
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
```

å› ä¸ºè¿™å‡ ä¸ªoptimizerã€lossã€metrics éƒ½æ˜¯å¸¸ç”¨çš„ï¼Œæ‰€ä»¥ Keras å†…ç½®äº†ï¼Œå¯ä»¥ç›´æ¥ä¼ å­—ç¬¦ä¸²ã€‚
ä½†ä¹Ÿå¯ä»¥ä¼ ç±»å®ä¾‹æ¥å®šåˆ¶ä¸€äº›å‚æ•°ï¼š


```python
from tensorflow.keras import optimizers
from tensorflow.keras import losses
from tensorflow.keras import metrics

model.compile(optimizer=optimizers.RMSprop(lr=0.001),
              loss=losses.binary_crossentropy,
              metrics=[metrics.binary_accuracy])
```

### è®­ç»ƒæ¨¡å‹

ä¸ºäº†åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­éªŒè¯æ¨¡å‹åœ¨å®ƒæ²¡è§è¿‡çš„æ•°æ®ä¸Šç²¾åº¦å¦‚ä½•ï¼Œæˆ‘ä»¬ä»åŸæ¥çš„è®­ç»ƒæ•°æ®é‡Œåˆ† 10,000 ä¸ªæ ·æœ¬å‡ºæ¥ï¼š


```python
x_val = x_train[:10000]
partial_x_train = x_train[10000:]

y_val = y_train[:10000]
partial_y_train = y_train[10000:]
```

ç”¨ä¸€æ‰¹ 512 ä¸ªæ ·æœ¬çš„ mini-batchesï¼Œè·‘ 20 è½®ï¼ˆæ‰€æœ‰x_trainé‡Œçš„æ•°æ®è·‘ä¸€éç®—ä¸€è½®ï¼‰ï¼Œ
å¹¶ç”¨åˆšåˆ†å‡ºæ¥çš„ 10,000 çš„æ ·æœ¬åšç²¾åº¦éªŒè¯ï¼š


```python
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['acc'])

history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

    Train on 15000 samples, validate on 10000 samples
    Epoch 1/20
    15000/15000 [==============================] - 3s 205us/sample - loss: 0.5340 - acc: 0.7867 - val_loss: 0.4386 - val_acc: 0.8340
    .......
    Epoch 20/20
    15000/15000 [==============================] - 1s 74us/sample - loss: 0.0053 - acc: 0.9995 - val_loss: 0.7030 - val_acc: 0.8675


`fit` é˜”ä»¥è¿”å› historyï¼Œé‡Œé¢ä¿å­˜äº†è®­ç»ƒè¿‡ç¨‹é‡Œæ¯ä¸ª Epoch çš„é»‘å†å²ï¼š


```python
history_dict = history.history
history_dict.keys()
```

è¾“å‡ºï¼š


    dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])



æˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›ä¸œè¥¿ç”»å›¾å‡ºæ¥çœ‹ï¼š


```python
# ç”»è®­ç»ƒå’ŒéªŒè¯çš„æŸå¤±

import matplotlib.pyplot as plt

history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs = range(1, len(loss_values) + 1)

plt.plot(epochs, loss_values, 'ro-', label='Training loss')
plt.plot(epochs, val_loss_values, 'bs-', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjeapcizwj30at07qjri.jpg)



```python
# ç”»è®­ç»ƒå’ŒéªŒè¯çš„å‡†ç¡®åº¦

plt.clf()

acc = history_dict['acc']
val_acc = history_dict['val_acc']

plt.plot(epochs, acc, 'ro-', label='Training acc')
plt.plot(epochs, val_acc, 'bs-', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjeasgbdsj30az07q3ym.jpg)


æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè®­ç»ƒé›†ä¸Šçš„ç²¾åº¦å€’æ˜¯ä¸€ç›´åœ¨å¢ï¼ˆæŸå¤±ä¸€ç›´å‡ï¼‰ï¼Œ
ä½†åœ¨éªŒè¯é›†ä¸Šï¼Œåˆ°äº†åé¢æŸå¤±åè€Œå¤§äº†ï¼Œå·®ä¸å¤šç¬¬4è½®å·¦å³å°±åˆ°æœ€å¥½çš„å³°å€¼äº†ã€‚

è¿™å°±æ˜¯è¿‡æ‹Ÿåˆäº†ï¼Œå…¶å®ä»ç¬¬äºŒè½®å¼€å§‹å°±å¼€å§‹è¿‡äº†ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å…¶å®è·‘ä¸ª3ã€4è½®å°± ok äº†ã€‚
è¦æ˜¯å†è·‘ä¸‹å»ï¼Œå’±çš„æ¨¡å‹å°±åªâ€œç²¾é€šâ€è®­ç»ƒé›†ï¼Œä¸è®¤è¯†å…¶ä»–æ²¡è§è¿‡çš„æ•°æ®äº†ã€‚

æ‰€ä»¥ï¼Œæˆ‘ä»¬é‡æ–°è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼ˆè¦ä»å»ºç«‹ç½‘ç»œå¼€å§‹é‡å†™ï¼Œä¸ç„¶fitæ˜¯æ¥ç€åˆšæ‰å·²ç»è¿›è¡Œè¿‡çš„è¿™äº›ï¼‰ï¼Œé‚£å»ç”¨æµ‹è¯•é›†æµ‹è¯•ï¼š


```python
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['accuracy'])

model.fit(x_train, y_train, epochs=4, batch_size=512)
result = model.evaluate(x_test, y_test, verbose=2)    # verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286
```

    Train on 25000 samples
    Epoch 1/4
    25000/25000 [==============================] - 2s 69us/sample - loss: 0.4829 - accuracy: 0.8179
    Epoch 2/4
    25000/25000 [==============================] - 1s 42us/sample - loss: 0.2827 - accuracy: 0.9054
    Epoch 3/4
    25000/25000 [==============================] - 1s 42us/sample - loss: 0.2109 - accuracy: 0.9253
    Epoch 4/4
    25000/25000 [==============================] - 1s 43us/sample - loss: 0.1750 - accuracy: 0.9380
    25000/1 - 3s - loss: 0.2819 - accuracy: 0.8836

æŠŠç»“æœè¾“å‡ºå‡ºæ¥çœ‹çœ‹ï¼š

```python
print(result)
```


    [0.2923990402317047, 0.8836]



è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å½“ç„¶æƒ³å®é™…è¯•ä¸€ä¸‹ï¼Œå¯¹å§ã€‚æ‰€ä»¥æˆ‘ä»¬é¢„æµ‹ä¸€ä¸‹æµ‹è¯•é›†ï¼ŒæŠŠç»“æœæ‰“å‡ºæ¥çœ‹çœ‹ï¼š


```python
model.predict(x_test)
```

Output:


    array([[0.17157233],
           [0.99989915],
           [0.79564804],
           ...,
           [0.11750051],
           [0.05890778],
           [0.5040823 ]], dtype=float32)



### è¿›ä¸€æ­¥å®éªŒ

1. å°è¯•åªç”¨ä¸€ä¸ªå±‚


```python
model = models.Sequential()
# model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))
# model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid', input_shape=(10000, )))

model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['accuracy'])

model.fit(x_train, y_train, epochs=4, batch_size=512)
result = model.evaluate(x_test, y_test, verbose=2)    # verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286
print(result)
```

    Train on 25000 samples
    Epoch 1/4
    25000/25000 [==============================] - 3s 116us/sample - loss: 0.5865 - accuracy: 0.7814
    Epoch 2/4
    25000/25000 [==============================] - 1s 31us/sample - loss: 0.4669 - accuracy: 0.8608
    Epoch 3/4
    25000/25000 [==============================] - 1s 32us/sample - loss: 0.3991 - accuracy: 0.8790
    Epoch 4/4
    25000/25000 [==============================] - 1s 33us/sample - loss: 0.3538 - accuracy: 0.8920
    25000/1 - 3s - loss: 0.3794 - accuracy: 0.8732
    [0.3726908649635315, 0.8732]


...è¿™é—®é¢˜æ¯”è¾ƒç®€å•ï¼Œä¸€ä¸ªå±‚æ•ˆæœéƒ½è¿™ä¹ˆå¥½ï¼Œä½†æ¯”ä¹‹å‰æ­£ç»çš„å·®è¿œäº†

2. å¤šæå‡ ä¸ªå±‚


```python
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['accuracy'])

model.fit(x_train, y_train, epochs=4, batch_size=512)
result = model.evaluate(x_test, y_test, verbose=2)    # verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286
print(result)
```

    Train on 25000 samples
    Epoch 1/4
    25000/25000 [==============================] - 3s 123us/sample - loss: 0.5285 - accuracy: 0.7614
    Epoch 2/4
    25000/25000 [==============================] - 1s 45us/sample - loss: 0.2683 - accuracy: 0.9072s - loss:
    Epoch 3/4
    25000/25000 [==============================] - 1s 45us/sample - loss: 0.1949 - accuracy: 0.9297
    Epoch 4/4
    25000/25000 [==============================] - 1s 47us/sample - loss: 0.1625 - accuracy: 0.9422
    25000/1 - 2s - loss: 0.3130 - accuracy: 0.8806
    [0.30894253887176515, 0.88056]


å¥½ä¸€ç‚¹ï¼Œä½†ä¹Ÿè¿˜ä¸å¦‚æ­£ç»çš„ç‰ˆæœ¬

3. å¤šå‡ ä¸ªéšè—å±‚çš„å•å…ƒ


```python
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))
model.add(layers.Dense(1024, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['accuracy'])

model.fit(x_train, y_train, epochs=4, batch_size=512)
result = model.evaluate(x_test, y_test, verbose=2)    # verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286
print(result)
```

    Train on 25000 samples
    Epoch 1/4
    25000/25000 [==============================] - 15s 593us/sample - loss: 0.5297 - accuracy: 0.7964
    Epoch 2/4
    25000/25000 [==============================] - 12s 490us/sample - loss: 0.2233 - accuracy: 0.9109
    Epoch 3/4
    25000/25000 [==============================] - 12s 489us/sample - loss: 0.1148 - accuracy: 0.9593
    Epoch 4/4
    25000/25000 [==============================] - 12s 494us/sample - loss: 0.0578 - accuracy: 0.9835
    25000/1 - 9s - loss: 0.3693 - accuracy: 0.8812
    [0.4772889766550064, 0.8812]


ä¸æ˜¯è¿œå¤šè¶Šå¥½å‘€ã€‚

4. ç”¨ mse æŸå¤±


```python
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
             loss='mse',
             metrics=['accuracy'])

model.fit(x_train, y_train, epochs=4, batch_size=512)
result = model.evaluate(x_test, y_test, verbose=2)    # verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286
print(result)
```

    Train on 25000 samples
    Epoch 1/4
    25000/25000 [==============================] - 3s 119us/sample - loss: 0.1472 - accuracy: 0.8188
    Epoch 2/4
    25000/25000 [==============================] - 1s 46us/sample - loss: 0.0755 - accuracy: 0.9121
    Epoch 3/4
    25000/25000 [==============================] - 1s 50us/sample - loss: 0.0577 - accuracy: 0.9319
    Epoch 4/4
    25000/25000 [==============================] - 1s 47us/sample - loss: 0.0474 - accuracy: 0.9442
    25000/1 - 3s - loss: 0.0914 - accuracy: 0.8828
    [0.08648386991858482, 0.88276]


5. ç”¨ tanh æ¿€æ´»


```python
model = models.Sequential()
model.add(layers.Dense(16, activation='tanh', input_shape=(10000, )))
model.add(layers.Dense(16, activation='tanh'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['accuracy'])

model.fit(x_train, y_train, epochs=4, batch_size=512)
result = model.evaluate(x_test, y_test, verbose=2)    # verbose=2 to avoid a looooong progress bar that fills the screen with '='. https://github.com/tensorflow/tensorflow/issues/32286
print(result)
```

    Train on 25000 samples
    Epoch 1/4
    25000/25000 [==============================] - 4s 149us/sample - loss: 0.4237 - accuracy: 0.8241
    Epoch 2/4
    25000/25000 [==============================] - 1s 46us/sample - loss: 0.2310 - accuracy: 0.9163
    Epoch 3/4
    25000/25000 [==============================] - 1s 46us/sample - loss: 0.1779 - accuracy: 0.9329
    Epoch 4/4
    25000/25000 [==============================] - 1s 49us/sample - loss: 0.1499 - accuracy: 0.9458
    25000/1 - 3s - loss: 0.3738 - accuracy: 0.8772
    [0.3238203083658218, 0.87716]


æ‰€ä»¥ï¼Œè¿™äº›å®éªŒå°±æ˜¯è¯´ï¼Œä¹‹å‰æˆ‘ä»¬ä¹¦ä¸Šç”¨çš„æ¨¡å‹æ˜¯åˆç†çš„ï¼Œä½ æ”¹æ¥æ”¹å»éƒ½ä¸å¦‚ä»–é‚£ä¸ªğŸ˜‚ã€‚


## æ–°é—»åˆ†ç±»: å¤šåˆ†ç±»é—®é¢˜

[åŸæ–‡é“¾æ¥](https://livebook.manning.com/book/deep-learning-with-python/chapter-3/192)

åˆšæ‰æˆ‘ä»¬ç”µå½±è¯„è®ºé—®é¢˜ä¸æ˜¯æŠŠå‘é‡è¾“å…¥åˆ†æˆä¸¤ç±»å˜›ï¼Œè¿™èŠ‚æˆ‘ä»¬è¦æŠŠä¸œè¥¿åˆ†æˆå¤šç±»ï¼Œå³åšâ€œå¤šåˆ†ç±»ï¼ˆmulti-class classificationï¼‰â€ã€‚

æˆ‘ä»¬è¦æŠŠæ¥è‡ªè·¯é€ç¤¾çš„æ–°é—»åˆ†åˆ° 46 ä¸ªè¯é¢˜ç§ç±»é‡Œï¼Œè¿™é‡Œè¦æ±‚ä¸€æ¡æ–°é—»åªèƒ½å±äºä¸€ä¸ªç±»ï¼Œæ‰€ä»¥å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯ä¸€ä¸ªâ€œå•æ ‡ç­¾å¤šåˆ†ç±»ï¼ˆsingle-label, multiclass classificationï¼‰â€é—®é¢˜ã€‚

### è·¯é€ç¤¾æ•°æ®é›†

the Reuters datasetï¼Œè·¯é€ç¤¾åœ¨ 1986 å¹´ï¼ˆæ¯”æˆ‘è€å¤šäº†ğŸ˜‚ï¼‰å‘å¸ƒçš„æ•°æ®é›†ï¼Œé‡Œé¢æœ‰ 46 ç±»æ–°é—»ï¼Œè®­ç»ƒé›†é‡Œæ¯ç±»è‡³å°‘ 10 æ¡æ•°æ®ã€‚

è¿™ä¸ªç©å…·æ•°æ®é›†å’Œ IMDBã€MNIST ä¸€æ ·ï¼Œä¹Ÿåœ¨  Keras é‡Œå†…ç½®äº†ï¼š


```python
from tensorflow.keras.datasets import reuters

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(
    num_words=10000)
```

    Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz
    2113536/2110848 [==============================] - 6s 3us/step


è¿™ä¸ªæ•°æ®é›†é‡Œé¢çš„æ•°æ®å’Œä¹‹å‰çš„ IMDB ä¸€æ ·ï¼ŒæŠŠå•è¯ç¿»è¯‘æˆäº†æ•°å­—ï¼Œç„¶åæˆ‘ä»¬åªæˆªå–å‡ºç°é¢‘ç‡æœ€é«˜çš„10000ä¸ªè¯ã€‚

å’±ä»¬è¿™ä¸ªè®­ç»ƒé›†é‡Œæœ‰ 8K+ æ¡æ•°æ®ï¼Œæµ‹è¯•é›† 2K+ï¼š


```python
print(len(train_data), len(test_data))
```

    8982 2246


å’±ä»¬è¿˜æ˜¯åƒæ IMDB æ—¶é‚£æ ·ï¼ŒæŠŠæ•°æ®è¿˜åŸä¼šæ–‡æœ¬çœ‹çœ‹ï¼š


```python
def decode_news(data):
    reverse_word_index = {v: k for k, v in reuters.get_word_index().items()}
    return ' '.join([reverse_word_index.get(i - 3, '?') for i in data])
    # i - 3 æ˜¯å› ä¸º 0ã€1ã€2 ä¸ºä¿ç•™è¯ â€œpaddingâ€(å¡«å……)ã€â€œstart of sequenceâ€(åºåˆ—å¼€å§‹)ã€â€œunknownâ€(æœªçŸ¥è¯)


text = decode_news(train_data[0])
print(text)
```

    ? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3


æ ‡ç­¾æ˜¯ 0~45 çš„æ•°å­—ï¼š


```python
train_labels[0]
```

Output:


    3



### æ•°æ®å‡†å¤‡

é¦–å…ˆï¼Œè¿˜æ˜¯æŠŠæ•°æ®ä½å‘é‡åŒ–ï¼Œç›´æ¥å¥—ç”¨æˆ‘ä»¬æ IMDB æ—¶å†™çš„ä»£ç ï¼š


```python
import numpy as np

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results


x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
```

ç„¶åå°±æ˜¯è¿™ç§æ•ˆæœï¼š


```python
print(x_train)
```


    array([[0., 1., 1., ..., 0., 0., 0.],
           [0., 1., 1., ..., 0., 0., 0.],
           [0., 1., 1., ..., 0., 0., 0.],
           ...,
           [0., 1., 1., ..., 0., 0., 0.],
           [0., 1., 1., ..., 0., 0., 0.],
           [0., 1., 1., ..., 0., 0., 0.]])



ç„¶åè¦å¤„ç†æ ‡ç­¾ã€‚æˆ‘ä»¬å¯ä»¥æŠŠæ ‡ç­¾å¤„ç†æˆæ•´æ•°å¼ é‡ï¼Œä¹Ÿå¯ä»¥ç”¨ `One-hot` ç¼–ç 
å¯¹äºåˆ†ç±»è¿™ç§é—®é¢˜ï¼Œæˆ‘ä»¬å¸¸ç”¨ one-hot ç¼–ç ï¼ˆä¹Ÿå«*åˆ†ç±»ç¼–ç *ï¼Œcategorical encodingï¼‰ã€‚

å¯¹äºæˆ‘ä»¬å½“å‰çš„é—®é¢˜ï¼Œä½¿ç”¨ one-hot ç¼–ç ï¼Œå³ç”¨é™¤äº†æ ‡ç­¾ç´¢å¼•ä½ç½®ä¸º 1 å…¶ä½™ä½ç½®å…¨ä¸º 0 çš„å‘é‡ï¼š


```python
def to_one_hot(labels, dimension=46):
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1.
    return results


one_hot_train_labels = to_one_hot(train_labels)
one_hot_test_labels = to_one_hot(test_labels)
```

å…¶å®ï¼Œï¼Œï¼ŒKeras é‡Œè‡ªå¸¦äº†ä¸€ä¸ªå¯ä»¥å¹²è¿™ä¸ªäº‹æƒ…çš„å‡½æ•°ï¼š


```python
from tensorflow.keras.utils import to_categorical
# ä¹¦ä¸Šæ˜¯ from keras.utils.np_utils import to_categorical ä½†ï¼Œï¼Œï¼Œæ—¶ä»£å˜äº†ï¼Œè€Œä¸”å’±è¿™ç”¨çš„æ˜¯ tensorflow.kerasï¼Œæ‰€ä»¥ç¨å¾®æœ‰ç‚¹åŒºåˆ«

one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)
```


```python
print(one_hot_train_labels)
```


    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           ...,
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)



### æ„å»ºç½‘ç»œ

è¿™ä¸ªé—®é¢˜å’Œä¹‹å‰çš„ç”µå½±è¯„è®ºåˆ†ç±»é—®é¢˜è¿˜æ˜¯å·®ä¸å¤šçš„ï¼Œåªæ˜¯æœ€åçš„è§£çš„å¯èƒ½ä» 2 -> 46ï¼Œè§£ç©ºé—´å¤§äº†å¤ªå¤šäº†ã€‚

å¯¹äºæˆ‘ä»¬ç”¨çš„ Dense å±‚å †å ï¼Œæ¯å±‚éƒ½æ˜¯æ¥æ”¶ä¸Šä¸€å±‚è¾“å‡ºçš„ä¿¡æ¯ä½œä¸ºè¾“å…¥ã€‚
æ‰€ä»¥ï¼Œå¦‚æœæŸä¸€å±‚ä¸¢å¤±äº†ä¸€äº›ä¿¡æ¯ï¼Œé‚£ä¹ˆè¿™äº›ä¿¡æ¯å°±å†ä¹Ÿä¸èƒ½è¢«åé¢çš„å±‚æ‰¾å›æ¥äº†ã€‚
å¦‚æœä¸¢å¤±çš„ä¿¡æ¯å¯¹åˆ†ç±»æ²¡ç”¨ï¼Œé‚£è¿™ç§ä¸¢å¤±æ˜¯å¥½çš„ã€æˆ‘ä»¬æœŸæœ›å‘ç”Ÿçš„ï¼›
ä½†å¦‚æœè¿™äº›ä¸¢å¤±çš„ä¿¡æ¯æ˜¯å¯¹æœ€ååˆ†ç±»èµ·ä½œç”¨çš„ï¼Œé‚£è¿™ç§ä¸¢å¤±å°±åˆ¶çº¦ç½‘ç»œçš„ç»“æœäº†ã€‚
ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™å¯èƒ½é€ æˆä¸€ç§â€œä¿¡æ¯ç“¶é¢ˆâ€ã€‚è¿™ç§ç“¶é¢ˆåœ¨æ¯ä¸€å±‚éƒ½å¯èƒ½å‘ç”Ÿã€‚

ä¹‹å‰çš„ç”µå½±è¯„è®ºåˆ†ç±»æœ€ååªè¦ 2 ä¸ªç»“æœï¼Œæ‰€ä»¥æˆ‘ä»¬æŠŠå±‚é‡Œçš„å•å…ƒæ˜¯ç”¨äº† 16 ä¸ªï¼Œ
å³è®©æœºå™¨åœ¨ä¸€ä¸ª 16 ç»´ç©ºé—´é‡Œå­¦ä¹ ï¼Œä»¥åŠè¶³å¤Ÿå¤§äº†ï¼Œä¸å¤ªä¼šæœ‰â€œä¿¡æ¯ç“¶é¢ˆâ€œã€‚

è€Œæˆ‘ä»¬ç°åœ¨çš„é—®é¢˜ï¼Œè§£ç©ºé—´æ˜¯ 46 ç»´çš„ã€‚
ç›´æ¥ç…§æ¬ä¹‹å‰çš„ä»£ç ï¼Œè®©å®ƒåœ¨ 16 ç»´ç©ºé—´é‡Œå­¦ä¹ ï¼Œè‚¯å®šæœ‰ç“¶é¢ˆï¼

è§£å†³ç“¶é¢ˆçš„åŠæ³•å¾ˆç®€å•ï¼Œç›´æ¥å¢åŠ å±‚é‡Œçš„å•å…ƒå°±å¥½ã€‚è¿™é‡Œæˆ‘ä»¬æ˜¯ 16 -> 64:


```python
from tensorflow.keras import models
from tensorflow.keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))
```

åœ¨æœ€åä¸€å±‚ï¼Œæˆ‘ä»¬çš„è¾“å‡ºæ˜¯ 46 ç»´çš„ï¼Œå¯¹åº” 46 ç§åˆ†ç±»ï¼Œ
è€Œè¿™ä¸€å±‚çš„æ¿€æ´»å‡½æ•°æ˜¯ softmaxï¼Œå’Œæˆ‘ä»¬åœ¨è®­ç»ƒ MNIST æ—¶ç”¨çš„ä¸€æ ·ã€‚

ç”¨ softmax å¯ä»¥è®©ç½‘ç»œè¾“å‡ºåœ¨ 46 ç§åˆ†ç±»ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œå³ä¸€ä¸ª 46 ç»´çš„å‘é‡ï¼Œ
å…¶ä¸­ç¬¬ i ä¸ªå…ƒç´ ä»£è¡¨è¾“å…¥å±äºç¬¬ i ç§åˆ†ç±»çš„å¯èƒ½æ€§ï¼Œ
å¹¶ä¸”è¿™ 46 ä¸ªå…ƒç´ çš„æ€»å’Œä¸º `1`ã€‚

### ç¼–è¯‘æ¨¡å‹

ç¼–è¯‘æ¨¡å‹ï¼Œåˆè¦ç¡®å®šæŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œä¼˜åŒ–çš„ç›®æ ‡äº†ã€‚

- æŸå¤±å‡½æ•°ï¼Œåˆ†ç±»é—®é¢˜å˜›ï¼Œè¿˜æ˜¯ç”¨â€œåˆ†ç±»äº¤å‰ç†µâ€ categorical_crossentropyã€‚
- ä¼˜åŒ–å™¨ï¼Œå…¶å®å¯¹å¾ˆå¤šé—®é¢˜æˆ‘ä»¬éƒ½æ˜¯ç”¨ rmsprop
- ç›®æ ‡è¿˜æ˜¯ä¸€ä¸ªï¼Œé¢„æµ‹çš„ç²¾åº¦ accuracy


```python
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
```

### éªŒè¯æ•ˆæœ

æˆ‘ä»¬è¿˜æ˜¯è¦æä¸€ä¸ªéªŒè¯é›†æ¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°æ¨¡å‹çš„ã€‚ä»è®­ç»ƒé›†é‡Œåˆ†ä¸ª 1K æ¡æ•°æ®å‡ºæ¥å°±å¥½ï¼š


```python
x_val = x_train[:1000]
partial_x_train = x_train[1000:]

y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]
```

### è®­ç»ƒæ¨¡å‹

å¥½äº†ï¼Œå‡†å¤‡å·¥ä½œå®Œæˆï¼Œåˆå¯ä»¥çœ‹åˆ°æœ€è¿·äººçš„è®­ç»ƒè¿‡ç¨‹äº†ï¼


```python
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
```

    Train on 7982 samples, validate on 1000 samples
    Epoch 1/20
    7982/7982 [==============================] - 3s 372us/sample - loss: 2.6180 - accuracy: 0.5150 - val_loss: 1.7517 - val_accuracy: 0.6290
    ......
    Epoch 20/20
    7982/7982 [==============================] - 1s 91us/sample - loss: 0.1134 - accuracy: 0.9578 - val_loss: 1.0900 - val_accuracy: 0.8040


ğŸ†—æŒºå¿«çš„ï¼Œç…§ä¾‹ï¼Œè¿˜æ˜¯ç”»å›¾çœ‹çœ‹è®­ç»ƒè¿‡ç¨‹ã€‚

1. è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±


```python
import matplotlib.pyplot as plt

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'rs-', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjed34amjj30at07qdfx.jpg)


2. è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç²¾åº¦


```python
plt.clf()

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

plt.plot(epochs, acc, 'bo-', label='Training acc')
plt.plot(epochs, val_acc, 'rs-', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjed4tgtzj30at07qglp.jpg)


Emmmmï¼Œè¯´ï¼Œç¬¬9è½® epochs çš„æ—¶å€™å¼€å§‹è¿‡æ‹Ÿåˆäº†ï¼ˆä½ çœ‹validationçš„æ›²çº¿æŠ–åœ¨ç¬¬9è½®äº†ä¸€ä¸‹ï¼‰ã€‚
æ‰€ä»¥åªè¦è·‘ 9 è½®å°±å¤Ÿäº†ã€‚


```python
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(partial_x_train,
          partial_y_train,
          epochs=9,
          batch_size=512,
          validation_data=(x_val, y_val))
```

    Train on 7982 samples, validate on 1000 samples
    Epoch 1/9
    7982/7982 [==============================] - 1s 153us/sample - loss: 2.5943 - accuracy: 0.5515 - val_loss: 1.7017 - val_accuracy: 0.6410
    ......
    Epoch 9/9
    7982/7982 [==============================] - 1s 84us/sample - loss: 0.2793 - accuracy: 0.9402 - val_loss: 0.8758 - val_accuracy: 0.8170
    
    <tensorflow.python.keras.callbacks.History at 0x16eb5d6d0>

ç„¶åï¼Œç”¨æµ‹è¯•é›†æµ‹è¯•ä¸€ä¸‹:


```python
results = model.evaluate(x_test, one_hot_test_labels, verbose=2)
print(results)
```

    2246/1 - 0s - loss: 1.7611 - accuracy: 0.7912
    [0.983459981976082, 0.7911843]


ç²¾åº¦å·®ä¸å¤š 80%ï¼Œå…¶å®è¿˜æ˜¯ä¸é”™çš„äº†ï¼Œæ¯”éšæœºçåˆ’çº¿å»åˆ†å¥½å¤šäº†ã€‚

å¦‚æœéšæœºåˆ’çº¿å»åˆ†ç±»çš„è¯ï¼Œå¯¹äºŒå…ƒåˆ†ç±»é—®é¢˜ç²¾åº¦æ˜¯ 50 %ï¼Œè€Œå¯¹è¿™ 46 å…ƒçš„åˆ†ç±»ç²¾åº¦åªè¦ä¸åˆ° 19% äº†ï¼š


```python
import copy

test_labels_copy = copy.copy(test_labels)
np.random.shuffle(test_labels_copy)
hits_array = np.array(test_labels) == np.array(test_labels_copy)
float(np.sum(hits_array)) / len(test_labels)
```


    0.18432769367764915



è°ƒç”¨ model å®ä¾‹çš„ predict æ–¹æ³•ï¼Œå¯ä»¥å¾—åˆ°å¯¹è¾“å…¥åœ¨ 46 ä¸ªåˆ†ç±»ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼š


```python
predictions = model.predict(x_test)
print(predictions)
```


    array([[4.7181980e-05, 2.0765587e-05, 8.6653872e-06, ..., 3.1266565e-05,
            8.2046267e-07, 6.0611728e-06],
           [5.9005950e-04, 1.3404934e-02, 1.2290048e-03, ..., 4.2919168e-05,
            5.7422225e-05, 4.0201416e-05],
           [8.5751421e-04, 9.2367262e-01, 1.5855590e-03, ..., 4.8341672e-04,
            4.5594123e-05, 2.6183401e-05],
           ...,
           [8.5679676e-05, 2.0081598e-04, 4.1808224e-05, ..., 7.6962686e-05,
            6.5783697e-06, 2.9889508e-05],
           [1.7291466e-03, 2.5600385e-02, 1.8182390e-03, ..., 1.4499390e-03,
            4.8478998e-04, 8.5257640e-04],
           [2.5776261e-04, 8.6797208e-01, 3.9900807e-03, ..., 2.6547859e-04,
            6.5820634e-05, 6.8603881e-06]], dtype=float32)

predictions åˆ†åˆ«ä»£è¡¨ 46 ä¸ªåˆ†ç±»çš„å¯èƒ½:


```python
predictions[0].shape
```


    (46,)

ä»–ä»¬çš„æ€»å’Œä¸º 1ï¼š


```python
np.sum(predictions[0])
```


    0.99999994

 å…¶ä¸­æœ€å¤§çš„ï¼Œå³æˆ‘ä»¬è®¤ä¸ºè¿™æ¡æ–°é—»å±äºè¿™ä¸ªåˆ†ç±»


```python
np.argmax(predictions[0])
```


    3



### å¤„ç†æ ‡ç­¾å’ŒæŸå¤±çš„å¦ä¸€ç§æ–¹æ³•

å‰é¢æåˆ°äº†æ ‡ç­¾å¯ä»¥ä½¿ç”¨ one-hot ç¼–ç ï¼Œæˆ–è€…ç›´æ¥æŠŠæ ‡ç­¾å¤„ç†æˆæ•´æ•°å¼ é‡ï¼š


```python
y_train = np.array(train_labels)
y_test = np.array(test_labels)
```

ç”¨è¿™ç§çš„è¯ï¼ŒæŸå¤±å‡½æ•°ä¹Ÿè¦è·Ÿç€æ”¹ï¼Œæ”¹æˆ sparse_categorical_crossentropyï¼Œ
è¿™ä¸ªå’Œ categorical_crossentropy åœ¨æ•°å­¦ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯æ¥å£ä¸åŒï¼š


```python
 model.compile(optimizer='rmsprop',
               loss='sparse_categorical_crossentropy',
               metrics=['acc'])
```

### ä¸­é—´å±‚ç»´åº¦è¶³å¤Ÿå¤§çš„é‡è¦æ€§

ä¹‹å‰è®¨è®ºäº†å…³äºâ€œä¿¡æ¯ç“¶é¢ˆâ€çš„äº‹ï¼Œç„¶åæˆ‘ä»¬å°±è¯´å¯¹è¿™ä¸ª 46 ç»´ç»“æœçš„ç½‘ç»œï¼Œä¸­é—´å±‚çš„ç»´åº¦è¦è¶³å¤Ÿå¤§ï¼

ç°åœ¨å’±è¯•è¯•å¦‚æœä¸å¤Ÿå¤§ï¼ˆå¯¼è‡´ä¿¡æ¯ç“¶é¢ˆï¼‰ä¼šæ€ä¹ˆæ ·ï¼Œå’±æå¤¸å¼ ä¸€ç‚¹ï¼Œä» 64 å‡åˆ° 4ï¼š


```python
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(4, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(partial_x_train,
          partial_y_train,
          epochs=20,
          batch_size=128,
          validation_data=(x_val, y_val))
```

    Train on 7982 samples, validate on 1000 samples
    Epoch 1/20
    7982/7982 [==============================] - 2s 288us/sample - loss: 2.8097 - accuracy: 0.4721 - val_loss: 2.0554 - val_accuracy: 0.5430
    .......
    Epoch 20/20
    7982/7982 [==============================] - 1s 121us/sample - loss: 0.6443 - accuracy: 0.8069 - val_loss: 1.8962 - val_accuracy: 0.6800
    
    <tensorflow.python.keras.callbacks.History at 0x16f628b50>

çœ‹çœ‹è¿™ï¼Œè¿™è®­ç»ƒå‡ºæ¥æ¯”ä¹‹å‰ 64 ç»´çš„å·®çš„ä¸æ˜¯ä¸€ç‚¹åŠç‚¹å“ˆï¼Œå·®è·ç›¸å½“æ˜æ˜¾äº†ã€‚

å‘ç”Ÿè¿™ç§æ•ˆæœçš„ä¸‹é™å°±æ˜¯å› ä¸ºä½ ç»™ä»–å­¦ä¹ çš„ç©ºé—´ç»´åº¦å¤ªä½äº†ï¼Œä»–æŠŠå¥½å¤šå¯¹åˆ†ç±»æœ‰ç”¨çš„ä¿¡æ¯æŠ›å¼ƒäº†ã€‚

é‚£æ˜¯ä¸æ˜¯è¶Šå¤§è¶Šå¥½ï¼Ÿæˆ‘ä»¬å†è¯•è¯•æŠŠä¸­é—´å±‚åŠ å¤§ä¸€äº›ï¼š


```python
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(4096, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(partial_x_train,
          partial_y_train,
          epochs=20,
          batch_size=128,
          validation_data=(x_val, y_val))
```

    Train on 7982 samples, validate on 1000 samples
    Epoch 1/20
    7982/7982 [==============================] - 2s 273us/sample - loss: 1.5523 - accuracy: 0.6310 - val_loss: 1.1903 - val_accuracy: 0.7060
    ......
    Epoch 20/20
    7982/7982 [==============================] - 2s 296us/sample - loss: 0.0697 - accuracy: 0.9605 - val_loss: 3.5296 - val_accuracy: 0.7850
    
    <tensorflow.python.keras.callbacks.History at 0x1707fcf90>



å¯ä»¥çœ‹åˆ°è®­ç»ƒç”¨çš„æ—¶é—´é•¿äº†ä¸€ç‚¹ï¼Œç”µè„‘æ›´æš–æ‰‹äº†ä¸€ç‚¹ï¼Œä½†æ•ˆæœå´æ²¡æœ‰å¤šå¤§çš„æå‡ã€‚
è¿™æ˜¯ç”±äºç¬¬ä¸€å±‚è¾“å…¥åˆ°ä¸­é—´å±‚çš„åªæœ‰ 64 ç»´å˜›ï¼Œä¸­é—´å±‚å†å¤§ï¼Œä¹Ÿè¢«ç¬¬ä¸€å±‚çš„ç“¶é¢ˆåˆ¶çº¦äº†ã€‚

åœ¨è¯•è¯•æŠŠç¬¬ä¸€å±‚ä¹ŸåŠ å¤§ï¼


```python
model = models.Sequential()
model.add(layers.Dense(512, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(partial_x_train,
          partial_y_train,
          epochs=20,
          batch_size=128,
          validation_data=(x_val, y_val))
```

    Train on 7982 samples, validate on 1000 samples
    Epoch 1/20
    7982/7982 [==============================] - 5s 662us/sample - loss: 1.3423 - accuracy: 0.6913 - val_loss: 0.9565 - val_accuracy: 0.7920
    ......
    Epoch 20/20
    7982/7982 [==============================] - 5s 583us/sample - loss: 0.0648 - accuracy: 0.9597 - val_loss: 2.9887 - val_accuracy: 0.8030
    
    <tensorflow.python.keras.callbacks.History at 0x176fbbd90>

ï¼ˆç¨å¾®å°ä¸€ç‚¹ï¼Œæœ¬æ¥æ˜¯ç”¨ 4096 çš„ï¼Œä½†å¤ªå¤§äº†ï¼Œå’±ä¹ä¸ç‰ˆ mbp è·‘çš„è´¼æ…¢ï¼Œè·‘å®Œè¦20å¤šåˆ†é’Ÿï¼Œæˆ‘æ‡’å¾—ç­‰ï¼‰

è¿™ä¸ªå¤šæµªè´¹äº†å¥½å¤šæ—¶é—´ï¼Œè€Œä¸”ä»–å¾ˆå¿«å°±~~è¿‡æ³¥æ²³~~è¿‡æ‹Ÿåˆäº†ï¼Œè¿‡å¾—è¿˜è¿‡å¾—å¾ˆä¸¥é‡ï¼Œç”»ä¸ªå›¾çœ‹ä¸€ä¸‹ï¼š


```python
import matplotlib.pyplot as plt

loss = _.history['loss']
val_loss = _.history['val_loss']
epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'rs-', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjefsh6f1j30at07qwel.jpg)


æ‰€ä»¥ï¼Œå¤ªå¤§äº†ä¹Ÿä¸å¥½ã€‚è¿˜æ˜¯è¦æœ‰ä¸ªåº¦ï¼

### å°è¯•ä½¿ç”¨æ›´å°‘/æ›´å¤šçš„å±‚

1. æ›´å°‘çš„å±‚


```python
model = models.Sequential()
model.add(layers.Dense(46, activation='softmax', input_shape=(10000,)))

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(partial_x_train,
          partial_y_train,
          epochs=20,
          batch_size=128,
          validation_data=(x_val, y_val))

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'rs-', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

    Train on 7982 samples, validate on 1000 samples
    Epoch 1/20
    7982/7982 [==============================] - 1s 132us/sample - loss: 2.4611 - accuracy: 0.6001 - val_loss: 1.8556 - val_accuracy: 0.6440
    ......
    Epoch 20/20
    7982/7982 [==============================] - 1s 85us/sample - loss: 0.1485 - accuracy: 0.9570 - val_loss: 1.2116 - val_accuracy: 0.7960



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjeg0o13pj30at07qaa5.jpg)

å¿«å‘€ï¼ç»“æœç¨å¾®å·®äº†ä¸€ç‚¹ç‚¹ã€‚

```java
Clown0te("é˜²ç›—æ–‡çˆ¬:)è™«çš„è¿½è¸ªæ ‡ç­¾ï¼Œè¯»è€…ä¸å¿…åœ¨æ„").by(CDFMLR)
```

2. æ›´å¤šçš„å±‚


```python
model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(partial_x_train,
          partial_y_train,
          epochs=20,
          batch_size=128,
          validation_data=(x_val, y_val))

loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'bo-', label='Training loss')
plt.plot(epochs, val_loss, 'rs-', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

    Train on 7982 samples, validate on 1000 samples
    Epoch 1/20
    7982/7982 [==============================] - 2s 188us/sample - loss: 1.8340 - accuracy: 0.5829 - val_loss: 1.3336 - val_accuracy: 0.6910
    ......
    Epoch 20/20
    7982/7982 [==============================] - 1s 115us/sample - loss: 0.0891 - accuracy: 0.9600 - val_loss: 1.7227 - val_accuracy: 0.7900



![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjem9vu9zj30b007qmxh.jpg)


æ‰€ä»¥ï¼Œè¿™ä¸ªä¹Ÿä¸æ˜¯è¶Šå¤šè¶Šå¥½å‘€ï¼


## é¢„æµ‹æˆ¿ä»·: å›å½’é—®é¢˜

[åŸæ–‡é“¾æ¥](https://livebook.manning.com/book/deep-learning-with-python/chapter-3/271)

å‰é¢ä¸¤ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬éƒ½æ˜¯åœ¨åšåˆ†ç±»é—®é¢˜ï¼ˆé¢„æµ‹ç¦»æ•£çš„æ ‡ç­¾ï¼‰ã€‚è¿™æ¬¡çœ‹ä¸€ä¸ªå›å½’é—®é¢˜ï¼ˆé¢„æµ‹è¿ç»­çš„æ•°å€¼ï¼‰ã€‚

### æ³¢å£«é¡¿æˆ¿é—´æ•°æ®é›†

æˆ‘ä»¬ç”¨ Boston Housing Price dataset è¿™ä¸ªæ•°æ®é›†æ¥é¢„æµ‹ 70 å¹´ä»£ä¸­æœŸçš„æ³¢å£«é¡¿éƒŠåŒºæˆ¿ä»·ã€‚æ•°æ®é›†é‡Œæœ‰å½“æ—¶é‚£ä¸ªåœ°æ–¹çš„ä¸€äº›æ•°æ®ï¼Œæ¯”å¦‚çŠ¯ç½ªå¾‹ã€ç¨ç‡ä»€ä¹ˆçš„ã€‚

è¿™ä¸ªæ•°æ®é›†æ¯”èµ·æˆ‘ä»¬å‰ä¸¤ä¸ªåˆ†ç±»çš„æ•°æ®é›†ï¼Œæ•°æ®ç›¸å½“å°‘ï¼Œåªæœ‰ 506 ä¸ªï¼Œ404 ä¸ªåœ¨è®­ç»ƒé›†ï¼Œ102 éä¸ªåœ¨æµ‹è¯•é›†ã€‚æ•°æ®é‡Œæ¯ç§ç‰¹å¾ï¼ˆfeatureï¼‰çš„è¾“å…¥æ•°æ®æ•°é‡çº§ä¹Ÿä¸å°½ç›¸åŒã€‚

æˆ‘ä»¬å…ˆæŠŠæ•°æ®å¯¼è¿›æ¥çœ‹çœ‹ï¼ˆè¿™ä¸ªæ•°æ®é›†ä¹Ÿæ˜¯ Keras è‡ªå¸¦æœ‰çš„ï¼‰ï¼š


```python
from tensorflow.keras.datasets import boston_housing

(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()

print(train_data.shape, test_data.shape)
```

    (404, 13) (102, 13)



```python
train_data[0:3]
```


    array([[1.23247e+00, 0.00000e+00, 8.14000e+00, 0.00000e+00, 5.38000e-01,
            6.14200e+00, 9.17000e+01, 3.97690e+00, 4.00000e+00, 3.07000e+02,
            2.10000e+01, 3.96900e+02, 1.87200e+01],
           [2.17700e-02, 8.25000e+01, 2.03000e+00, 0.00000e+00, 4.15000e-01,
            7.61000e+00, 1.57000e+01, 6.27000e+00, 2.00000e+00, 3.48000e+02,
            1.47000e+01, 3.95380e+02, 3.11000e+00],
           [4.89822e+00, 0.00000e+00, 1.81000e+01, 0.00000e+00, 6.31000e-01,
            4.97000e+00, 1.00000e+02, 1.33250e+00, 2.40000e+01, 6.66000e+02,
            2.02000e+01, 3.75520e+02, 3.26000e+00]])




```python
train_targets[0:3]
```


    array([15.2, 42.3, 50. ])



targets é‡Œçš„æ•°æ®å•ä½æ˜¯ *åƒç¾å…ƒ*ï¼Œ è¿™ä¸ªæ—¶å€™çš„æˆ¿ä»·è¿˜æ¯”è¾ƒä¾¿å®œï¼š


```python
min(train_targets), sum(train_targets)/len(train_targets), max(train_targets)
```


    (5.0, 22.395049504950496, 50.0)




```python
import matplotlib.pyplot as plt

x = range(len(train_targets))
y = train_targets

plt.plot(x, y, 'o', label='data')
plt.title('House Prices')
plt.xlabel('train_targets')
plt.ylabel('prices')
plt.legend()
plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje0e4s1qj30aq07r3yo.jpg)


### æ•°æ®å‡†å¤‡

æˆ‘ä»¬å–‚ç»™ç¥ç»ç½‘ç»œçš„æ•°æ®å€¼çš„èŒƒå›´ä¸åº”è¯¥å·®è·å¤ªå¤§ï¼Œè™½ç„¶ç¥ç»ç½‘ç»œæ˜¯å¯ä»¥å¤„ç†å·®è·å¤§çš„æ•°æ®çš„ï¼Œä½†æ€»å½’ä¸å¤ªå¥½ã€‚
å¯¹äºè¿™ç§å·®è·å¤§çš„æ•°æ®ï¼Œæˆ‘ä»¬ä¸€èˆ¬éƒ½ä¼šå¯¹æ¯ä¸ªç‰¹å¾åšæ ‡å‡†åŒ–ï¼ˆfeature-wise normalizationï¼‰ã€‚

å…·ä½“çš„æ“ä½œæ˜¯å¯¹æ¯ä¸ªç‰¹å¾ï¼ˆè¾“å…¥æ•°æ®çŸ©é˜µçš„ä¸€åˆ—ï¼‰å‡å»è¯¥åˆ—çš„å‡å€¼ï¼Œå†é™¤ä»¥å…¶æ ‡å‡†å·®ã€‚
è¿™æ ·åšå®Œä¹‹åï¼Œæ•°æ®ä¼šå˜æˆä»¥ 0 ä¸ºä¸­å¿ƒçš„ï¼Œæœ‰ä¸€ä¸ªæ ‡å‡†å·®çš„ï¼ˆæ ‡å‡†å·®ä¸º 1 ï¼‰ã€‚

ç”¨ Numpy å¯ä»¥å¾ˆå®¹æ˜“çš„åšè¿™ä¸ªï¼š


```python
mean = train_data.mean(axis=0)
std = train_data.std(axis=0)

train_data -= mean
train_data /= std

test_data -= mean
test_data /= std
```

æ³¨æ„å“ˆï¼Œå¯¹æµ‹è¯•é›†çš„å¤„ç†ç”¨çš„æ˜¯è®­ç»ƒé›†çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚

å¤„ç†å®Œæ•°æ®å°±å¯ä»¥æ„å»ºç½‘ç»œã€è®­ç»ƒäº†ï¼ˆæ ‡ç­¾ä¸ç”¨å¤„ç†ï¼Œæ¯”åˆ†ç±»æ–¹ä¾¿ï¼‰

### æ„å»ºç½‘ç»œ

æ•°æ®è¶Šå°‘ï¼Œè¶Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè¦å‡ç¼“è¿‡æ‹Ÿåˆï¼Œå¯ä»¥ä½¿ç”¨æ¯”è¾ƒå°çš„ç½‘ç»œã€‚

æ¯”å¦‚ï¼Œåœ¨è¿™ä¸ªé—®é¢˜ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåªæœ‰ä¸¤ä¸ªéšè—å±‚çš„ç½‘ç»œï¼Œæ¯ä¸ª 64 å•å…ƒï¼š


```python
from tensorflow.keras import models
from tensorflow.keras import layers

def build_model():
    model = models.Sequential()
    model.add(layers.Dense(64, activation="relu", input_shape=(train_data.shape[1], )))
    model.add(layers.Dense(64, activation="relu"))
    model.add(layers.Dense(1))
    
    model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
    
    return model
```

ç½‘ç»œçš„æœ€åä¸€å±‚åªæœ‰ä¸€ä¸ªå•å…ƒï¼Œå¹¶ä¸”æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼ˆæ‰€ä»¥æ˜¯ä¸ªçº¿æ€§çš„å±‚ï¼‰ã€‚è¿™ç§å±‚æ˜¯æˆ‘ä»¬åšè¿ç»­å•å€¼å›å½’é—®é¢˜çš„æœ€åä¸€æ­¥çš„æ ‡é…ã€‚

å¦‚æœåŠ äº†æ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºçš„å€¼å°±ä¼šæœ‰èŒƒå›´é™åˆ¶ï¼Œæ¯”å¦‚ sigmoid ä¼šæŠŠå€¼é™åˆ¶åˆ° `[0, 1]`ã€‚æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œè¿™ä¸ªçº¿æ€§çš„å±‚è¾“å‡ºçš„å€¼å°±æ²¡æœ‰é™åˆ¶äº†ã€‚

æˆ‘ä»¬åœ¨ç¼–è¯‘æ¨¡å‹æ—¶ï¼Œä½¿ç”¨çš„æŸå¤±å‡½æ•°æ˜¯ `mse` ï¼ˆmean squared errorï¼Œå‡æ–¹è¯¯å·®ï¼‰ã€‚è¿™ä¸ªå‡½æ•°è¿”å›é¢„æµ‹å’ŒçœŸå®ç›®æ ‡å€¼çš„å·®çš„å¹³æ–¹ã€‚å›å½’é—®é¢˜å¤šç”¨è¿™ä¸ªæŸå¤±ã€‚

ç„¶åæˆ‘ä»¬è¿˜ç”¨äº†ä»¥å‰æ²¡ç”¨è¿‡çš„è®­ç»ƒæŒ‡æ ‡ â€”â€” `mae`ï¼ˆmean absolute errorï¼Œå¹³å‡ç»å¯¹è¯¯å·®ï¼‰ï¼Œè¿™ä¸ªä¸œè¥¿æ˜¯é¢„æµ‹å’ŒçœŸå®ç›®æ ‡çš„å·®çš„ç»å¯¹å€¼ã€‚

### æ‹ŸåˆéªŒè¯ â€”â€” K-fold éªŒè¯

æˆ‘ä»¬ä¹‹å‰ä¸€ç›´åœ¨åš â€”â€” ä¸ºäº†å¯¹ç½‘ç»œè¿›è¡Œè¯„ä¼°ï¼Œæ¥è°ƒèŠ‚ç½‘ç»œå‚æ•°(æ¯”å¦‚è®­ç»ƒçš„è½®æ•°)ï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦è¿™ä¹ˆåšã€‚
ä½†æœ‰ä¸€ç‚¹éº»çƒ¦æ˜¯ï¼Œæˆ‘ä»¬è¿™æ¬¡çš„æ•°æ®å¤ªå°‘äº†ï¼Œæ‰€ä»¥åˆ†å‡ºæ¥çš„éªŒè¯é›†å°±å¾ˆå°ï¼ˆå¯èƒ½æ‰æœ‰100æ¡æ•°æ®ï¼‰ã€‚è¿™ç§æƒ…å†µä¸‹ï¼ŒéªŒè¯é›†é€‰æ‹©çš„æ•°æ®ä¸åŒï¼Œå¯èƒ½å¯¹éªŒè¯çš„ç»“æœæœ‰å¾ˆå¤§å½±å“ï¼ˆå³éªŒè¯é›†çš„åˆ’åˆ†æ–¹å¼ä¸åŒå¯èƒ½é€ æˆéªŒè¯ç»“æœçš„æ–¹å·®å¾ˆå¤§ï¼‰ï¼Œè¿™ç§æƒ…å†µä¼šå½±å“æˆ‘ä»¬å¯¹æ¨¡å‹çš„éªŒè¯ã€‚

åœ¨è¿™ç§å°´å°¬çš„å¢ƒåœ°ï¼Œæœ€ä½³å®è·µæ˜¯ä½¿ç”¨ *K-fold* äº¤å‰éªŒè¯ï¼ˆKæŠ˜äº¤å‰éªŒè¯ï¼‰ã€‚

ç”¨ K-fold éªŒè¯ï¼Œæˆ‘ä»¬æŠŠæ•°æ®åˆ†æˆ K ä¸ªéƒ¨åˆ†ï¼ˆä¸€èˆ¬ k = 4 or 5ï¼‰ï¼Œç„¶åå®ä¾‹åŒ– K ä¸ªç‹¬ç«‹çš„æ¨¡å‹ï¼Œæ¯ä¸ªç”¨ K-1 ä»½æ•°æ®å»è®­ç»ƒï¼Œç„¶åç”¨å‰©ä½™çš„ä¸€ä»½å»éªŒè¯ï¼Œæœ€ç»ˆæ¨¡å‹çš„éªŒè¯åˆ†æ•°ä½¿ç”¨ K ä¸ªéƒ¨åˆ†çš„å¹³å‡å€¼ã€‚

![K-fold éªŒè¯ç¤ºæ„å›¾](https://tva1.sinaimg.cn/large/007S8ZIlgy1gert24t4p8j317u0mstci.jpg)

K-fold éªŒè¯çš„ä»£ç å®ç°ï¼š

ç¨å¾®æ”¹ä¸€ä¸‹ä¹¦ä¸Šçš„ä¾‹å­ï¼Œæˆ‘ä»¬åŠ ä¸€ç‚¹ç”¨ TensorBoard æ¥å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹çš„ä»£ç ã€‚é¦–å…ˆåœ¨ Jupyter Lab ç¬”è®°æœ¬é‡ŒåŠ è½½ tensorboardï¼š


```python
# Load the TensorBoard notebook extension
# TensorBoard å¯ä»¥å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
%load_ext tensorboard
# Clear any logs from previous runs
!rm -rf ./logs/ 
```

è¾“å‡ºï¼š

    The tensorboard extension is already loaded. To reload it, use:
      %reload_ext tensorboard

ç„¶åå¼€å§‹å†™ä¸»è¦çš„ä»£ç ï¼š

```python
import numpy as np
import datetime
import tensorflow as tf

k = 4
num_val_samples = len(train_data) // k
num_epochs = 100
all_scores = []

# å‡†å¤‡ TensorBoard
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

for i in range(k):
    print(f'processing fold #{i} ({i+1}/{k})')
    
    # å‡†å¤‡éªŒè¯æ•°æ®
    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    partial_train_data = np.concatenate(
        [train_data[: i * num_val_samples],
         train_data[(i+1) * num_val_samples :]],
        axis=0)
    partial_train_targets = np.concatenate(
        [train_targets[: i * num_val_samples], 
         train_targets[(i+1) *  num_val_samples :]], 
        axis=0)
    
    #æ„å»ºã€è®­ç»ƒæ¨¡å‹
    model = build_model()
    model.fit(partial_train_data, partial_train_targets, 
              epochs=num_epochs, batch_size=1, verbose=0,
              callbacks=[tensorboard_callback])
    
    # æœ‰éªŒè¯é›†è¯„ä¼°æ¨¡å‹
    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
    all_scores.append(val_mae)


np.mean(all_scores)
```

    processing fold #0 (1/4)
    processing fold #1 (2/4)
    processing fold #2 (3/4)
    processing fold #3 (4/4)
    
    2.4046657

ç”¨ä¸‹é¢è¿™ä¸ªå‘½ä»¤å¯ä»¥åœ¨ Jupyter Lab ç¬”è®°æœ¬é‡Œæ˜¾ç¤º tensorboardï¼š


```python
%tensorboard --logdir logs/fit
```

å®ƒå¤§æ¦‚æ˜¯è¿™æ ·çš„ï¼š

![tensorboardçš„æˆªå›¾](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje37vwedj31fw0u0dmn.jpg)

è¿™ä¸ªä¸œè¥¿ä¹Ÿå¯ä»¥ç›´æ¥åœ¨ä½ çš„æµè§ˆå™¨é‡Œç›´æ¥æ‰“å¼€ `http://localhost:6006` å¾—åˆ°ã€‚

åˆšæ‰é‚£ä¸ªåªæ˜¯ç©ä¸€ä¸‹å“ˆï¼Œç°åœ¨æˆ‘ä»¬æ”¹ä¸€ä¸‹ï¼Œè¿­ä»£ 500 è½®ï¼ˆæ²¡ç‹¬æ˜¾çš„mbpè·‘è¿™ä¸ªå¥½æ…¢å•Šå•Šå•Šï¼‰ï¼ŒæŠŠè®­ç»ƒçš„ç»“æœè®°ä¸‹æ¥ï¼š


```python
k = 4
num_val_samples = len(train_data) // k

num_epochs = 500
all_mae_histories = []

for i in range(k):
    print(f'processing fold #{i} ({i+1}/{k})')
    
    # å‡†å¤‡éªŒè¯æ•°æ®
    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    partial_train_data = np.concatenate(
        [train_data[: i * num_val_samples],
         train_data[(i+1) * num_val_samples :]],
        axis=0)
    partial_train_targets = np.concatenate(
        [train_targets[: i * num_val_samples], 
         train_targets[(i+1) *  num_val_samples :]], 
        axis=0)
    
    #æ„å»ºã€è®­ç»ƒæ¨¡å‹
    model = build_model()
    history = model.fit(partial_train_data, partial_train_targets,
                        validation_data=(val_data, val_targets),
                        epochs=num_epochs, batch_size=1, verbose=0)

    mae_history = history.history['val_mae']
    all_mae_histories.append(mae_history)


print("Done.")
```

    processing fold #0 (1/4)
    processing fold #1 (2/4)
    processing fold #2 (3/4)
    processing fold #3 (4/4)
    Done.

ç”»å›¾ï¼š

```python
average_mae_history = [
    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]
```


```python
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje0zqitmj30b007eq35.jpg)


è¿™ä¸ªå›¾å¤ªå¯†äº†ï¼Œçœ‹ä¸æ¸…ï¼Œæ‰€ä»¥è¦å¤„ç†ä¸€ä¸‹ï¼š

- å»æ‰å‰åç»„æ•°æ®ï¼Œè¿™ç«¯æ˜æ˜¾å’Œå…¶ä»–çš„æ•°é‡å·®è·æ¯”è¾ƒå¤§ï¼›
- æŠŠæ¯ä¸ªç‚¹éƒ½æ¢æˆå‰é¢æ•°æ®ç‚¹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡å€¼ï¼ˆan exponential moving average of the previous pointsï¼‰ï¼ŒæŠŠæ›²çº¿å˜å¹³æ»‘ï¼›


```python
def smooth_curve(points, factor=0.9):
  smoothed_points = []
  for point in points:
    if smoothed_points:
      previous = smoothed_points[-1]
      smoothed_points.append(previous * factor + point * (1 - factor))
    else:
      smoothed_points.append(point)
  return smoothed_points

smooth_mae_history = smooth_curve(average_mae_history[10:])

plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)
plt.xlabel('Epochs')
plt.ylabel('Validation MAE')
plt.show()
```


![png](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggje11t08ij30b007egls.jpg)


ä»è¿™ä¸ªå›¾çœ‹ï¼Œå·®ä¸å¤šè¿‡äº† 80 ä¸ª epochs åå°±è¿‡æ‹Ÿåˆäº†ã€‚

åœ¨å°è¯•äº†è¿™äº›ä¹‹åï¼Œæˆ‘ä»¬æ‰¾å‡ºæœ€ä½³çš„å‚æ•°ï¼ˆè½®æ•°å•¦ï¼Œç½‘ç»œçš„å±‚æ•°å•¦ï¼Œè¿™äº›éƒ½å¯ä»¥è¯•è¯•ï¼‰ï¼Œç„¶åç”¨æœ€ä½³çš„å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šè®­ç»ƒï¼Œæ¥å¾—å‡ºæœ€ç»ˆçš„ç”Ÿäº§æ¨¡å‹ã€‚


```python
# è®­ç»ƒæœ€ç»ˆæ¨¡å‹

model = build_model()
model.fit(train_data, train_targets, 
          epochs=80, batch_size=16, verbose=0)

# æœ€åè¯„ä¼°ä¸€ä¸‹
test_mse_score, test_mae_score = model.evaluate(test_data, test_targets, verbose=0)
print(test_mse_score, test_mae_score)
```

    17.43332971311083 2.6102107


è¿™ä¸ª test_mae_score çš„å€¼è¯´æ˜ï¼Œæˆ‘ä»¬è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹çš„é¢„æµ‹å’Œå®é™…å¤§æ¦‚å·®äº† 2k+ ç¾å…ƒã€‚ã€‚ã€‚ğŸ˜­



